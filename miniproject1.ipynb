{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Miniproject 1: Image Classification\n",
    "\n",
    "## Introduction\n",
    "\n",
    "### Description\n",
    "\n",
    "One of the deepest traditions in learning about deep learning is to first [tackle the exciting problem of MNIST classification](http://deeplearning.net/tutorial/logreg.html). [The MNIST database](https://en.wikipedia.org/wiki/MNIST_database) (Modified National Institute of Standards and Technology database) is a large database of handwritten digits that was [recently extended](https://arxiv.org/abs/1702.05373). We break with this tradition (just a little bit) and tackle first the related problem of classifying cropped, downsampled and grayscaled images of house numbers in the [The Street View House Numbers (SVHN) Dataset](http://ufldl.stanford.edu/housenumbers/).\n",
    "\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "- You should have a running installation of [tensorflow](https://www.tensorflow.org/install/) and [keras](https://keras.io/).\n",
    "- You should know the concepts \"multilayer perceptron\", \"stochastic gradient descent with minibatches\", \"training and validation data\", \"overfitting\" and \"early stopping\".\n",
    "\n",
    "### What you will learn\n",
    "\n",
    "- You will learn how to define feedforward neural networks in keras and fit them to data.\n",
    "- You will be guided through a prototyping procedure for the application of deep learning to a specific domain.\n",
    "- You will get in contact with concepts discussed later in the lecture, like \"regularization\", \"batch normalization\" and \"convolutional networks\".\n",
    "- You will gain some experience on the influence of network architecture, optimizer and regularization choices on the goodness of fit.\n",
    "- You will learn to be more patient :) Some fits may take your computer quite a bit of time; run them over night.\n",
    "\n",
    "### Evaluation criteria\n",
    "\n",
    "The evaluation is (mostly) based on the figures you submit and your answer sentences. \n",
    "We will only do random tests of your code and not re-run the full notebook.\n",
    "\n",
    "### Your names\n",
    "\n",
    "Before you start, please enter your full name(s) in the field below; they are used to load the data. The variable student2 may remain empty, if you work alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-09T09:08:24.514461Z",
     "start_time": "2018-03-09T09:08:24.506410Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "student1 = \"Amaury Combes\"\n",
    "student2 = \"Vincenzo Bazzucchi\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some helper functions\n",
    "\n",
    "For your convenience we provide here some functions to preprocess the data and plot the results later. Simply run the following cells with `Shift-Enter`.\n",
    "\n",
    "### Dependencies and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-09T09:09:16.113721Z",
     "start_time": "2018-03-09T09:09:16.100520Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, Flatten, BatchNormalization\n",
    "from keras.optimizers import SGD, Adam\n",
    "\n",
    "import itertools\n",
    "\n",
    "# you may experiment with different subsets, \n",
    "# but make sure in the submission \n",
    "# it is generated with the correct random seed for all exercises.\n",
    "np.random.seed(hash(student1 + student2) % 2**32)\n",
    "subset_of_classes = np.random.choice(range(10), 5, replace = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 10, 6\n",
    "def plot_some_samples(x, y = [], yhat = [], select_from = [], \n",
    "                      ncols = 6, nrows = 4, xdim = 16, ydim = 16,\n",
    "                      label_mapping = range(10)):\n",
    "    \"\"\"plot some input vectors as grayscale images (optionally together with their assigned or predicted labels).\n",
    "    \n",
    "    x is an NxD - dimensional array, where D is the length of an input vector and N is the number of samples.\n",
    "    Out of the N samples, ncols x nrows indices are randomly selected from the list select_from (if it is empty, select_from becomes range(N)).\n",
    "    \n",
    "    Keyword arguments:\n",
    "    y             -- corresponding labels to plot in green below each image.\n",
    "    yhat          -- corresponding predicted labels to plot in red below each image.\n",
    "    select_from   -- list of indices from which to select the images.\n",
    "    ncols, nrows  -- number of columns and rows to plot.\n",
    "    xdim, ydim    -- number of pixels of the images in x- and y-direction.\n",
    "    label_mapping -- map labels to digits.\n",
    "    \n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(nrows, ncols)\n",
    "    if len(select_from) == 0:\n",
    "        select_from = range(x.shape[0])\n",
    "    indices = np.random.choice(select_from, size = min(ncols * nrows, len(select_from)), replace = False)\n",
    "    for i, ind in enumerate(indices):\n",
    "        thisax = ax[i//ncols,i%ncols]\n",
    "        thisax.matshow(x[ind].reshape(xdim, ydim), cmap='gray')\n",
    "        thisax.set_axis_off()\n",
    "        if len(y) != 0:\n",
    "            j = y[ind] if type(y[ind]) != np.ndarray else y[ind].argmax()\n",
    "            thisax.text(0, 0, (label_mapping[j]+1)%10, color='green', \n",
    "                                                       verticalalignment='top',\n",
    "                                                       transform=thisax.transAxes)\n",
    "        if len(yhat) != 0:\n",
    "            k = yhat[ind] if type(yhat[ind]) != np.ndarray else yhat[ind].argmax()\n",
    "            thisax.text(1, 0, (label_mapping[k]+1)%10, color='red',\n",
    "                                             verticalalignment='top',\n",
    "                                             horizontalalignment='right',\n",
    "                                             transform=thisax.transAxes)\n",
    "    return fig\n",
    "\n",
    "def prepare_standardplot(title, xlabel):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.suptitle(title)\n",
    "    ax1.set_ylabel('categorical cross entropy')\n",
    "    ax1.set_xlabel(xlabel)\n",
    "    ax1.set_yscale('log')\n",
    "    ax2.set_ylabel('accuracy [% correct]')\n",
    "    ax2.set_xlabel(xlabel)\n",
    "    return fig, ax1, ax2\n",
    "\n",
    "def finalize_standardplot(fig, ax1, ax2):\n",
    "    ax1handles, ax1labels = ax1.get_legend_handles_labels()\n",
    "    if len(ax1labels) > 0:\n",
    "        ax1.legend(ax1handles, ax1labels)\n",
    "    ax2handles, ax2labels = ax2.get_legend_handles_labels()\n",
    "    if len(ax2labels) > 0:\n",
    "        ax2.legend(ax2handles, ax2labels)\n",
    "    fig.tight_layout()\n",
    "    plt.subplots_adjust(top=0.9)\n",
    "\n",
    "def plot_history(history, title):\n",
    "    fig, ax1, ax2 = prepare_standardplot(title, 'epoch')\n",
    "    ax1.plot(history.history['loss'], label = \"training\")\n",
    "    ax1.plot(history.history['val_loss'], label = \"validation\")\n",
    "    ax2.plot(history.history['acc'], label = \"training\")\n",
    "    ax2.plot(history.history['val_acc'], label = \"validation\")\n",
    "    finalize_standardplot(fig, ax1, ax2)\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and preprocessing the data\n",
    "\n",
    "The data consists of RGB color images with 32x32 pixels, loaded into an array of dimension 32x32x3x(number of images). We convert them to grayscale (using [this method](https://en.wikipedia.org/wiki/SRGB#The_reverse_transformation)) and we downsample them to images of 16x16 pixels by averaging over patches of 2x2 pixels.\n",
    "\n",
    "With these preprocessing steps we obviously remove some information that could be helpful in classifying the images. But, since the processed data is much lower dimensional, the fitting procedures converge faster. This is an advantage in situations like here (or generally when prototyping), were we want to try many different things without having to wait too long for computations to finish. After having gained some experience, one may want to go back to work on the 32x32 RGB images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert RGB images x to grayscale using the formula for Y_linear in https://en.wikipedia.org/wiki/Grayscale#Colorimetric_(perceptual_luminance-preserving)_conversion_to_grayscale\n",
    "def grayscale(x):\n",
    "    x = x.astype('float32')/255\n",
    "    x = np.piecewise(x, [x <= 0.04045, x > 0.04045], \n",
    "                        [lambda x: x/12.92, lambda x: ((x + .055)/1.055)**2.4])\n",
    "    return .2126 * x[:,:,0,:] + .7152 * x[:,:,1,:]  + .07152 * x[:,:,2,:]\n",
    "\n",
    "def downsample(x):\n",
    "    return sum([x[i::2,j::2,:] for i in range(2) for j in range(2)])/4\n",
    "\n",
    "def preprocess(data):\n",
    "    gray = grayscale(data['X'])\n",
    "    downsampled = downsample(gray)\n",
    "    return (downsampled.reshape(16*16, gray.shape[2]).transpose(),\n",
    "            data['y'].flatten() - 1)\n",
    "\n",
    "\n",
    "data_train = scipy.io.loadmat('housenumbers/train_32x32.mat')\n",
    "data_test = scipy.io.loadmat('housenumbers/test_32x32.mat')\n",
    "\n",
    "x_train_all, y_train_all = preprocess(data_train)\n",
    "x_test_all, y_test_all = preprocess(data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting a subset of classes\n",
    "\n",
    "We furter reduce the size of the dataset (and thus reduce computation time) by selecting only the 5 (out of 10 digits) in subset_of_classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_classes(x, y, classes):\n",
    "    indices = []\n",
    "    labels = []\n",
    "    count = 0\n",
    "    for c in classes:\n",
    "        tmp = np.where(y == c)[0]\n",
    "        indices.extend(tmp)\n",
    "        labels.extend(np.ones(len(tmp), dtype='uint8') * count)\n",
    "        count += 1\n",
    "    return x[indices], labels\n",
    "\n",
    "x_train, y_train = extract_classes(x_train_all, y_train_all, subset_of_classes)\n",
    "x_test, y_test = extract_classes(x_test_all, y_test_all, subset_of_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us plot some examples now. The green digit at the bottom left of each image indicates the corresponding label in y_test.\n",
    "For further usage of the function plot_some_samples, please have a look at its definition in the plotting section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13278, 256)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_some_samples(x_test, y_test, label_mapping = subset_of_classes);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prepare for fitting we transform the labels to one hot coding, i.e. for 5 classes, label 2 becomes the vector [0, 0, 1, 0, 0] (python uses 0-indexing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = keras.utils.to_categorical(y_train)\n",
    "y_test = keras.utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: No hidden layer\n",
    "\n",
    "### Description\n",
    "\n",
    "Define and fit a model without a hidden layer. \n",
    "\n",
    "1. Use the softmax activation for the output layer.\n",
    "2. Use the categorical_crossentropy loss.\n",
    "3. Add the accuracy metric to the metrics.\n",
    "4. Choose stochastic gradient descent for the optimizer.\n",
    "5. Choose a minibatch size of 128.\n",
    "6. Fit for as many epochs as needed to see no further decrease in the validation loss.\n",
    "7. Plot the output of the fitting procedure (a history object) using the function plot_history defined above.\n",
    "8. Determine the indices of all test images that are misclassified by the fitted model and plot some of them using the function \n",
    "   `plot_some_samples(x_test, y_test, yhat_test, error_indices, label_mapping = subset_of_classes)`\n",
    "\n",
    "\n",
    "Hints:\n",
    "* Read the keras docs, in particular [Getting started with the Keras Sequential model](https://keras.io/getting-started/sequential-model-guide/).\n",
    "* Have a look at the keras [examples](https://github.com/keras-team/keras/tree/master/examples), e.g. [mnist_mlp](https://github.com/keras-team/keras/blob/master/examples/mnist_mlp.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use the `EarlyStopping` callback to ensure point 7: it will stop the learning process when the validation loss stops decreasing for 5 iterations. As discussed in the forum, with early stopping we mean a different behavior. Below we try to provide a simple implementation of this regularization technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ALL HYPERPARAMETERS NEED TUNING\n",
    "model = Sequential([\n",
    "    Dense(y_train.shape[1], input_shape=(x_train.shape[1],), activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    loss=keras.losses.categorical_crossentropy,\n",
    "    optimizer=SGD(lr=0.1), #find params\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    x_train, y_train,\n",
    "    validation_data=(x_test, y_test),\n",
    "    epochs=9999999999, batch_size=128,\n",
    "    callbacks=[keras.callbacks.EarlyStopping('val_loss', patience=5, min_delta=0.001)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for metric, value in zip(model.metrics_names, model.evaluate(x_test, y_test)):\n",
    "    print(metric, '=', value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_history(history, \"No Hidden Layer\") # if do not store in var is displayed twice..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: One hidden layer, different optizimizers\n",
    "### Description\n",
    "\n",
    "Train a network with one hidden layer and compare different optimizers.\n",
    "\n",
    "1. Use one hidden layer with 64 units and the 'relu' activation. Use the [summary method](https://keras.io/models/about-keras-models/) to inspect your model.\n",
    "2. Fit the model for 50 epochs with different learning rates of stochastic gradient descent and answer the question below.\n",
    "3. Replace the stochastic gradient descent optimizer with the [Adam optimizer](https://keras.io/optimizers/#adam).\n",
    "4. Plot the learning curves of SGD with a reasonable learning rate together with the learning curves of Adam in the same figure. Take care of a reasonable labeling of the curves in the plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(64, input_shape=(x_train.shape[1],), activation=\"relu\"),\n",
    "    Dense(y_train.shape[1], activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    loss=keras.losses.categorical_crossentropy,\n",
    "    optimizer=SGD(lr=0.01),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "historySGD = model.fit(\n",
    "    x_train, y_train,\n",
    "    validation_data=(x_test, y_test),\n",
    "    epochs=50,\n",
    "    verbose=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_history(historySGD, \"SGD\") # if do not store in var is displayed twice..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LARGE_RATE = 0.9\n",
    "SMALL_RATE = 10**(-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sgd_test_rate = Sequential([\n",
    "    Dense(64, input_shape=(x_train.shape[1],), activation=\"relu\"),\n",
    "    Dense(y_train.shape[1], activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sgd_test_rate.compile(\n",
    "    loss=keras.losses.categorical_crossentropy,\n",
    "    optimizer=SGD(lr=LARGE_RATE),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "_ = plot_history(sgd_test_rate.fit(\n",
    "    x_train, y_train,\n",
    "    validation_data=(x_test, y_test),\n",
    "    epochs=50,\n",
    "    verbose=0\n",
    "), \"SGD with very large learning rate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sgd_test_rate.compile(\n",
    "    loss=keras.losses.categorical_crossentropy,\n",
    "    optimizer=SGD(lr=SMALL_RATE),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "_ = plot_history(sgd_test_rate.fit(\n",
    "    x_train, y_train,\n",
    "    validation_data=(x_test, y_test),\n",
    "    epochs=50,\n",
    "    verbose=0\n",
    "), \"SGD with very small learning rate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: What happens if the learning rate of SGD is A) very large B) very small? Please answer A) and B) with one full sentence (double click this markdown cell to edit).\n",
    "\n",
    "**Answer**:\n",
    "\n",
    "A) The validation error and accuracy are very \"unstable\", the search for the optimal value goes in the wrong direction many times\n",
    "\n",
    "B) The improvement is very slow but constant: we would need three times the number of epochs to reach the best result obtained with the larger learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss=keras.losses.categorical_crossentropy,\n",
    "    optimizer=Adam(lr=0.001),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "historyAdam = model.fit(\n",
    "    x_train, y_train,\n",
    "    validation_data=(x_test, y_test),\n",
    "    epochs=50,\n",
    "    verbose=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_ = plot_history(historyAdam, \"Adam\") # if do not store in var is displayed twice..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the same learning rate with Adam and SGD does not allow Adam to improve its accuracy, therefore we changed it here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, ax1, ax2 = prepare_standardplot(\"Comparing SGD and ADAM\", 'epoch')\n",
    "ax1.plot(historySGD.history['loss'], label = \"training with SGD\", linestyle='--', c='b')\n",
    "ax1.plot(historySGD.history['val_loss'], label = \"validation with SGD\", linestyle='-', c='b')\n",
    "ax2.plot(historySGD.history['acc'], label = \"training with SGD\", linestyle='--', c='b')\n",
    "ax2.plot(historySGD.history['val_acc'], label = \"validation with SGD\", linestyle='-', c='b')\n",
    "\n",
    "ax1.plot(historyAdam.history['loss'], label = \"training with Adam\", linestyle='--', c='orange')\n",
    "ax1.plot(historyAdam.history['val_loss'], label = \"validation with Adam\", linestyle='-', c='orange')\n",
    "ax2.plot(historyAdam.history['acc'], label = \"training with Adam\", linestyle='--', c='orange')\n",
    "ax2.plot(historyAdam.history['val_acc'], label = \"validation with Adam\", linestyle='-', c='orange')\n",
    "finalize_standardplot(fig, ax1, ax2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Overfitting and early stopping with Adam\n",
    "\n",
    "### Description\n",
    "\n",
    "Run the above simulation with Adam for sufficiently many epochs (be patient!) until you see clear overfitting.\n",
    "\n",
    "1. Plot the learning curves of a fit with Adam and sufficiently many epochs and answer the questions below.\n",
    "\n",
    "A simple, but effective mean to avoid overfitting is early stopping, i.e. a fit is not run until convergence but stopped as soon as the validation error starts to increase. We will use early stopping in all subsequent exercises.\n",
    "\n",
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_ = plot_history(historyAdam, \"Adam\") # if do not store in var is displayed twice..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training we ran before was already overfitting: we can clearly see that the training error keeps decreasing while the validation error stays stable. We can see the same pattern observing the accuracy: the training accuracy keeps increasing while the validation accuracy is mostly stable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1**: At which epoch (approximately) does the model start to overfit? Please answer with one full sentence.\n",
    "\n",
    "**Answer**: The model start to overfit right away but after epoch 15 we clearly see that the validation error stays stable or increases while the training error keeps decreasing\n",
    "\n",
    "**Question 2**: Explain the qualitative difference between the loss curves and the accuracy curves with respect to signs of overfitting. Please answer with at most 3 full sentences.\n",
    "\n",
    "**Answer**: # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed in the forum, we should not use `keras.callbacks.EarlyStopping`. Therefore we implemented early stopping as described in the Deeplearning book: each time the validation accuracy improves, we take a snapshot of the model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RealEarlyStopper(keras.callbacks.Callback):\n",
    "    def __init__(self, set_best_at_end=True):\n",
    "        self._best_score = -1\n",
    "        self._best_weights = None\n",
    "        self._set_best_at_end = set_best_at_end\n",
    "    \n",
    "    def on_epoch_end(self, epoch=None, logs={}):\n",
    "        valacc = logs['val_acc']\n",
    "        if valacc > self._best_score:\n",
    "            self._best_score = valacc\n",
    "            self._best_weights = [layer.get_weigths().copy() for layer in self.model.layers]\n",
    "        \n",
    "    def on_train_end(self, logs={}):\n",
    "        if not self._set_best_at_end:\n",
    "            return\n",
    "        for layer, best_weights in zip(self.model.layers, self._best_weights):\n",
    "            layer.set_weights(best_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Model performance as a function of number of hidden neurons\n",
    "\n",
    "### Description\n",
    "\n",
    "Investigate how the best validation loss and accuracy depends on the number of hidden neurons in a single layer.\n",
    "\n",
    "1. Fit a reasonable number of models with different hidden layer size (between 10 and 1000 hidden neurons) for a fixed number of epochs well beyond the point of overfitting.\n",
    "2. Collect some statistics by fitting the same models as in 1. for multiple initial conditions. Hints: 1. If you don't reset the random seed, you get different initial conditions each time you create a new model. 2. Let your computer work while you are asleep.\n",
    "3. Plot summary statistics of the final validation loss and accuracy versus the number of hidden neurons. Hint: [boxplots](https://matplotlib.org/examples/pylab_examples/boxplot_demo.html) (also [here](https://matplotlib.org/api/_as_gen/matplotlib.axes.Axes.boxplot.html?highlight=boxplot#matplotlib.axes.Axes.boxplot)) are useful. You may also want to use the matplotlib method set_xticklabels.\n",
    "4. Plot summary statistics of the loss and accuracy for early stopping versus the number of hidden neurons.\n",
    "\n",
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we observed from our previous plots, the Adam optimizer converges much quicker than the SGD one. Therefore we will use it here. We saw that the model start to overfit at around 15 so we will train it for 30 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_model_result(hidden_neurons):\n",
    "    m = Sequential([\n",
    "        Dense(hidden_neurons, input_shape=(x_train.shape[1],), activation=\"relu\"),\n",
    "        Dense(y_train.shape[1], activation=\"softmax\")\n",
    "    ])\n",
    "    \n",
    "    m.compile(\n",
    "        loss=keras.losses.categorical_crossentropy,\n",
    "        optimizer=Adam(lr=0.001),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    h = model.fit(\n",
    "        x_train, y_train,\n",
    "        validation_data=(x_test, y_test),\n",
    "        epochs=30,\n",
    "        verbose=0,\n",
    "        callbacks=[keras.callbacks.EarlyStopping('val_loss', patience=3, min_delta=0.001)]\n",
    "    )\n",
    "    \n",
    "    return h.history['val_loss'][-1], h.history['val_acc'][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions 1 and 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = 20\n",
    "hidden_neurons = list(map(int, np.logspace(1, 3, N)))\n",
    "stats = []\n",
    "for idx, hid in enumerate(hidden_neurons):\n",
    "    # This seed or the seed parameter of initializer?\n",
    "    # https://keras.io/initializers/\n",
    "    np.random.seed(hash(student1 + student2) % 2**32)\n",
    "    stats.append(get_model_result(hid))\n",
    "    print(\"Completed train {}/{}\".format(idx+1, N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, (ax_loss, ax_acc) = plt.subplots(nrows=1, ncols=2)#, sharey=True)\n",
    "\n",
    "losses, accuracies = zip(*stats)\n",
    "\n",
    "ax_loss.boxplot(losses)\n",
    "ax_loss.set_title(\"Validation loss\")\n",
    "#ax_loss.set_yticklabels(hidden_neurons) # I am not sure it makes any sense: to build boxplot points are not kept in the same order\n",
    "\n",
    "\n",
    "ax_acc.boxplot(accuracies)\n",
    "ax_acc.set_title(\"Validation accuracy\")\n",
    "#ax_acc.set_yticklabels(hidden_neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, (ax_loss, ax_acc) = plt.subplots(nrows=1, ncols=2, sharey=True)\n",
    "\n",
    "ax_loss.plot(hidden_neurons, losses)\n",
    "ax_loss.set_title(\"Validation loss\")\n",
    "ax_acc.plot(hidden_neurons, accuracies)\n",
    "ax_acc.set_title(\"Validation accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: Comparison to deep models\n",
    "\n",
    "### Description\n",
    "\n",
    "Instead of choosing one hidden layer (with many neurons) you experiment here with multiple hidden layers (each with not so many neurons).\n",
    "\n",
    "1. Fit models with 2, 3 and 4 hidden layers with approximately the same number of parameters as a network with one hidden layer of 100 neurons. Hint: Calculate the number of parameters in a network with input dimensionality N_in, K hidden layers with N_h units, one output layer with N_out dimensions and solve for N_h. Confirm you result with the keras method model.summary().\n",
    "2. Run each model multiple times with different initial conditions and plot summary statistics of the best validation loss and accuracy versus the number of hidden layers.\n",
    "\n",
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit models with 2, 3 and 4 hidden layers with approximately the same number of parameters as a network with one hidden layer of 100 neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dense_factory(input_, output, weight_regularizer, bias_regularizer):\n",
    "    return Dense(output, input_shape=(input_,), activation=\"softmax\", kernel_regularizer=weight_regularizer, bias_regularizer=bias_regularizer)\n",
    "\n",
    "def model_factory(units, layers, batch_normalization, weight_regularizer, bias_regularizer, dropout=None):\n",
    "    # Check input validity for dropout\n",
    "    if dropout is not None and len(dropout) < 2:\n",
    "        dropout.append(0)\n",
    "        \n",
    "    model_builder = Sequential()\n",
    "    \n",
    "    # Input layer\n",
    "    model_builder.add(dense_factory(x_train.shape[1], units, weight_regularizer, bias_regularizer)) \n",
    "    if dropout is not None:\n",
    "        model_builder.add(Dropout(dropout[0]))\n",
    "        \n",
    "    if batch_normalization is not None:\n",
    "        model_builder.add(BatchNormalization(momentum=batch_normalization))\n",
    "        \n",
    "    # Hidden layers\n",
    "    for i in range(layers-1):\n",
    "        model_builder.add(dense_factory((units,), units, weight_regularizer, bias_regularizer))\n",
    "        if dropout is not None:\n",
    "            model_builder.add(Dropout(dropout[1]))\n",
    "            \n",
    "        if batch_normalization is not None:\n",
    "            model_builder.add(BatchNormalization(momentum=batch_normalization))\n",
    "        \n",
    "    # Output layer\n",
    "    model_builder.add(dense_factory((units,), y_train.shape[1], weight_regularizer, bias_regularizer))\n",
    "    if dropout is not None:\n",
    "        model_builder.add(Dropout(dropout[1]))\n",
    "        \n",
    "    if batch_normalization is not None:\n",
    "        model_builder.add(BatchNormalization(momentum=batch_normalization))\n",
    "    \n",
    "    return model_builder\n",
    "\n",
    "# Dummy regularizer, used as non existing regularizer\n",
    "null_reg = keras.regularizers.l1(0)\n",
    "\n",
    "# Factories for required models\n",
    "size_per_layer_1_hidden = 100\n",
    "size_per_layer_2_hidden = 77\n",
    "size_per_layer_3_hidden = 66\n",
    "size_per_layer_4_hidden = 59\n",
    "\n",
    "def model_1_hidden_factory(batch_normalization=None, weight_regularizer=None, bias_regularizer=None, dropout=None):\n",
    "    return model_factory(size_per_layer_1_hidden, 1, batch_normalization, weight_regularizer, bias_regularizer, dropout)\n",
    "\n",
    "def model_2_hidden_factory(batch_normalization=None, weight_regularizer=None, bias_regularizer=None, dropout=None):\n",
    "    return model_factory(size_per_layer_2_hidden, 2, batch_normalization, weight_regularizer, bias_regularizer, dropout)\n",
    "\n",
    "def model_3_hidden_factory(batch_normalization=None, weight_regularizer=None, bias_regularizer=None, dropout=None):\n",
    "    return model_factory(size_per_layer_3_hidden, 3, batch_normalization, weight_regularizer, bias_regularizer, dropout)\n",
    "\n",
    "def model_4_hidden_factory(batch_normalization=None, weight_regularizer=None, bias_regularizer=None, dropout=None):\n",
    "    return model_factory(size_per_layer_4_hidden, 4, batch_normalization, weight_regularizer, bias_regularizer, dropout)\n",
    "\n",
    "def compile_model(model):\n",
    "    model.compile(\n",
    "        loss=keras.losses.categorical_crossentropy,\n",
    "        optimizer=Adam(lr=0.01),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "def fit_model(model, x_train=x_train, y_train=y_train, x_test=x_test, y_test=y_test):\n",
    "    h = model.fit(\n",
    "        x_train, y_train,\n",
    "        validation_data=(x_test, y_test),\n",
    "        epochs=100,\n",
    "        verbose=1,\n",
    "        #callbacks=[keras.callbacks.EarlyStopping('val_loss', patience=5, min_delta=0.001)]\n",
    "        #callbacks=[RealEarlyStopper]\n",
    "    )   \n",
    "    return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confirm you result with the keras method model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 100)               25700     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 505       \n",
      "=================================================================\n",
      "Total params: 26,205\n",
      "Trainable params: 26,205\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 77)                19789     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 77)                6006      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 5)                 390       \n",
      "=================================================================\n",
      "Total params: 26,185\n",
      "Trainable params: 26,185\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 66)                16962     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 66)                4422      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 66)                4422      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 5)                 335       \n",
      "=================================================================\n",
      "Total params: 26,141\n",
      "Trainable params: 26,141\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_10 (Dense)             (None, 59)                15163     \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 59)                3540      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 59)                3540      \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 59)                3540      \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 5)                 300       \n",
      "=================================================================\n",
      "Total params: 26,083\n",
      "Trainable params: 26,083\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_1_hidden_factory().summary()\n",
    "model_2_hidden_factory().summary()\n",
    "model_3_hidden_factory().summary()\n",
    "model_4_hidden_factory().summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.\n",
    "#### Run each model multiple times with different initial conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model 1 / 4\n",
      "Inner step: 1 / 40\n",
      "Train on 37686 samples, validate on 13278 samples\n",
      "Epoch 1/100\n",
      "37686/37686 [==============================] - 3s 68us/step - loss: 1.3622 - acc: 0.4325 - val_loss: 1.2940 - val_acc: 0.4740\n",
      "Epoch 2/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 1.2780 - acc: 0.4670 - val_loss: 1.3564 - val_acc: 0.4618\n",
      "Epoch 3/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 1.2732 - acc: 0.4693 - val_loss: 1.2929 - val_acc: 0.4779\n",
      "Epoch 4/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 1.2698 - acc: 0.4716 - val_loss: 1.3080 - val_acc: 0.4761\n",
      "Epoch 5/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 1.2674 - acc: 0.4737 - val_loss: 1.3032 - val_acc: 0.4809\n",
      "Epoch 6/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 1.2292 - acc: 0.4987 - val_loss: 1.2484 - val_acc: 0.5044\n",
      "Epoch 7/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 1.1948 - acc: 0.5129 - val_loss: 1.2618 - val_acc: 0.5002\n",
      "Epoch 8/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 1.1867 - acc: 0.5164 - val_loss: 1.2364 - val_acc: 0.5133\n",
      "Epoch 9/100\n",
      "37686/37686 [==============================] - 2s 61us/step - loss: 1.1803 - acc: 0.5157 - val_loss: 1.2550 - val_acc: 0.5047\n",
      "Epoch 10/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 1.1783 - acc: 0.5172 - val_loss: 1.2379 - val_acc: 0.5128\n",
      "Epoch 11/100\n",
      "37686/37686 [==============================] - 2s 52us/step - loss: 1.1746 - acc: 0.5179 - val_loss: 1.2237 - val_acc: 0.5127\n",
      "Epoch 12/100\n",
      "37686/37686 [==============================] - 2s 48us/step - loss: 1.1746 - acc: 0.5189 - val_loss: 1.2498 - val_acc: 0.5087\n",
      "Epoch 13/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 1.1726 - acc: 0.5180 - val_loss: 1.2272 - val_acc: 0.5093\n",
      "Epoch 14/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 1.1719 - acc: 0.5193 - val_loss: 1.2378 - val_acc: 0.5054\n",
      "Epoch 15/100\n",
      "37686/37686 [==============================] - 2s 51us/step - loss: 1.1709 - acc: 0.5197 - val_loss: 1.2246 - val_acc: 0.5151\n",
      "Epoch 16/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 1.1693 - acc: 0.5198 - val_loss: 1.2504 - val_acc: 0.5105\n",
      "Epoch 17/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 1.1718 - acc: 0.5191 - val_loss: 1.2645 - val_acc: 0.5024\n",
      "Epoch 18/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 1.1681 - acc: 0.5202 - val_loss: 1.2184 - val_acc: 0.5194\n",
      "Epoch 19/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 1.1473 - acc: 0.5277 - val_loss: 1.1459 - val_acc: 0.5484\n",
      "Epoch 20/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 1.0694 - acc: 0.5566 - val_loss: 1.0972 - val_acc: 0.5654\n",
      "Epoch 21/100\n",
      "37686/37686 [==============================] - 2s 63us/step - loss: 1.0309 - acc: 0.5738 - val_loss: 1.0697 - val_acc: 0.5693\n",
      "Epoch 22/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.9999 - acc: 0.5828 - val_loss: 1.0326 - val_acc: 0.5733\n",
      "Epoch 23/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.9792 - acc: 0.5859 - val_loss: 1.0066 - val_acc: 0.5877\n",
      "Epoch 24/100\n",
      "37686/37686 [==============================] - 2s 51us/step - loss: 0.9653 - acc: 0.5899 - val_loss: 1.0256 - val_acc: 0.5796\n",
      "Epoch 25/100\n",
      "37686/37686 [==============================] - 2s 52us/step - loss: 0.9670 - acc: 0.5931 - val_loss: 1.0403 - val_acc: 0.5914\n",
      "Epoch 26/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.9590 - acc: 0.5952 - val_loss: 1.0074 - val_acc: 0.5889\n",
      "Epoch 27/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.9568 - acc: 0.5965 - val_loss: 1.0041 - val_acc: 0.5908\n",
      "Epoch 28/100\n",
      "37686/37686 [==============================] - 2s 51us/step - loss: 0.9479 - acc: 0.6004 - val_loss: 1.0168 - val_acc: 0.5911\n",
      "Epoch 29/100\n",
      "37686/37686 [==============================] - 2s 50us/step - loss: 0.9495 - acc: 0.6004 - val_loss: 1.0145 - val_acc: 0.5854\n",
      "Epoch 30/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 0.9437 - acc: 0.6018 - val_loss: 0.9745 - val_acc: 0.6164\n",
      "Epoch 31/100\n",
      "37686/37686 [==============================] - 2s 51us/step - loss: 0.8747 - acc: 0.6492 - val_loss: 0.9025 - val_acc: 0.6560\n",
      "Epoch 32/100\n",
      "37686/37686 [==============================] - 2s 49us/step - loss: 0.8403 - acc: 0.6658 - val_loss: 0.9201 - val_acc: 0.6406\n",
      "Epoch 33/100\n",
      "37686/37686 [==============================] - 2s 49us/step - loss: 0.8345 - acc: 0.6660 - val_loss: 0.9140 - val_acc: 0.6548\n",
      "Epoch 34/100\n",
      "37686/37686 [==============================] - 2s 49us/step - loss: 0.8299 - acc: 0.6700 - val_loss: 0.9055 - val_acc: 0.6499\n",
      "Epoch 35/100\n",
      "37686/37686 [==============================] - 2s 48us/step - loss: 0.8249 - acc: 0.6692 - val_loss: 0.9150 - val_acc: 0.6497\n",
      "Epoch 36/100\n",
      "37686/37686 [==============================] - 2s 48us/step - loss: 0.8227 - acc: 0.6719 - val_loss: 0.9196 - val_acc: 0.6619\n",
      "Epoch 37/100\n",
      "37686/37686 [==============================] - 2s 49us/step - loss: 0.8164 - acc: 0.6764 - val_loss: 0.9252 - val_acc: 0.6638\n",
      "Epoch 38/100\n",
      "37686/37686 [==============================] - 2s 48us/step - loss: 0.8137 - acc: 0.6788 - val_loss: 0.8906 - val_acc: 0.6673\n",
      "Epoch 39/100\n",
      "37686/37686 [==============================] - 2s 51us/step - loss: 0.8135 - acc: 0.6823 - val_loss: 0.8877 - val_acc: 0.6655\n",
      "Epoch 40/100\n",
      "37686/37686 [==============================] - 2s 50us/step - loss: 0.8045 - acc: 0.6850 - val_loss: 0.8928 - val_acc: 0.6554\n",
      "Epoch 41/100\n",
      "37686/37686 [==============================] - 2s 51us/step - loss: 0.8076 - acc: 0.6847 - val_loss: 0.9124 - val_acc: 0.6606\n",
      "Epoch 42/100\n",
      "37686/37686 [==============================] - 2s 51us/step - loss: 0.8065 - acc: 0.6866 - val_loss: 0.8930 - val_acc: 0.6681\n",
      "Epoch 43/100\n",
      "37686/37686 [==============================] - 2s 52us/step - loss: 0.7989 - acc: 0.6880 - val_loss: 0.8850 - val_acc: 0.6700\n",
      "Epoch 44/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.7970 - acc: 0.6912 - val_loss: 0.9107 - val_acc: 0.6596\n",
      "Epoch 45/100\n",
      "37686/37686 [==============================] - 2s 50us/step - loss: 0.7991 - acc: 0.6878 - val_loss: 0.8892 - val_acc: 0.6734\n",
      "Epoch 46/100\n",
      "37686/37686 [==============================] - 2s 49us/step - loss: 0.7867 - acc: 0.6945 - val_loss: 0.8743 - val_acc: 0.6736\n",
      "Epoch 47/100\n",
      "37686/37686 [==============================] - 2s 61us/step - loss: 0.7784 - acc: 0.6953 - val_loss: 0.8719 - val_acc: 0.6750\n",
      "Epoch 48/100\n",
      "37686/37686 [==============================] - 2s 61us/step - loss: 0.7689 - acc: 0.6965 - val_loss: 0.8533 - val_acc: 0.6792\n",
      "Epoch 49/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.7574 - acc: 0.7002 - val_loss: 0.8890 - val_acc: 0.6668\n",
      "Epoch 50/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.7413 - acc: 0.7233 - val_loss: 0.8599 - val_acc: 0.7040\n",
      "Epoch 51/100\n",
      "37686/37686 [==============================] - 2s 52us/step - loss: 0.7205 - acc: 0.7410 - val_loss: 0.8107 - val_acc: 0.7193\n",
      "Epoch 52/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 0.7031 - acc: 0.7491 - val_loss: 0.7998 - val_acc: 0.7323\n",
      "Epoch 53/100\n",
      "37686/37686 [==============================] - 2s 49us/step - loss: 0.6907 - acc: 0.7548 - val_loss: 0.7994 - val_acc: 0.7306\n",
      "Epoch 54/100\n",
      "37686/37686 [==============================] - 2s 50us/step - loss: 0.6837 - acc: 0.7591 - val_loss: 0.8084 - val_acc: 0.7291\n",
      "Epoch 55/100\n",
      "37686/37686 [==============================] - 2s 52us/step - loss: 0.6815 - acc: 0.7597 - val_loss: 0.8367 - val_acc: 0.7262\n",
      "Epoch 56/100\n",
      "37686/37686 [==============================] - 2s 52us/step - loss: 0.6803 - acc: 0.7593 - val_loss: 0.8147 - val_acc: 0.7367\n",
      "Epoch 57/100\n",
      "37686/37686 [==============================] - 2s 50us/step - loss: 0.6746 - acc: 0.7628 - val_loss: 0.7862 - val_acc: 0.7314\n",
      "Epoch 58/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.6709 - acc: 0.7646 - val_loss: 0.7771 - val_acc: 0.7406\n",
      "Epoch 59/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.6720 - acc: 0.7633 - val_loss: 0.8253 - val_acc: 0.7289\n",
      "Epoch 60/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.6651 - acc: 0.7679 - val_loss: 0.7932 - val_acc: 0.7365\n",
      "Epoch 61/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.6658 - acc: 0.7653 - val_loss: 0.7850 - val_acc: 0.7359\n",
      "Epoch 62/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.6624 - acc: 0.7671 - val_loss: 0.7921 - val_acc: 0.7375\n",
      "Epoch 63/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.6599 - acc: 0.7711 - val_loss: 0.8076 - val_acc: 0.7296\n",
      "Epoch 64/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.6582 - acc: 0.7708 - val_loss: 0.7947 - val_acc: 0.7421\n",
      "Epoch 65/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.6561 - acc: 0.7695 - val_loss: 0.8100 - val_acc: 0.7339\n",
      "Epoch 66/100\n",
      "37686/37686 [==============================] - 2s 60us/step - loss: 0.6533 - acc: 0.7716 - val_loss: 0.7862 - val_acc: 0.7412\n",
      "Epoch 67/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.6517 - acc: 0.7744 - val_loss: 0.8293 - val_acc: 0.7341\n",
      "Epoch 68/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.6570 - acc: 0.7728 - val_loss: 0.8375 - val_acc: 0.7277\n",
      "Epoch 69/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.6500 - acc: 0.7735 - val_loss: 0.7923 - val_acc: 0.7369\n",
      "Epoch 70/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.6473 - acc: 0.7749 - val_loss: 0.7966 - val_acc: 0.7377\n",
      "Epoch 71/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.6449 - acc: 0.7749 - val_loss: 0.7853 - val_acc: 0.7352\n",
      "Epoch 72/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.6404 - acc: 0.7760 - val_loss: 0.7946 - val_acc: 0.7427\n",
      "Epoch 73/100\n",
      "37686/37686 [==============================] - 2s 63us/step - loss: 0.6400 - acc: 0.7774 - val_loss: 0.7998 - val_acc: 0.7334\n",
      "Epoch 74/100\n",
      "37686/37686 [==============================] - 2s 61us/step - loss: 0.6383 - acc: 0.7767 - val_loss: 0.8413 - val_acc: 0.7296\n",
      "Epoch 75/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.6402 - acc: 0.7752 - val_loss: 0.7757 - val_acc: 0.7442\n",
      "Epoch 76/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.6368 - acc: 0.7787 - val_loss: 0.7891 - val_acc: 0.7393\n",
      "Epoch 77/100\n",
      "37686/37686 [==============================] - 2s 66us/step - loss: 0.6366 - acc: 0.7784 - val_loss: 0.7881 - val_acc: 0.7367\n",
      "Epoch 78/100\n",
      "37686/37686 [==============================] - 3s 73us/step - loss: 0.6351 - acc: 0.7766 - val_loss: 0.7775 - val_acc: 0.7421\n",
      "Epoch 79/100\n",
      "37686/37686 [==============================] - 2s 64us/step - loss: 0.6299 - acc: 0.7789 - val_loss: 0.8078 - val_acc: 0.7302\n",
      "Epoch 80/100\n",
      "37686/37686 [==============================] - 2s 63us/step - loss: 0.6342 - acc: 0.7787 - val_loss: 0.7763 - val_acc: 0.7485\n",
      "Epoch 81/100\n",
      "37686/37686 [==============================] - 2s 65us/step - loss: 0.6317 - acc: 0.7800 - val_loss: 0.7836 - val_acc: 0.7420\n",
      "Epoch 82/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.6281 - acc: 0.7801 - val_loss: 0.7748 - val_acc: 0.7440\n",
      "Epoch 83/100\n",
      "37686/37686 [==============================] - 2s 51us/step - loss: 0.6309 - acc: 0.7797 - val_loss: 0.8247 - val_acc: 0.7293\n",
      "Epoch 84/100\n",
      "37686/37686 [==============================] - 2s 48us/step - loss: 0.6283 - acc: 0.7806 - val_loss: 0.7728 - val_acc: 0.7447\n",
      "Epoch 85/100\n",
      "37686/37686 [==============================] - 2s 51us/step - loss: 0.6245 - acc: 0.7805 - val_loss: 0.8063 - val_acc: 0.7321\n",
      "Epoch 86/100\n",
      "37686/37686 [==============================] - 2s 48us/step - loss: 0.6264 - acc: 0.7782 - val_loss: 0.8198 - val_acc: 0.7316\n",
      "Epoch 87/100\n",
      "37686/37686 [==============================] - 2s 49us/step - loss: 0.6234 - acc: 0.7824 - val_loss: 0.7846 - val_acc: 0.7442\n",
      "Epoch 88/100\n",
      "37686/37686 [==============================] - 2s 48us/step - loss: 0.6247 - acc: 0.7810 - val_loss: 0.8090 - val_acc: 0.7306\n",
      "Epoch 89/100\n",
      "37686/37686 [==============================] - 2s 48us/step - loss: 0.6244 - acc: 0.7806 - val_loss: 0.7709 - val_acc: 0.7458\n",
      "Epoch 90/100\n",
      "37686/37686 [==============================] - 2s 51us/step - loss: 0.6189 - acc: 0.7815 - val_loss: 0.8040 - val_acc: 0.7378\n",
      "Epoch 91/100\n",
      "37686/37686 [==============================] - 2s 51us/step - loss: 0.6201 - acc: 0.7829 - val_loss: 0.8034 - val_acc: 0.7417\n",
      "Epoch 92/100\n",
      "37686/37686 [==============================] - 2s 51us/step - loss: 0.6208 - acc: 0.7807 - val_loss: 0.7751 - val_acc: 0.7497\n",
      "Epoch 93/100\n",
      "37686/37686 [==============================] - 2s 51us/step - loss: 0.6177 - acc: 0.7835 - val_loss: 0.7983 - val_acc: 0.7353\n",
      "Epoch 94/100\n",
      "37686/37686 [==============================] - 2s 51us/step - loss: 0.6147 - acc: 0.7830 - val_loss: 0.7923 - val_acc: 0.7469\n",
      "Epoch 95/100\n",
      "37686/37686 [==============================] - 2s 50us/step - loss: 0.6198 - acc: 0.7836 - val_loss: 0.7959 - val_acc: 0.7376\n",
      "Epoch 96/100\n",
      "37686/37686 [==============================] - 2s 52us/step - loss: 0.6162 - acc: 0.7842 - val_loss: 0.7794 - val_acc: 0.7461\n",
      "Epoch 97/100\n",
      "37686/37686 [==============================] - 2s 51us/step - loss: 0.6168 - acc: 0.7837 - val_loss: 0.7999 - val_acc: 0.7418\n",
      "Epoch 98/100\n",
      "37686/37686 [==============================] - 2s 51us/step - loss: 0.6107 - acc: 0.7866 - val_loss: 0.7798 - val_acc: 0.7412\n",
      "Epoch 99/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.6159 - acc: 0.7835 - val_loss: 0.7839 - val_acc: 0.7408\n",
      "Epoch 100/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 0.6112 - acc: 0.7854 - val_loss: 0.8021 - val_acc: 0.7461\n",
      "Inner step: 2 / 40\n",
      "Train on 37686 samples, validate on 13278 samples\n",
      "Epoch 1/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 1.3530 - acc: 0.4414 - val_loss: 1.2901 - val_acc: 0.4756\n",
      "Epoch 2/100\n",
      "37686/37686 [==============================] - 2s 50us/step - loss: 1.2822 - acc: 0.4682 - val_loss: 1.2975 - val_acc: 0.4734\n",
      "Epoch 3/100\n",
      "37686/37686 [==============================] - 2s 51us/step - loss: 1.2713 - acc: 0.4712 - val_loss: 1.2876 - val_acc: 0.4786\n",
      "Epoch 4/100\n",
      "37686/37686 [==============================] - 2s 52us/step - loss: 1.2682 - acc: 0.4735 - val_loss: 1.2961 - val_acc: 0.4779\n",
      "Epoch 5/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 1.2674 - acc: 0.4741 - val_loss: 1.2984 - val_acc: 0.4747\n",
      "Epoch 6/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 1.2644 - acc: 0.4745 - val_loss: 1.2867 - val_acc: 0.4793\n",
      "Epoch 7/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 1.2456 - acc: 0.4901 - val_loss: 1.2566 - val_acc: 0.5029\n",
      "Epoch 8/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 1.1999 - acc: 0.5115 - val_loss: 1.2275 - val_acc: 0.5061\n",
      "Epoch 9/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 1.1892 - acc: 0.5163 - val_loss: 1.2374 - val_acc: 0.5078\n",
      "Epoch 10/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 1.1806 - acc: 0.5165 - val_loss: 1.2208 - val_acc: 0.5111\n",
      "Epoch 11/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 1.1802 - acc: 0.5178 - val_loss: 1.2188 - val_acc: 0.5147\n",
      "Epoch 12/100\n",
      "37686/37686 [==============================] - 2s 51us/step - loss: 1.1774 - acc: 0.5178 - val_loss: 1.2237 - val_acc: 0.5116\n",
      "Epoch 13/100\n",
      "37686/37686 [==============================] - 2s 51us/step - loss: 1.1738 - acc: 0.5198 - val_loss: 1.2384 - val_acc: 0.5150\n",
      "Epoch 14/100\n",
      "37686/37686 [==============================] - 2s 52us/step - loss: 1.1718 - acc: 0.5189 - val_loss: 1.2178 - val_acc: 0.5106\n",
      "Epoch 15/100\n",
      "37686/37686 [==============================] - 2s 62us/step - loss: 1.1715 - acc: 0.5213 - val_loss: 1.2354 - val_acc: 0.5080\n",
      "Epoch 16/100\n",
      "37686/37686 [==============================] - 3s 69us/step - loss: 1.1700 - acc: 0.5203 - val_loss: 1.2244 - val_acc: 0.5165\n",
      "Epoch 17/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37686/37686 [==============================] - 2s 57us/step - loss: 1.1660 - acc: 0.5245 - val_loss: 1.2333 - val_acc: 0.5182\n",
      "Epoch 18/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 1.1570 - acc: 0.5328 - val_loss: 1.2147 - val_acc: 0.5272\n",
      "Epoch 19/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 1.1562 - acc: 0.5374 - val_loss: 1.2403 - val_acc: 0.5255\n",
      "Epoch 20/100\n",
      "37686/37686 [==============================] - 2s 64us/step - loss: 1.1422 - acc: 0.5481 - val_loss: 1.2700 - val_acc: 0.5729\n",
      "Epoch 21/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 1.0299 - acc: 0.6133 - val_loss: 1.0280 - val_acc: 0.6243\n",
      "Epoch 22/100\n",
      "37686/37686 [==============================] - 2s 51us/step - loss: 0.9209 - acc: 0.6517 - val_loss: 0.9404 - val_acc: 0.6549\n",
      "Epoch 23/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 0.8567 - acc: 0.6771 - val_loss: 0.9118 - val_acc: 0.6616\n",
      "Epoch 24/100\n",
      "37686/37686 [==============================] - 2s 52us/step - loss: 0.8364 - acc: 0.6843 - val_loss: 0.9027 - val_acc: 0.6656\n",
      "Epoch 25/100\n",
      "37686/37686 [==============================] - 2s 52us/step - loss: 0.8268 - acc: 0.6888 - val_loss: 0.9117 - val_acc: 0.6646\n",
      "Epoch 26/100\n",
      "37686/37686 [==============================] - 2s 52us/step - loss: 0.8189 - acc: 0.6923 - val_loss: 0.9078 - val_acc: 0.6678\n",
      "Epoch 27/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.8159 - acc: 0.6941 - val_loss: 0.8974 - val_acc: 0.6705\n",
      "Epoch 28/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.8114 - acc: 0.6941 - val_loss: 0.9067 - val_acc: 0.6607\n",
      "Epoch 29/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.8057 - acc: 0.6973 - val_loss: 0.9001 - val_acc: 0.6718\n",
      "Epoch 30/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.8003 - acc: 0.6990 - val_loss: 0.9258 - val_acc: 0.6693\n",
      "Epoch 31/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.7981 - acc: 0.6991 - val_loss: 0.9373 - val_acc: 0.6730\n",
      "Epoch 32/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.7938 - acc: 0.7016 - val_loss: 0.9215 - val_acc: 0.6674\n",
      "Epoch 33/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.7873 - acc: 0.7035 - val_loss: 0.9026 - val_acc: 0.6757\n",
      "Epoch 34/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.7869 - acc: 0.7024 - val_loss: 0.8909 - val_acc: 0.6814\n",
      "Epoch 35/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.7843 - acc: 0.7047 - val_loss: 0.8765 - val_acc: 0.6834\n",
      "Epoch 36/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.7795 - acc: 0.7057 - val_loss: 0.8908 - val_acc: 0.6854\n",
      "Epoch 37/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.7805 - acc: 0.7042 - val_loss: 0.8885 - val_acc: 0.6725\n",
      "Epoch 38/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.7730 - acc: 0.7067 - val_loss: 0.8871 - val_acc: 0.6792\n",
      "Epoch 39/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.7740 - acc: 0.7061 - val_loss: 0.9147 - val_acc: 0.6667\n",
      "Epoch 40/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.7681 - acc: 0.7094 - val_loss: 0.8757 - val_acc: 0.6786\n",
      "Epoch 41/100\n",
      "37686/37686 [==============================] - 2s 66us/step - loss: 0.7674 - acc: 0.7079 - val_loss: 0.9068 - val_acc: 0.6688\n",
      "Epoch 42/100\n",
      "37686/37686 [==============================] - 2s 64us/step - loss: 0.7666 - acc: 0.7097 - val_loss: 0.8815 - val_acc: 0.6802\n",
      "Epoch 43/100\n",
      "37686/37686 [==============================] - 2s 52us/step - loss: 0.7638 - acc: 0.7103 - val_loss: 0.8930 - val_acc: 0.6723\n",
      "Epoch 44/100\n",
      "37686/37686 [==============================] - 2s 51us/step - loss: 0.7614 - acc: 0.7101 - val_loss: 0.9014 - val_acc: 0.6814\n",
      "Epoch 45/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.7596 - acc: 0.7108 - val_loss: 0.8773 - val_acc: 0.6828\n",
      "Epoch 46/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.7595 - acc: 0.7119 - val_loss: 0.8491 - val_acc: 0.6900\n",
      "Epoch 47/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.7590 - acc: 0.7119 - val_loss: 0.8808 - val_acc: 0.6762\n",
      "Epoch 48/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.7532 - acc: 0.7118 - val_loss: 0.8808 - val_acc: 0.6829\n",
      "Epoch 49/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 0.7512 - acc: 0.7127 - val_loss: 0.8604 - val_acc: 0.6853\n",
      "Epoch 50/100\n",
      "37686/37686 [==============================] - 2s 51us/step - loss: 0.7504 - acc: 0.7127 - val_loss: 0.8706 - val_acc: 0.6792\n",
      "Epoch 51/100\n",
      "37686/37686 [==============================] - 2s 52us/step - loss: 0.7462 - acc: 0.7133 - val_loss: 0.8647 - val_acc: 0.6880\n",
      "Epoch 52/100\n",
      "37686/37686 [==============================] - 2s 60us/step - loss: 0.7442 - acc: 0.7147 - val_loss: 0.8562 - val_acc: 0.6897\n",
      "Epoch 53/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 0.7428 - acc: 0.7150 - val_loss: 0.8664 - val_acc: 0.6873\n",
      "Epoch 54/100\n",
      "37686/37686 [==============================] - 2s 51us/step - loss: 0.7401 - acc: 0.7148 - val_loss: 0.8695 - val_acc: 0.6782\n",
      "Epoch 55/100\n",
      "37686/37686 [==============================] - 2s 51us/step - loss: 0.7380 - acc: 0.7148 - val_loss: 0.9083 - val_acc: 0.6750\n",
      "Epoch 56/100\n",
      "37686/37686 [==============================] - 2s 52us/step - loss: 0.7331 - acc: 0.7154 - val_loss: 0.8451 - val_acc: 0.6897\n",
      "Epoch 57/100\n",
      "37686/37686 [==============================] - 2s 52us/step - loss: 0.7304 - acc: 0.7155 - val_loss: 0.9039 - val_acc: 0.6701\n",
      "Epoch 58/100\n",
      "37686/37686 [==============================] - 2s 52us/step - loss: 0.7188 - acc: 0.7188 - val_loss: 0.8563 - val_acc: 0.6938\n",
      "Epoch 59/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.7049 - acc: 0.7324 - val_loss: 0.8705 - val_acc: 0.6947\n",
      "Epoch 60/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.6844 - acc: 0.7497 - val_loss: 0.8319 - val_acc: 0.7131\n",
      "Epoch 61/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.6577 - acc: 0.7642 - val_loss: 0.8043 - val_acc: 0.7311\n",
      "Epoch 62/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.6511 - acc: 0.7650 - val_loss: 0.8559 - val_acc: 0.7061\n",
      "Epoch 63/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.6398 - acc: 0.7687 - val_loss: 0.8021 - val_acc: 0.7265\n",
      "Epoch 64/100\n",
      "37686/37686 [==============================] - 2s 60us/step - loss: 0.6350 - acc: 0.7691 - val_loss: 0.8536 - val_acc: 0.7073\n",
      "Epoch 65/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.6295 - acc: 0.7743 - val_loss: 0.7707 - val_acc: 0.7399\n",
      "Epoch 66/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 0.6299 - acc: 0.7724 - val_loss: 0.8049 - val_acc: 0.7326\n",
      "Epoch 67/100\n",
      "37686/37686 [==============================] - 2s 51us/step - loss: 0.6298 - acc: 0.7736 - val_loss: 0.7784 - val_acc: 0.7357\n",
      "Epoch 68/100\n",
      "37686/37686 [==============================] - 2s 52us/step - loss: 0.6239 - acc: 0.7750 - val_loss: 0.7821 - val_acc: 0.7390\n",
      "Epoch 69/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.6229 - acc: 0.7754 - val_loss: 0.7855 - val_acc: 0.7375\n",
      "Epoch 70/100\n",
      "37686/37686 [==============================] - 2s 61us/step - loss: 0.6175 - acc: 0.7783 - val_loss: 0.7803 - val_acc: 0.7419\n",
      "Epoch 71/100\n",
      "37686/37686 [==============================] - 2s 64us/step - loss: 0.6193 - acc: 0.7765 - val_loss: 0.7701 - val_acc: 0.7409\n",
      "Epoch 72/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.6101 - acc: 0.7781 - val_loss: 0.8000 - val_acc: 0.7320\n",
      "Epoch 73/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.6151 - acc: 0.7788 - val_loss: 0.7666 - val_acc: 0.7377\n",
      "Epoch 74/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.6142 - acc: 0.7776 - val_loss: 0.7702 - val_acc: 0.7396\n",
      "Epoch 75/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.6100 - acc: 0.7811 - val_loss: 0.7631 - val_acc: 0.7445\n",
      "Epoch 76/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.6100 - acc: 0.7823 - val_loss: 0.7621 - val_acc: 0.7415\n",
      "Epoch 77/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.6087 - acc: 0.7797 - val_loss: 0.7861 - val_acc: 0.7299\n",
      "Epoch 78/100\n",
      "37686/37686 [==============================] - 2s 52us/step - loss: 0.6056 - acc: 0.7832 - val_loss: 0.7606 - val_acc: 0.7478\n",
      "Epoch 79/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.6054 - acc: 0.7833 - val_loss: 0.7608 - val_acc: 0.7451\n",
      "Epoch 80/100\n",
      "37686/37686 [==============================] - 2s 60us/step - loss: 0.6070 - acc: 0.7822 - val_loss: 0.7558 - val_acc: 0.7529\n",
      "Epoch 81/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.6002 - acc: 0.7841 - val_loss: 0.7654 - val_acc: 0.7476\n",
      "Epoch 82/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.6034 - acc: 0.7841 - val_loss: 0.7530 - val_acc: 0.7513\n",
      "Epoch 83/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.6019 - acc: 0.7832 - val_loss: 0.7831 - val_acc: 0.7320\n",
      "Epoch 84/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.5989 - acc: 0.7853 - val_loss: 0.7694 - val_acc: 0.7371\n",
      "Epoch 85/100\n",
      "37686/37686 [==============================] - 2s 60us/step - loss: 0.5948 - acc: 0.7852 - val_loss: 0.8234 - val_acc: 0.7292\n",
      "Epoch 86/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.6005 - acc: 0.7853 - val_loss: 0.7788 - val_acc: 0.7460\n",
      "Epoch 87/100\n",
      "37686/37686 [==============================] - 2s 52us/step - loss: 0.5941 - acc: 0.7866 - val_loss: 0.7558 - val_acc: 0.7509\n",
      "Epoch 88/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 0.5947 - acc: 0.7848 - val_loss: 0.7774 - val_acc: 0.7488\n",
      "Epoch 89/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 0.5977 - acc: 0.7842 - val_loss: 0.7592 - val_acc: 0.7465\n",
      "Epoch 90/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.5934 - acc: 0.7874 - val_loss: 0.7479 - val_acc: 0.7551\n",
      "Epoch 91/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.5926 - acc: 0.7874 - val_loss: 0.7959 - val_acc: 0.7369\n",
      "Epoch 92/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.5898 - acc: 0.7894 - val_loss: 0.7821 - val_acc: 0.7376\n",
      "Epoch 93/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.5893 - acc: 0.7881 - val_loss: 0.7657 - val_acc: 0.7475\n",
      "Epoch 94/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.5900 - acc: 0.7876 - val_loss: 0.7591 - val_acc: 0.7513\n",
      "Epoch 95/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.5889 - acc: 0.7881 - val_loss: 0.7547 - val_acc: 0.7517\n",
      "Epoch 96/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.5906 - acc: 0.7879 - val_loss: 0.7771 - val_acc: 0.7410\n",
      "Epoch 97/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.5836 - acc: 0.7917 - val_loss: 0.7882 - val_acc: 0.7289\n",
      "Epoch 98/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.5858 - acc: 0.7907 - val_loss: 0.7862 - val_acc: 0.7478\n",
      "Epoch 99/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 0.5825 - acc: 0.7891 - val_loss: 0.7626 - val_acc: 0.7531\n",
      "Epoch 100/100\n",
      "37686/37686 [==============================] - 2s 51us/step - loss: 0.5839 - acc: 0.7912 - val_loss: 0.7774 - val_acc: 0.7476\n",
      "Inner step: 3 / 40\n",
      "Train on 37686 samples, validate on 13278 samples\n",
      "Epoch 1/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 1.3986 - acc: 0.4130 - val_loss: 1.3132 - val_acc: 0.4601\n",
      "Epoch 2/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 1.2837 - acc: 0.4652 - val_loss: 1.3038 - val_acc: 0.4718\n",
      "Epoch 3/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 1.2729 - acc: 0.4686 - val_loss: 1.2885 - val_acc: 0.4764\n",
      "Epoch 4/100\n",
      "37686/37686 [==============================] - 2s 51us/step - loss: 1.2701 - acc: 0.4707 - val_loss: 1.2970 - val_acc: 0.4762\n",
      "Epoch 5/100\n",
      "37686/37686 [==============================] - 2s 50us/step - loss: 1.2679 - acc: 0.4718 - val_loss: 1.2979 - val_acc: 0.4754\n",
      "Epoch 6/100\n",
      "37686/37686 [==============================] - 2s 52us/step - loss: 1.2659 - acc: 0.4723 - val_loss: 1.2969 - val_acc: 0.4754\n",
      "Epoch 7/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 1.2655 - acc: 0.4720 - val_loss: 1.3096 - val_acc: 0.4758\n",
      "Epoch 8/100\n",
      "37686/37686 [==============================] - 2s 51us/step - loss: 1.2637 - acc: 0.4722 - val_loss: 1.2856 - val_acc: 0.4803\n",
      "Epoch 9/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 1.2635 - acc: 0.4729 - val_loss: 1.2938 - val_acc: 0.4779\n",
      "Epoch 10/100\n",
      "37686/37686 [==============================] - 2s 52us/step - loss: 1.2544 - acc: 0.4817 - val_loss: 1.2639 - val_acc: 0.4976\n",
      "Epoch 11/100\n",
      "37686/37686 [==============================] - 2s 52us/step - loss: 1.2173 - acc: 0.5077 - val_loss: 1.2554 - val_acc: 0.5074\n",
      "Epoch 12/100\n",
      "37686/37686 [==============================] - 2s 52us/step - loss: 1.1945 - acc: 0.5139 - val_loss: 1.2288 - val_acc: 0.5102\n",
      "Epoch 13/100\n",
      "37686/37686 [==============================] - 2s 51us/step - loss: 1.1855 - acc: 0.5162 - val_loss: 1.2363 - val_acc: 0.5115\n",
      "Epoch 14/100\n",
      "37686/37686 [==============================] - 2s 50us/step - loss: 1.1783 - acc: 0.5186 - val_loss: 1.1772 - val_acc: 0.5454\n",
      "Epoch 15/100\n",
      "37686/37686 [==============================] - 2s 51us/step - loss: 1.0674 - acc: 0.5629 - val_loss: 1.1119 - val_acc: 0.5376\n",
      "Epoch 16/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 1.0032 - acc: 0.5758 - val_loss: 1.0219 - val_acc: 0.5838\n",
      "Epoch 17/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 0.9839 - acc: 0.5860 - val_loss: 1.0216 - val_acc: 0.5851\n",
      "Epoch 18/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.9758 - acc: 0.5897 - val_loss: 1.0224 - val_acc: 0.5831\n",
      "Epoch 19/100\n",
      "37686/37686 [==============================] - 2s 52us/step - loss: 0.9675 - acc: 0.5958 - val_loss: 1.0209 - val_acc: 0.5992\n",
      "Epoch 20/100\n",
      "37686/37686 [==============================] - 2s 50us/step - loss: 0.9647 - acc: 0.5979 - val_loss: 1.0035 - val_acc: 0.5900\n",
      "Epoch 21/100\n",
      "37686/37686 [==============================] - 2s 50us/step - loss: 0.9602 - acc: 0.5969 - val_loss: 1.0108 - val_acc: 0.5871\n",
      "Epoch 22/100\n",
      "37686/37686 [==============================] - 2s 51us/step - loss: 0.9524 - acc: 0.5999 - val_loss: 0.9829 - val_acc: 0.5990\n",
      "Epoch 23/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 0.9110 - acc: 0.6307 - val_loss: 0.9004 - val_acc: 0.6789\n",
      "Epoch 24/100\n",
      "37686/37686 [==============================] - 2s 52us/step - loss: 0.7875 - acc: 0.7026 - val_loss: 0.8636 - val_acc: 0.6837\n",
      "Epoch 25/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 0.7526 - acc: 0.7195 - val_loss: 0.8208 - val_acc: 0.7048\n",
      "Epoch 26/100\n",
      "37686/37686 [==============================] - 2s 52us/step - loss: 0.7432 - acc: 0.7208 - val_loss: 0.8570 - val_acc: 0.6936\n",
      "Epoch 27/100\n",
      "37686/37686 [==============================] - 2s 51us/step - loss: 0.7427 - acc: 0.7214 - val_loss: 0.8143 - val_acc: 0.7066\n",
      "Epoch 28/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 0.7306 - acc: 0.7251 - val_loss: 0.8421 - val_acc: 0.6935\n",
      "Epoch 29/100\n",
      "37686/37686 [==============================] - 2s 50us/step - loss: 0.7299 - acc: 0.7261 - val_loss: 0.8380 - val_acc: 0.7015\n",
      "Epoch 30/100\n",
      "37686/37686 [==============================] - 2s 50us/step - loss: 0.7263 - acc: 0.7245 - val_loss: 0.8388 - val_acc: 0.6978\n",
      "Epoch 31/100\n",
      "37686/37686 [==============================] - 2s 50us/step - loss: 0.7223 - acc: 0.7270 - val_loss: 0.8141 - val_acc: 0.7054\n",
      "Epoch 32/100\n",
      "37686/37686 [==============================] - 2s 51us/step - loss: 0.7171 - acc: 0.7276 - val_loss: 0.8086 - val_acc: 0.7055\n",
      "Epoch 33/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.7149 - acc: 0.7272 - val_loss: 0.8179 - val_acc: 0.7009\n",
      "Epoch 34/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37686/37686 [==============================] - 2s 53us/step - loss: 0.7120 - acc: 0.7318 - val_loss: 0.8557 - val_acc: 0.7064\n",
      "Epoch 35/100\n",
      "37686/37686 [==============================] - 2s 51us/step - loss: 0.7099 - acc: 0.7292 - val_loss: 0.7932 - val_acc: 0.7136\n",
      "Epoch 36/100\n",
      "37686/37686 [==============================] - 2s 52us/step - loss: 0.7067 - acc: 0.7318 - val_loss: 0.8162 - val_acc: 0.7132\n",
      "Epoch 37/100\n",
      "37686/37686 [==============================] - 3s 73us/step - loss: 0.7058 - acc: 0.7299 - val_loss: 0.8086 - val_acc: 0.7095\n",
      "Epoch 38/100\n",
      "37686/37686 [==============================] - 4s 98us/step - loss: 0.7019 - acc: 0.7333 - val_loss: 0.7961 - val_acc: 0.7189\n",
      "Epoch 39/100\n",
      "37686/37686 [==============================] - 6s 163us/step - loss: 0.6978 - acc: 0.7353 - val_loss: 0.7821 - val_acc: 0.7161\n",
      "Epoch 40/100\n",
      "37686/37686 [==============================] - 5s 123us/step - loss: 0.6971 - acc: 0.7354 - val_loss: 0.8237 - val_acc: 0.6924\n",
      "Epoch 41/100\n",
      "37686/37686 [==============================] - 3s 91us/step - loss: 0.6924 - acc: 0.7365 - val_loss: 0.7870 - val_acc: 0.7264\n",
      "Epoch 42/100\n",
      "37686/37686 [==============================] - 3s 87us/step - loss: 0.6896 - acc: 0.7384 - val_loss: 0.8252 - val_acc: 0.7033\n",
      "Epoch 43/100\n",
      "37686/37686 [==============================] - 3s 71us/step - loss: 0.6873 - acc: 0.7384 - val_loss: 0.7931 - val_acc: 0.7202\n",
      "Epoch 44/100\n",
      "37686/37686 [==============================] - 2s 61us/step - loss: 0.6856 - acc: 0.7436 - val_loss: 0.8415 - val_acc: 0.7195\n",
      "Epoch 45/100\n",
      "37686/37686 [==============================] - 2s 60us/step - loss: 0.6844 - acc: 0.7459 - val_loss: 0.7722 - val_acc: 0.7227\n",
      "Epoch 46/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.6811 - acc: 0.7474 - val_loss: 0.7789 - val_acc: 0.7213\n",
      "Epoch 47/100\n",
      "37686/37686 [==============================] - 2s 60us/step - loss: 0.6763 - acc: 0.7496 - val_loss: 0.7751 - val_acc: 0.7334\n",
      "Epoch 48/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.6742 - acc: 0.7503 - val_loss: 0.8060 - val_acc: 0.7146\n",
      "Epoch 49/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.6710 - acc: 0.7540 - val_loss: 0.8060 - val_acc: 0.7210\n",
      "Epoch 50/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.6662 - acc: 0.7559 - val_loss: 0.7714 - val_acc: 0.7339\n",
      "Epoch 51/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.6622 - acc: 0.7581 - val_loss: 0.7771 - val_acc: 0.7350\n",
      "Epoch 52/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.6635 - acc: 0.7602 - val_loss: 0.8202 - val_acc: 0.7167\n",
      "Epoch 53/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.6586 - acc: 0.7649 - val_loss: 0.7927 - val_acc: 0.7420\n",
      "Epoch 54/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.6512 - acc: 0.7694 - val_loss: 0.8102 - val_acc: 0.7317\n",
      "Epoch 55/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.6442 - acc: 0.7716 - val_loss: 0.7688 - val_acc: 0.7517\n",
      "Epoch 56/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.6457 - acc: 0.7744 - val_loss: 0.7811 - val_acc: 0.7328\n",
      "Epoch 57/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.6391 - acc: 0.7768 - val_loss: 0.7854 - val_acc: 0.7524\n",
      "Epoch 58/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.6375 - acc: 0.7780 - val_loss: 0.7501 - val_acc: 0.7463\n",
      "Epoch 59/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.6275 - acc: 0.7824 - val_loss: 0.7903 - val_acc: 0.7450\n",
      "Epoch 60/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.6305 - acc: 0.7830 - val_loss: 0.7517 - val_acc: 0.7564\n",
      "Epoch 61/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.6277 - acc: 0.7853 - val_loss: 0.7639 - val_acc: 0.7485\n",
      "Epoch 62/100\n",
      "37686/37686 [==============================] - 2s 62us/step - loss: 0.6216 - acc: 0.7895 - val_loss: 0.7628 - val_acc: 0.7588\n",
      "Epoch 63/100\n",
      "37686/37686 [==============================] - 4s 99us/step - loss: 0.6184 - acc: 0.7911 - val_loss: 0.7798 - val_acc: 0.7587\n",
      "Epoch 64/100\n",
      "37686/37686 [==============================] - 3s 77us/step - loss: 0.6120 - acc: 0.7977 - val_loss: 0.7478 - val_acc: 0.7647\n",
      "Epoch 65/100\n",
      "37686/37686 [==============================] - 2s 65us/step - loss: 0.6060 - acc: 0.8024 - val_loss: 0.7370 - val_acc: 0.7704\n",
      "Epoch 66/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.6041 - acc: 0.8063 - val_loss: 0.7477 - val_acc: 0.7774\n",
      "Epoch 67/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.6017 - acc: 0.8085 - val_loss: 0.7367 - val_acc: 0.7768\n",
      "Epoch 68/100\n",
      "37686/37686 [==============================] - 2s 61us/step - loss: 0.5945 - acc: 0.8119 - val_loss: 0.7281 - val_acc: 0.7808\n",
      "Epoch 69/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.5852 - acc: 0.8162 - val_loss: 0.7242 - val_acc: 0.7839\n",
      "Epoch 70/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.5812 - acc: 0.8188 - val_loss: 0.7501 - val_acc: 0.7768\n",
      "Epoch 71/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.5764 - acc: 0.8218 - val_loss: 0.7290 - val_acc: 0.7821\n",
      "Epoch 72/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.5719 - acc: 0.8243 - val_loss: 0.7209 - val_acc: 0.7856\n",
      "Epoch 73/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.5713 - acc: 0.8234 - val_loss: 0.7202 - val_acc: 0.7884\n",
      "Epoch 74/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.5702 - acc: 0.8253 - val_loss: 0.7276 - val_acc: 0.7863\n",
      "Epoch 75/100\n",
      "37686/37686 [==============================] - 2s 60us/step - loss: 0.5692 - acc: 0.8270 - val_loss: 0.7444 - val_acc: 0.7777\n",
      "Epoch 76/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.5634 - acc: 0.8283 - val_loss: 0.7410 - val_acc: 0.7796\n",
      "Epoch 77/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.5642 - acc: 0.8269 - val_loss: 0.7387 - val_acc: 0.7881\n",
      "Epoch 78/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.5583 - acc: 0.8291 - val_loss: 0.7313 - val_acc: 0.7865\n",
      "Epoch 79/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.5617 - acc: 0.8280 - val_loss: 0.8116 - val_acc: 0.7601\n",
      "Epoch 80/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.5608 - acc: 0.8284 - val_loss: 0.7496 - val_acc: 0.7788\n",
      "Epoch 81/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.5588 - acc: 0.8291 - val_loss: 0.7138 - val_acc: 0.7927\n",
      "Epoch 82/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.5537 - acc: 0.8301 - val_loss: 0.7293 - val_acc: 0.7851\n",
      "Epoch 83/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.5541 - acc: 0.8318 - val_loss: 0.7077 - val_acc: 0.7961\n",
      "Epoch 84/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.5627 - acc: 0.8286 - val_loss: 0.7385 - val_acc: 0.7802\n",
      "Epoch 85/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.5480 - acc: 0.8324 - val_loss: 0.7545 - val_acc: 0.7769\n",
      "Epoch 86/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.5496 - acc: 0.8323 - val_loss: 0.7237 - val_acc: 0.7912\n",
      "Epoch 87/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.5551 - acc: 0.8302 - val_loss: 0.7069 - val_acc: 0.7904\n",
      "Epoch 88/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.5486 - acc: 0.8336 - val_loss: 0.7342 - val_acc: 0.7842\n",
      "Epoch 89/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.5468 - acc: 0.8337 - val_loss: 0.7607 - val_acc: 0.7814\n",
      "Epoch 90/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.5463 - acc: 0.8344 - val_loss: 0.7417 - val_acc: 0.7775\n",
      "Epoch 91/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.5514 - acc: 0.8324 - val_loss: 0.7080 - val_acc: 0.7913\n",
      "Epoch 92/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.5453 - acc: 0.8340 - val_loss: 0.7227 - val_acc: 0.7882\n",
      "Epoch 93/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.5435 - acc: 0.8334 - val_loss: 0.7444 - val_acc: 0.7896\n",
      "Epoch 94/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.5439 - acc: 0.8355 - val_loss: 0.7182 - val_acc: 0.7916\n",
      "Epoch 95/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.5390 - acc: 0.8362 - val_loss: 0.7129 - val_acc: 0.7950\n",
      "Epoch 96/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.5451 - acc: 0.8339 - val_loss: 0.7320 - val_acc: 0.7851\n",
      "Epoch 97/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.5396 - acc: 0.8363 - val_loss: 0.7310 - val_acc: 0.7820\n",
      "Epoch 98/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.5476 - acc: 0.8351 - val_loss: 0.7355 - val_acc: 0.7851\n",
      "Epoch 99/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.5403 - acc: 0.8359 - val_loss: 0.7246 - val_acc: 0.7878\n",
      "Epoch 100/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.5320 - acc: 0.8387 - val_loss: 0.7303 - val_acc: 0.7877\n",
      "Inner step: 4 / 40\n",
      "Train on 37686 samples, validate on 13278 samples\n",
      "Epoch 1/100\n",
      "37686/37686 [==============================] - 2s 64us/step - loss: 1.1854 - acc: 0.5085 - val_loss: 1.0616 - val_acc: 0.5563\n",
      "Epoch 2/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 1.0109 - acc: 0.5754 - val_loss: 1.0177 - val_acc: 0.5804\n",
      "Epoch 3/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 0.9959 - acc: 0.5826 - val_loss: 1.0075 - val_acc: 0.5881\n",
      "Epoch 4/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 0.9780 - acc: 0.5900 - val_loss: 1.0213 - val_acc: 0.5849\n",
      "Epoch 5/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.9742 - acc: 0.5947 - val_loss: 1.0171 - val_acc: 0.5828\n",
      "Epoch 6/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.9656 - acc: 0.6015 - val_loss: 1.0030 - val_acc: 0.5954\n",
      "Epoch 7/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 0.9626 - acc: 0.6077 - val_loss: 0.9973 - val_acc: 0.6085\n",
      "Epoch 8/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.9470 - acc: 0.6180 - val_loss: 1.0141 - val_acc: 0.6085\n",
      "Epoch 9/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 0.9292 - acc: 0.6278 - val_loss: 1.0296 - val_acc: 0.5949\n",
      "Epoch 10/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 0.9273 - acc: 0.6288 - val_loss: 0.9936 - val_acc: 0.6139\n",
      "Epoch 11/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.9212 - acc: 0.6321 - val_loss: 0.9785 - val_acc: 0.6228\n",
      "Epoch 12/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.9190 - acc: 0.6333 - val_loss: 0.9980 - val_acc: 0.6093\n",
      "Epoch 13/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 0.9179 - acc: 0.6362 - val_loss: 0.9974 - val_acc: 0.6136\n",
      "Epoch 14/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.9141 - acc: 0.6361 - val_loss: 0.9932 - val_acc: 0.6200\n",
      "Epoch 15/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.9130 - acc: 0.6428 - val_loss: 0.9976 - val_acc: 0.6264\n",
      "Epoch 16/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.9036 - acc: 0.6520 - val_loss: 0.9587 - val_acc: 0.6436\n",
      "Epoch 17/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 0.8954 - acc: 0.6588 - val_loss: 0.9653 - val_acc: 0.6482\n",
      "Epoch 18/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.8839 - acc: 0.6632 - val_loss: 0.9520 - val_acc: 0.6537\n",
      "Epoch 19/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.8683 - acc: 0.6695 - val_loss: 1.0368 - val_acc: 0.6251\n",
      "Epoch 20/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 0.8643 - acc: 0.6723 - val_loss: 0.9191 - val_acc: 0.6630\n",
      "Epoch 21/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.8590 - acc: 0.6735 - val_loss: 0.9386 - val_acc: 0.6638\n",
      "Epoch 22/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.8541 - acc: 0.6747 - val_loss: 0.9468 - val_acc: 0.6488\n",
      "Epoch 23/100\n",
      "37686/37686 [==============================] - 2s 61us/step - loss: 0.8532 - acc: 0.6744 - val_loss: 0.9078 - val_acc: 0.6676\n",
      "Epoch 24/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.8461 - acc: 0.6722 - val_loss: 0.9470 - val_acc: 0.6496\n",
      "Epoch 25/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.8501 - acc: 0.6731 - val_loss: 0.9820 - val_acc: 0.6583\n",
      "Epoch 26/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.8416 - acc: 0.6741 - val_loss: 0.8994 - val_acc: 0.6704\n",
      "Epoch 27/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.8408 - acc: 0.6726 - val_loss: 1.0010 - val_acc: 0.6238\n",
      "Epoch 28/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 0.8074 - acc: 0.7021 - val_loss: 0.8386 - val_acc: 0.7102\n",
      "Epoch 29/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.7573 - acc: 0.7231 - val_loss: 0.8448 - val_acc: 0.6978\n",
      "Epoch 30/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.7300 - acc: 0.7327 - val_loss: 0.8337 - val_acc: 0.7091\n",
      "Epoch 31/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 0.7259 - acc: 0.7317 - val_loss: 0.8259 - val_acc: 0.7113\n",
      "Epoch 32/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.7231 - acc: 0.7304 - val_loss: 0.7976 - val_acc: 0.7066\n",
      "Epoch 33/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 0.7203 - acc: 0.7278 - val_loss: 0.8379 - val_acc: 0.7110\n",
      "Epoch 34/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.7128 - acc: 0.7311 - val_loss: 0.8106 - val_acc: 0.7072\n",
      "Epoch 35/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.7102 - acc: 0.7329 - val_loss: 0.7848 - val_acc: 0.7194\n",
      "Epoch 36/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.7056 - acc: 0.7338 - val_loss: 0.8149 - val_acc: 0.7034\n",
      "Epoch 37/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 0.7070 - acc: 0.7341 - val_loss: 0.8052 - val_acc: 0.7092\n",
      "Epoch 38/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 0.7066 - acc: 0.7314 - val_loss: 0.7852 - val_acc: 0.7090\n",
      "Epoch 39/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.7019 - acc: 0.7334 - val_loss: 0.8076 - val_acc: 0.7093\n",
      "Epoch 40/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.7014 - acc: 0.7347 - val_loss: 0.7980 - val_acc: 0.7080\n",
      "Epoch 41/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.7004 - acc: 0.7350 - val_loss: 0.8198 - val_acc: 0.7049\n",
      "Epoch 42/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.7013 - acc: 0.7324 - val_loss: 0.8339 - val_acc: 0.7137\n",
      "Epoch 43/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.6968 - acc: 0.7343 - val_loss: 0.8110 - val_acc: 0.7045\n",
      "Epoch 44/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.6990 - acc: 0.7340 - val_loss: 0.7929 - val_acc: 0.7101\n",
      "Epoch 45/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 0.6948 - acc: 0.7347 - val_loss: 0.8054 - val_acc: 0.7100\n",
      "Epoch 46/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.6915 - acc: 0.7356 - val_loss: 0.7882 - val_acc: 0.7105\n",
      "Epoch 47/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 0.6932 - acc: 0.7358 - val_loss: 0.8202 - val_acc: 0.7050\n",
      "Epoch 48/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.6885 - acc: 0.7389 - val_loss: 0.7927 - val_acc: 0.7158\n",
      "Epoch 49/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.6923 - acc: 0.7346 - val_loss: 0.7875 - val_acc: 0.7160\n",
      "Epoch 50/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.6886 - acc: 0.7378 - val_loss: 0.8224 - val_acc: 0.7024\n",
      "Epoch 51/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.6920 - acc: 0.7359 - val_loss: 0.7942 - val_acc: 0.7088\n",
      "Epoch 52/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.6896 - acc: 0.7355 - val_loss: 0.7904 - val_acc: 0.7151\n",
      "Epoch 53/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.6844 - acc: 0.7374 - val_loss: 0.8045 - val_acc: 0.7110\n",
      "Epoch 54/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.6896 - acc: 0.7376 - val_loss: 0.7986 - val_acc: 0.7080\n",
      "Epoch 55/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.6892 - acc: 0.7357 - val_loss: 0.8065 - val_acc: 0.7050\n",
      "Epoch 56/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.6925 - acc: 0.7371 - val_loss: 0.8017 - val_acc: 0.7099\n",
      "Epoch 57/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 0.6859 - acc: 0.7405 - val_loss: 0.8002 - val_acc: 0.7099\n",
      "Epoch 58/100\n",
      "37686/37686 [==============================] - 2s 60us/step - loss: 0.6860 - acc: 0.7382 - val_loss: 0.8005 - val_acc: 0.7116\n",
      "Epoch 59/100\n",
      "37686/37686 [==============================] - 2s 63us/step - loss: 0.6860 - acc: 0.7372 - val_loss: 0.8063 - val_acc: 0.7027\n",
      "Epoch 60/100\n",
      "37686/37686 [==============================] - 3s 74us/step - loss: 0.6799 - acc: 0.7406 - val_loss: 0.8081 - val_acc: 0.7113\n",
      "Epoch 61/100\n",
      "37686/37686 [==============================] - 3s 73us/step - loss: 0.6838 - acc: 0.7396 - val_loss: 0.7975 - val_acc: 0.7138\n",
      "Epoch 62/100\n",
      "37686/37686 [==============================] - 2s 66us/step - loss: 0.6809 - acc: 0.7399 - val_loss: 0.7994 - val_acc: 0.7166\n",
      "Epoch 63/100\n",
      "37686/37686 [==============================] - 3s 78us/step - loss: 0.6787 - acc: 0.7418 - val_loss: 0.7950 - val_acc: 0.7152\n",
      "Epoch 64/100\n",
      "37686/37686 [==============================] - 2s 61us/step - loss: 0.6769 - acc: 0.7423 - val_loss: 0.8303 - val_acc: 0.6960\n",
      "Epoch 65/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.6742 - acc: 0.7435 - val_loss: 0.7962 - val_acc: 0.7097\n",
      "Epoch 66/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.6760 - acc: 0.7416 - val_loss: 0.8052 - val_acc: 0.7231\n",
      "Epoch 67/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.6749 - acc: 0.7431 - val_loss: 0.7870 - val_acc: 0.7129\n",
      "Epoch 68/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.6703 - acc: 0.7427 - val_loss: 0.7906 - val_acc: 0.7196\n",
      "Epoch 69/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.6737 - acc: 0.7434 - val_loss: 0.7885 - val_acc: 0.7159\n",
      "Epoch 70/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.6691 - acc: 0.7475 - val_loss: 0.7850 - val_acc: 0.7236\n",
      "Epoch 71/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.6668 - acc: 0.7461 - val_loss: 0.7806 - val_acc: 0.7284\n",
      "Epoch 72/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.6639 - acc: 0.7478 - val_loss: 0.7887 - val_acc: 0.7286\n",
      "Epoch 73/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.6638 - acc: 0.7446 - val_loss: 0.8185 - val_acc: 0.7132\n",
      "Epoch 74/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.6643 - acc: 0.7478 - val_loss: 0.7857 - val_acc: 0.7167\n",
      "Epoch 75/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.6568 - acc: 0.7489 - val_loss: 0.7866 - val_acc: 0.7204\n",
      "Epoch 76/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.6582 - acc: 0.7505 - val_loss: 0.7804 - val_acc: 0.7204\n",
      "Epoch 77/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.6553 - acc: 0.7515 - val_loss: 0.7724 - val_acc: 0.7302\n",
      "Epoch 78/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.6523 - acc: 0.7513 - val_loss: 0.7903 - val_acc: 0.7231\n",
      "Epoch 79/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 0.6539 - acc: 0.7537 - val_loss: 0.7934 - val_acc: 0.7267\n",
      "Epoch 80/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.6527 - acc: 0.7540 - val_loss: 0.7629 - val_acc: 0.7351\n",
      "Epoch 81/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.6486 - acc: 0.7578 - val_loss: 0.7656 - val_acc: 0.7327\n",
      "Epoch 82/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.6496 - acc: 0.7578 - val_loss: 0.7747 - val_acc: 0.7351\n",
      "Epoch 83/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.6434 - acc: 0.7588 - val_loss: 0.7884 - val_acc: 0.7305\n",
      "Epoch 84/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.6425 - acc: 0.7592 - val_loss: 0.7925 - val_acc: 0.7336\n",
      "Epoch 85/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.6417 - acc: 0.7606 - val_loss: 0.7743 - val_acc: 0.7296\n",
      "Epoch 86/100\n",
      "37686/37686 [==============================] - 2s 60us/step - loss: 0.6393 - acc: 0.7624 - val_loss: 0.7711 - val_acc: 0.7373\n",
      "Epoch 87/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.6377 - acc: 0.7639 - val_loss: 0.7608 - val_acc: 0.7380\n",
      "Epoch 88/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.6369 - acc: 0.7651 - val_loss: 0.7793 - val_acc: 0.7365\n",
      "Epoch 89/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.6351 - acc: 0.7652 - val_loss: 0.7662 - val_acc: 0.7326\n",
      "Epoch 90/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.6288 - acc: 0.7689 - val_loss: 0.7668 - val_acc: 0.7383\n",
      "Epoch 91/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.6336 - acc: 0.7670 - val_loss: 0.7738 - val_acc: 0.7369\n",
      "Epoch 92/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.6279 - acc: 0.7702 - val_loss: 0.7580 - val_acc: 0.7421\n",
      "Epoch 93/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.6229 - acc: 0.7717 - val_loss: 0.7512 - val_acc: 0.7463\n",
      "Epoch 94/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.6243 - acc: 0.7733 - val_loss: 0.7514 - val_acc: 0.7427\n",
      "Epoch 95/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.6195 - acc: 0.7749 - val_loss: 0.7749 - val_acc: 0.7437\n",
      "Epoch 96/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.6173 - acc: 0.7758 - val_loss: 0.7865 - val_acc: 0.7393\n",
      "Epoch 97/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.6188 - acc: 0.7759 - val_loss: 0.7414 - val_acc: 0.7471\n",
      "Epoch 98/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 0.6168 - acc: 0.7776 - val_loss: 0.7611 - val_acc: 0.7451\n",
      "Epoch 99/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 0.6166 - acc: 0.7784 - val_loss: 0.7603 - val_acc: 0.7463\n",
      "Epoch 100/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.6118 - acc: 0.7812 - val_loss: 0.7534 - val_acc: 0.7490\n",
      "Inner step: 5 / 40\n",
      "Train on 37686 samples, validate on 13278 samples\n",
      "Epoch 1/100\n",
      "37686/37686 [==============================] - 2s 63us/step - loss: 1.3077 - acc: 0.4461 - val_loss: 1.1592 - val_acc: 0.5060\n",
      "Epoch 2/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 1.1725 - acc: 0.4936 - val_loss: 1.1398 - val_acc: 0.5106\n",
      "Epoch 3/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 1.1016 - acc: 0.5340 - val_loss: 1.0991 - val_acc: 0.5392\n",
      "Epoch 4/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 1.0707 - acc: 0.5438 - val_loss: 1.1088 - val_acc: 0.5357\n",
      "Epoch 5/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 1.0624 - acc: 0.5478 - val_loss: 1.0596 - val_acc: 0.5562\n",
      "Epoch 6/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 1.0580 - acc: 0.5497 - val_loss: 1.0672 - val_acc: 0.5565\n",
      "Epoch 7/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 1.0563 - acc: 0.5489 - val_loss: 1.0640 - val_acc: 0.5546\n",
      "Epoch 8/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 1.0546 - acc: 0.5531 - val_loss: 1.0611 - val_acc: 0.5615\n",
      "Epoch 9/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37686/37686 [==============================] - 2s 54us/step - loss: 1.0529 - acc: 0.5531 - val_loss: 1.0587 - val_acc: 0.5590\n",
      "Epoch 10/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 1.0510 - acc: 0.5515 - val_loss: 1.0609 - val_acc: 0.5617\n",
      "Epoch 11/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 1.0459 - acc: 0.5562 - val_loss: 1.0985 - val_acc: 0.5686\n",
      "Epoch 12/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 1.0252 - acc: 0.5727 - val_loss: 1.0344 - val_acc: 0.5871\n",
      "Epoch 13/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.9954 - acc: 0.5874 - val_loss: 1.0370 - val_acc: 0.5811\n",
      "Epoch 14/100\n",
      "37686/37686 [==============================] - 3s 69us/step - loss: 0.9804 - acc: 0.5932 - val_loss: 1.0241 - val_acc: 0.5836\n",
      "Epoch 15/100\n",
      "37686/37686 [==============================] - 2s 60us/step - loss: 0.9739 - acc: 0.5961 - val_loss: 1.0509 - val_acc: 0.5767\n",
      "Epoch 16/100\n",
      "37686/37686 [==============================] - 2s 66us/step - loss: 0.9688 - acc: 0.5977 - val_loss: 1.0135 - val_acc: 0.5849\n",
      "Epoch 17/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.9636 - acc: 0.5998 - val_loss: 1.0172 - val_acc: 0.6005\n",
      "Epoch 18/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.9536 - acc: 0.6083 - val_loss: 1.0083 - val_acc: 0.5955\n",
      "Epoch 19/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.9482 - acc: 0.6144 - val_loss: 0.9878 - val_acc: 0.6118\n",
      "Epoch 20/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.9394 - acc: 0.6186 - val_loss: 0.9937 - val_acc: 0.6042\n",
      "Epoch 21/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.9421 - acc: 0.6202 - val_loss: 0.9923 - val_acc: 0.6083\n",
      "Epoch 22/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 0.9324 - acc: 0.6253 - val_loss: 1.0152 - val_acc: 0.6095\n",
      "Epoch 23/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.9242 - acc: 0.6302 - val_loss: 0.9848 - val_acc: 0.6312\n",
      "Epoch 24/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.9170 - acc: 0.6354 - val_loss: 0.9881 - val_acc: 0.6268\n",
      "Epoch 25/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.9092 - acc: 0.6360 - val_loss: 0.9885 - val_acc: 0.6164\n",
      "Epoch 26/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.9042 - acc: 0.6390 - val_loss: 0.9622 - val_acc: 0.6290\n",
      "Epoch 27/100\n",
      "37686/37686 [==============================] - 2s 64us/step - loss: 0.9053 - acc: 0.6403 - val_loss: 0.9766 - val_acc: 0.6234\n",
      "Epoch 28/100\n",
      "37686/37686 [==============================] - 3s 70us/step - loss: 0.9006 - acc: 0.6433 - val_loss: 0.9566 - val_acc: 0.6354\n",
      "Epoch 29/100\n",
      "37686/37686 [==============================] - 3s 68us/step - loss: 0.8973 - acc: 0.6425 - val_loss: 0.9726 - val_acc: 0.6268\n",
      "Epoch 30/100\n",
      "37686/37686 [==============================] - 3s 70us/step - loss: 0.8937 - acc: 0.6442 - val_loss: 0.9527 - val_acc: 0.6335\n",
      "Epoch 31/100\n",
      "37686/37686 [==============================] - 3s 67us/step - loss: 0.8569 - acc: 0.6670 - val_loss: 0.9138 - val_acc: 0.6582\n",
      "Epoch 32/100\n",
      "37686/37686 [==============================] - 3s 70us/step - loss: 0.8242 - acc: 0.7013 - val_loss: 0.8976 - val_acc: 0.6905\n",
      "Epoch 33/100\n",
      "37686/37686 [==============================] - 3s 68us/step - loss: 0.8089 - acc: 0.7129 - val_loss: 0.8811 - val_acc: 0.7063\n",
      "Epoch 34/100\n",
      "37686/37686 [==============================] - 3s 73us/step - loss: 0.7909 - acc: 0.7176 - val_loss: 0.9678 - val_acc: 0.6393\n",
      "Epoch 35/100\n",
      "37686/37686 [==============================] - 3s 70us/step - loss: 0.7792 - acc: 0.7216 - val_loss: 0.9067 - val_acc: 0.6790\n",
      "Epoch 36/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.7435 - acc: 0.7377 - val_loss: 0.8521 - val_acc: 0.7184\n",
      "Epoch 37/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.7030 - acc: 0.7542 - val_loss: 0.7873 - val_acc: 0.7319\n",
      "Epoch 38/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.6837 - acc: 0.7616 - val_loss: 0.7891 - val_acc: 0.7350\n",
      "Epoch 39/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 0.6762 - acc: 0.7632 - val_loss: 0.7963 - val_acc: 0.7344\n",
      "Epoch 40/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.6660 - acc: 0.7674 - val_loss: 0.7697 - val_acc: 0.7424\n",
      "Epoch 41/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.6682 - acc: 0.7680 - val_loss: 0.7898 - val_acc: 0.7366\n",
      "Epoch 42/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.6628 - acc: 0.7674 - val_loss: 0.7890 - val_acc: 0.7329\n",
      "Epoch 43/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.6562 - acc: 0.7727 - val_loss: 0.7722 - val_acc: 0.7390\n",
      "Epoch 44/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.6528 - acc: 0.7724 - val_loss: 0.7912 - val_acc: 0.7399\n",
      "Epoch 45/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.6505 - acc: 0.7712 - val_loss: 0.7721 - val_acc: 0.7418\n",
      "Epoch 46/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.6505 - acc: 0.7726 - val_loss: 0.8137 - val_acc: 0.7254\n",
      "Epoch 47/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 0.6478 - acc: 0.7751 - val_loss: 0.7423 - val_acc: 0.7491\n",
      "Epoch 48/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.6404 - acc: 0.7752 - val_loss: 0.8006 - val_acc: 0.7202\n",
      "Epoch 49/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.6417 - acc: 0.7746 - val_loss: 0.7518 - val_acc: 0.7472\n",
      "Epoch 50/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.6313 - acc: 0.7784 - val_loss: 0.7438 - val_acc: 0.7458\n",
      "Epoch 51/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.6302 - acc: 0.7775 - val_loss: 0.7681 - val_acc: 0.7413\n",
      "Epoch 52/100\n",
      "37686/37686 [==============================] - 2s 65us/step - loss: 0.6258 - acc: 0.7798 - val_loss: 0.7287 - val_acc: 0.7548\n",
      "Epoch 53/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.6228 - acc: 0.7797 - val_loss: 0.7455 - val_acc: 0.7481\n",
      "Epoch 54/100\n",
      "37686/37686 [==============================] - 2s 61us/step - loss: 0.6211 - acc: 0.7826 - val_loss: 0.7493 - val_acc: 0.7408\n",
      "Epoch 55/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.6162 - acc: 0.7824 - val_loss: 0.7328 - val_acc: 0.7494\n",
      "Epoch 56/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.6143 - acc: 0.7812 - val_loss: 0.7424 - val_acc: 0.7466\n",
      "Epoch 57/100\n",
      "37686/37686 [==============================] - 2s 63us/step - loss: 0.6125 - acc: 0.7831 - val_loss: 0.7458 - val_acc: 0.7439\n",
      "Epoch 58/100\n",
      "37686/37686 [==============================] - 3s 69us/step - loss: 0.6059 - acc: 0.7841 - val_loss: 0.7317 - val_acc: 0.7485\n",
      "Epoch 59/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.6042 - acc: 0.7838 - val_loss: 0.7329 - val_acc: 0.7479\n",
      "Epoch 60/100\n",
      "37686/37686 [==============================] - 2s 61us/step - loss: 0.6058 - acc: 0.7852 - val_loss: 0.7169 - val_acc: 0.7469\n",
      "Epoch 61/100\n",
      "37686/37686 [==============================] - 2s 64us/step - loss: 0.6021 - acc: 0.7859 - val_loss: 0.7094 - val_acc: 0.7592\n",
      "Epoch 62/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.6077 - acc: 0.7839 - val_loss: 0.7161 - val_acc: 0.7539\n",
      "Epoch 63/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.5988 - acc: 0.7859 - val_loss: 0.7348 - val_acc: 0.7497\n",
      "Epoch 64/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.5970 - acc: 0.7862 - val_loss: 0.7093 - val_acc: 0.7578\n",
      "Epoch 65/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.5963 - acc: 0.7855 - val_loss: 0.7196 - val_acc: 0.7489\n",
      "Epoch 66/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.5966 - acc: 0.7870 - val_loss: 0.7167 - val_acc: 0.7488\n",
      "Epoch 67/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.5923 - acc: 0.7863 - val_loss: 0.7295 - val_acc: 0.7495\n",
      "Epoch 68/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37686/37686 [==============================] - 2s 60us/step - loss: 0.5904 - acc: 0.7874 - val_loss: 0.7200 - val_acc: 0.7547\n",
      "Epoch 69/100\n",
      "37686/37686 [==============================] - 3s 90us/step - loss: 0.5933 - acc: 0.7874 - val_loss: 0.7136 - val_acc: 0.7564\n",
      "Epoch 70/100\n",
      "37686/37686 [==============================] - 3s 71us/step - loss: 0.5876 - acc: 0.7890 - val_loss: 0.7267 - val_acc: 0.7474\n",
      "Epoch 71/100\n",
      "37686/37686 [==============================] - 3s 69us/step - loss: 0.5868 - acc: 0.7891 - val_loss: 0.7030 - val_acc: 0.7621\n",
      "Epoch 72/100\n",
      "37686/37686 [==============================] - 2s 61us/step - loss: 0.5864 - acc: 0.7899 - val_loss: 0.7515 - val_acc: 0.7503\n",
      "Epoch 73/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.5858 - acc: 0.7891 - val_loss: 0.7292 - val_acc: 0.7460\n",
      "Epoch 74/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.5833 - acc: 0.7900 - val_loss: 0.7271 - val_acc: 0.7537\n",
      "Epoch 75/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.5870 - acc: 0.7889 - val_loss: 0.7293 - val_acc: 0.7517\n",
      "Epoch 76/100\n",
      "37686/37686 [==============================] - 2s 65us/step - loss: 0.5862 - acc: 0.7887 - val_loss: 0.7229 - val_acc: 0.7557\n",
      "Epoch 77/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.5864 - acc: 0.7893 - val_loss: 0.7087 - val_acc: 0.7540\n",
      "Epoch 78/100\n",
      "37686/37686 [==============================] - 3s 67us/step - loss: 0.5805 - acc: 0.7904 - val_loss: 0.7256 - val_acc: 0.7536\n",
      "Epoch 79/100\n",
      "37686/37686 [==============================] - 2s 60us/step - loss: 0.5753 - acc: 0.7914 - val_loss: 0.7251 - val_acc: 0.7494\n",
      "Epoch 80/100\n",
      "37686/37686 [==============================] - 3s 67us/step - loss: 0.5796 - acc: 0.7910 - val_loss: 0.7102 - val_acc: 0.7515\n",
      "Epoch 81/100\n",
      "37686/37686 [==============================] - 3s 68us/step - loss: 0.5752 - acc: 0.7925 - val_loss: 0.7311 - val_acc: 0.7545\n",
      "Epoch 82/100\n",
      "37686/37686 [==============================] - 2s 61us/step - loss: 0.5770 - acc: 0.7923 - val_loss: 0.7112 - val_acc: 0.7607\n",
      "Epoch 83/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.5736 - acc: 0.7930 - val_loss: 0.7341 - val_acc: 0.7489\n",
      "Epoch 84/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.5766 - acc: 0.7919 - val_loss: 0.7486 - val_acc: 0.7447\n",
      "Epoch 85/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.5716 - acc: 0.7919 - val_loss: 0.7112 - val_acc: 0.7607\n",
      "Epoch 86/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 0.5707 - acc: 0.7931 - val_loss: 0.7408 - val_acc: 0.7543\n",
      "Epoch 87/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.5693 - acc: 0.7941 - val_loss: 0.7274 - val_acc: 0.7591\n",
      "Epoch 88/100\n",
      "37686/37686 [==============================] - 2s 60us/step - loss: 0.5676 - acc: 0.7945 - val_loss: 0.7127 - val_acc: 0.7588\n",
      "Epoch 89/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.5717 - acc: 0.7937 - val_loss: 0.7118 - val_acc: 0.7554\n",
      "Epoch 90/100\n",
      "37686/37686 [==============================] - 2s 63us/step - loss: 0.5700 - acc: 0.7902 - val_loss: 0.7302 - val_acc: 0.7451\n",
      "Epoch 91/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.5695 - acc: 0.7942 - val_loss: 0.7075 - val_acc: 0.7547\n",
      "Epoch 92/100\n",
      "37686/37686 [==============================] - 3s 69us/step - loss: 0.5671 - acc: 0.7952 - val_loss: 0.7375 - val_acc: 0.7551\n",
      "Epoch 93/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.5658 - acc: 0.7941 - val_loss: 0.7388 - val_acc: 0.7497\n",
      "Epoch 94/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.5631 - acc: 0.7955 - val_loss: 0.7149 - val_acc: 0.7640\n",
      "Epoch 95/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 0.5625 - acc: 0.7968 - val_loss: 0.7633 - val_acc: 0.7525\n",
      "Epoch 96/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.5596 - acc: 0.7956 - val_loss: 0.7315 - val_acc: 0.7595\n",
      "Epoch 97/100\n",
      "37686/37686 [==============================] - 2s 61us/step - loss: 0.5589 - acc: 0.7979 - val_loss: 0.7249 - val_acc: 0.7580\n",
      "Epoch 98/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.5639 - acc: 0.7945 - val_loss: 0.7003 - val_acc: 0.7637\n",
      "Epoch 99/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.5616 - acc: 0.7966 - val_loss: 0.7317 - val_acc: 0.7522\n",
      "Epoch 100/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 0.5549 - acc: 0.7991 - val_loss: 0.7259 - val_acc: 0.7579\n",
      "Inner step: 6 / 40\n",
      "Train on 37686 samples, validate on 13278 samples\n",
      "Epoch 1/100\n",
      "37686/37686 [==============================] - 2s 63us/step - loss: 1.4039 - acc: 0.4116 - val_loss: 1.3723 - val_acc: 0.4274\n",
      "Epoch 2/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 1.3555 - acc: 0.4254 - val_loss: 1.3658 - val_acc: 0.4367\n",
      "Epoch 3/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 1.3249 - acc: 0.4478 - val_loss: 1.3123 - val_acc: 0.4672\n",
      "Epoch 4/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 1.2668 - acc: 0.4880 - val_loss: 1.2826 - val_acc: 0.4901\n",
      "Epoch 5/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 1.2538 - acc: 0.4928 - val_loss: 1.2904 - val_acc: 0.4918\n",
      "Epoch 6/100\n",
      "37686/37686 [==============================] - 2s 60us/step - loss: 1.2435 - acc: 0.4986 - val_loss: 1.2857 - val_acc: 0.4950\n",
      "Epoch 7/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 1.2418 - acc: 0.4963 - val_loss: 1.2781 - val_acc: 0.4844\n",
      "Epoch 8/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 1.2360 - acc: 0.4998 - val_loss: 1.3649 - val_acc: 0.4568\n",
      "Epoch 9/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 1.2299 - acc: 0.5016 - val_loss: 1.2631 - val_acc: 0.5063\n",
      "Epoch 10/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 1.1976 - acc: 0.5139 - val_loss: 1.2501 - val_acc: 0.5115\n",
      "Epoch 11/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 1.1878 - acc: 0.5204 - val_loss: 1.2349 - val_acc: 0.5204\n",
      "Epoch 12/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 1.1747 - acc: 0.5315 - val_loss: 1.2527 - val_acc: 0.5207\n",
      "Epoch 13/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 1.1643 - acc: 0.5384 - val_loss: 1.2468 - val_acc: 0.5294\n",
      "Epoch 14/100\n",
      "37686/37686 [==============================] - 2s 64us/step - loss: 1.1537 - acc: 0.5421 - val_loss: 1.2125 - val_acc: 0.5328\n",
      "Epoch 15/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 1.1462 - acc: 0.5455 - val_loss: 1.2170 - val_acc: 0.5325\n",
      "Epoch 16/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 1.1448 - acc: 0.5472 - val_loss: 1.2366 - val_acc: 0.5352\n",
      "Epoch 17/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 1.1420 - acc: 0.5479 - val_loss: 1.2351 - val_acc: 0.5312\n",
      "Epoch 18/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 1.1367 - acc: 0.5505 - val_loss: 1.2129 - val_acc: 0.5355\n",
      "Epoch 19/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 1.1362 - acc: 0.5492 - val_loss: 1.2090 - val_acc: 0.5335\n",
      "Epoch 20/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 1.1318 - acc: 0.5514 - val_loss: 1.2040 - val_acc: 0.5369\n",
      "Epoch 21/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 1.1310 - acc: 0.5527 - val_loss: 1.2375 - val_acc: 0.5330\n",
      "Epoch 22/100\n",
      "37686/37686 [==============================] - 2s 64us/step - loss: 1.1281 - acc: 0.5535 - val_loss: 1.1946 - val_acc: 0.5438\n",
      "Epoch 23/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 1.1267 - acc: 0.5534 - val_loss: 1.2199 - val_acc: 0.5365\n",
      "Epoch 24/100\n",
      "37686/37686 [==============================] - 2s 60us/step - loss: 1.1225 - acc: 0.5544 - val_loss: 1.1965 - val_acc: 0.5389\n",
      "Epoch 25/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 1.1183 - acc: 0.5548 - val_loss: 1.1845 - val_acc: 0.5421\n",
      "Epoch 26/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37686/37686 [==============================] - 2s 59us/step - loss: 1.0783 - acc: 0.5611 - val_loss: 1.1336 - val_acc: 0.5679\n",
      "Epoch 27/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 1.0310 - acc: 0.5907 - val_loss: 1.1173 - val_acc: 0.5836\n",
      "Epoch 28/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.9927 - acc: 0.6069 - val_loss: 1.0713 - val_acc: 0.5921\n",
      "Epoch 29/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.9655 - acc: 0.6251 - val_loss: 1.0249 - val_acc: 0.6148\n",
      "Epoch 30/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.9347 - acc: 0.6275 - val_loss: 1.0076 - val_acc: 0.6130\n",
      "Epoch 31/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.9203 - acc: 0.6300 - val_loss: 1.0260 - val_acc: 0.6021\n",
      "Epoch 32/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.9084 - acc: 0.6350 - val_loss: 1.0195 - val_acc: 0.6130\n",
      "Epoch 33/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.9007 - acc: 0.6432 - val_loss: 0.9735 - val_acc: 0.6274\n",
      "Epoch 34/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.8792 - acc: 0.6599 - val_loss: 0.9585 - val_acc: 0.6454\n",
      "Epoch 35/100\n",
      "37686/37686 [==============================] - 2s 60us/step - loss: 0.8471 - acc: 0.6793 - val_loss: 0.9143 - val_acc: 0.6652\n",
      "Epoch 36/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.8198 - acc: 0.6889 - val_loss: 0.8981 - val_acc: 0.6689\n",
      "Epoch 37/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.8076 - acc: 0.6924 - val_loss: 0.9117 - val_acc: 0.6715\n",
      "Epoch 38/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.7964 - acc: 0.6967 - val_loss: 0.9124 - val_acc: 0.6617\n",
      "Epoch 39/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.7873 - acc: 0.6999 - val_loss: 0.9304 - val_acc: 0.6624\n",
      "Epoch 40/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.7879 - acc: 0.7017 - val_loss: 0.9022 - val_acc: 0.6730\n",
      "Epoch 41/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.7833 - acc: 0.7028 - val_loss: 0.9113 - val_acc: 0.6612\n",
      "Epoch 42/100\n",
      "37686/37686 [==============================] - 2s 66us/step - loss: 0.7760 - acc: 0.7043 - val_loss: 0.8744 - val_acc: 0.6762\n",
      "Epoch 43/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.7700 - acc: 0.7062 - val_loss: 0.9110 - val_acc: 0.6628\n",
      "Epoch 44/100\n",
      "37686/37686 [==============================] - 2s 61us/step - loss: 0.7711 - acc: 0.7076 - val_loss: 0.8898 - val_acc: 0.6734\n",
      "Epoch 45/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.7676 - acc: 0.7085 - val_loss: 0.8774 - val_acc: 0.6856\n",
      "Epoch 46/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.7620 - acc: 0.7085 - val_loss: 0.8654 - val_acc: 0.6841\n",
      "Epoch 47/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.7594 - acc: 0.7106 - val_loss: 0.8604 - val_acc: 0.6877\n",
      "Epoch 48/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.7560 - acc: 0.7104 - val_loss: 0.8642 - val_acc: 0.6833\n",
      "Epoch 49/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.7590 - acc: 0.7096 - val_loss: 0.8558 - val_acc: 0.6820\n",
      "Epoch 50/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.7519 - acc: 0.7136 - val_loss: 0.8675 - val_acc: 0.6830\n",
      "Epoch 51/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.7474 - acc: 0.7140 - val_loss: 0.8763 - val_acc: 0.6805\n",
      "Epoch 52/100\n",
      "37686/37686 [==============================] - 2s 60us/step - loss: 0.7554 - acc: 0.7125 - val_loss: 0.9055 - val_acc: 0.6783\n",
      "Epoch 53/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.7468 - acc: 0.7142 - val_loss: 0.8613 - val_acc: 0.6793\n",
      "Epoch 54/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.7457 - acc: 0.7152 - val_loss: 0.8550 - val_acc: 0.6876\n",
      "Epoch 55/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.7497 - acc: 0.7127 - val_loss: 0.8549 - val_acc: 0.6799\n",
      "Epoch 56/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.7393 - acc: 0.7155 - val_loss: 0.8784 - val_acc: 0.6730\n",
      "Epoch 57/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.7421 - acc: 0.7153 - val_loss: 0.8988 - val_acc: 0.6702\n",
      "Epoch 58/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.7393 - acc: 0.7170 - val_loss: 0.8469 - val_acc: 0.6839\n",
      "Epoch 59/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.7389 - acc: 0.7159 - val_loss: 0.8641 - val_acc: 0.6873\n",
      "Epoch 60/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.7348 - acc: 0.7167 - val_loss: 0.8849 - val_acc: 0.6761\n",
      "Epoch 61/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.7338 - acc: 0.7191 - val_loss: 0.8692 - val_acc: 0.6773\n",
      "Epoch 62/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.7309 - acc: 0.7184 - val_loss: 0.8449 - val_acc: 0.6841\n",
      "Epoch 63/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.7126 - acc: 0.7321 - val_loss: 0.8312 - val_acc: 0.7177\n",
      "Epoch 64/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.6624 - acc: 0.7650 - val_loss: 0.7546 - val_acc: 0.7398\n",
      "Epoch 65/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.6440 - acc: 0.7710 - val_loss: 0.7789 - val_acc: 0.7410\n",
      "Epoch 66/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.6410 - acc: 0.7755 - val_loss: 0.7607 - val_acc: 0.7488\n",
      "Epoch 67/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.6342 - acc: 0.7752 - val_loss: 0.7486 - val_acc: 0.7408\n",
      "Epoch 68/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 0.6350 - acc: 0.7763 - val_loss: 0.7876 - val_acc: 0.7320\n",
      "Epoch 69/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.6232 - acc: 0.7792 - val_loss: 0.7610 - val_acc: 0.7406\n",
      "Epoch 70/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.6238 - acc: 0.7794 - val_loss: 0.7566 - val_acc: 0.7418\n",
      "Epoch 71/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.6190 - acc: 0.7798 - val_loss: 0.7603 - val_acc: 0.7401\n",
      "Epoch 72/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.6186 - acc: 0.7808 - val_loss: 0.7485 - val_acc: 0.7409\n",
      "Epoch 73/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.6145 - acc: 0.7819 - val_loss: 0.7832 - val_acc: 0.7348\n",
      "Epoch 74/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.6187 - acc: 0.7810 - val_loss: 0.7544 - val_acc: 0.7430\n",
      "Epoch 75/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.6122 - acc: 0.7824 - val_loss: 0.7227 - val_acc: 0.7506\n",
      "Epoch 76/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.6118 - acc: 0.7834 - val_loss: 0.7431 - val_acc: 0.7510\n",
      "Epoch 77/100\n",
      "37686/37686 [==============================] - 2s 62us/step - loss: 0.6115 - acc: 0.7840 - val_loss: 0.7525 - val_acc: 0.7427\n",
      "Epoch 78/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.6121 - acc: 0.7848 - val_loss: 0.7343 - val_acc: 0.7509\n",
      "Epoch 79/100\n",
      "37686/37686 [==============================] - 2s 60us/step - loss: 0.6082 - acc: 0.7845 - val_loss: 0.7506 - val_acc: 0.7451\n",
      "Epoch 80/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.6086 - acc: 0.7845 - val_loss: 0.7892 - val_acc: 0.7379\n",
      "Epoch 81/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.6081 - acc: 0.7843 - val_loss: 0.7305 - val_acc: 0.7489\n",
      "Epoch 82/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.6051 - acc: 0.7856 - val_loss: 0.7560 - val_acc: 0.7444\n",
      "Epoch 83/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.6025 - acc: 0.7846 - val_loss: 0.7477 - val_acc: 0.7503\n",
      "Epoch 84/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.6005 - acc: 0.7868 - val_loss: 0.7540 - val_acc: 0.7399\n",
      "Epoch 85/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.6016 - acc: 0.7868 - val_loss: 0.7642 - val_acc: 0.7465\n",
      "Epoch 86/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.6006 - acc: 0.7864 - val_loss: 0.7301 - val_acc: 0.7521\n",
      "Epoch 87/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.5992 - acc: 0.7879 - val_loss: 0.7559 - val_acc: 0.7486\n",
      "Epoch 88/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.5995 - acc: 0.7865 - val_loss: 0.7548 - val_acc: 0.7451\n",
      "Epoch 89/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.5995 - acc: 0.7885 - val_loss: 0.7444 - val_acc: 0.7492\n",
      "Epoch 90/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 0.5938 - acc: 0.7892 - val_loss: 0.7282 - val_acc: 0.7528\n",
      "Epoch 91/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.5974 - acc: 0.7875 - val_loss: 0.7475 - val_acc: 0.7454\n",
      "Epoch 92/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.5953 - acc: 0.7888 - val_loss: 0.7527 - val_acc: 0.7509\n",
      "Epoch 93/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.5946 - acc: 0.7885 - val_loss: 0.7567 - val_acc: 0.7454\n",
      "Epoch 94/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.5970 - acc: 0.7892 - val_loss: 0.7589 - val_acc: 0.7441\n",
      "Epoch 95/100\n",
      "37686/37686 [==============================] - 2s 60us/step - loss: 0.5869 - acc: 0.7910 - val_loss: 0.7424 - val_acc: 0.7534\n",
      "Epoch 96/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.5977 - acc: 0.7876 - val_loss: 0.7388 - val_acc: 0.7552\n",
      "Epoch 97/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.5852 - acc: 0.7905 - val_loss: 0.7542 - val_acc: 0.7482\n",
      "Epoch 98/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.5945 - acc: 0.7899 - val_loss: 0.7198 - val_acc: 0.7566\n",
      "Epoch 99/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.5904 - acc: 0.7922 - val_loss: 0.7588 - val_acc: 0.7455\n",
      "Epoch 100/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.5845 - acc: 0.7933 - val_loss: 0.7661 - val_acc: 0.7433\n",
      "Inner step: 7 / 40\n",
      "Train on 37686 samples, validate on 13278 samples\n",
      "Epoch 1/100\n",
      "37686/37686 [==============================] - 2s 65us/step - loss: 1.2507 - acc: 0.4737 - val_loss: 1.1051 - val_acc: 0.5320\n",
      "Epoch 2/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 1.0883 - acc: 0.5310 - val_loss: 1.1037 - val_acc: 0.5244\n",
      "Epoch 3/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 1.0695 - acc: 0.5408 - val_loss: 1.0717 - val_acc: 0.5417\n",
      "Epoch 4/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 1.0458 - acc: 0.5603 - val_loss: 1.0446 - val_acc: 0.5779\n",
      "Epoch 5/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 1.0043 - acc: 0.5814 - val_loss: 1.0557 - val_acc: 0.5626\n",
      "Epoch 6/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.9815 - acc: 0.5880 - val_loss: 1.0350 - val_acc: 0.5805\n",
      "Epoch 7/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.9762 - acc: 0.5915 - val_loss: 1.0446 - val_acc: 0.5716\n",
      "Epoch 8/100\n",
      "37686/37686 [==============================] - 2s 62us/step - loss: 0.9690 - acc: 0.5950 - val_loss: 1.0106 - val_acc: 0.5914\n",
      "Epoch 9/100\n",
      "37686/37686 [==============================] - 2s 63us/step - loss: 0.9620 - acc: 0.5926 - val_loss: 1.0188 - val_acc: 0.5813\n",
      "Epoch 10/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.9394 - acc: 0.6109 - val_loss: 0.9594 - val_acc: 0.6229\n",
      "Epoch 11/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.8864 - acc: 0.6439 - val_loss: 0.9156 - val_acc: 0.6365\n",
      "Epoch 12/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.8607 - acc: 0.6490 - val_loss: 0.9074 - val_acc: 0.6384\n",
      "Epoch 13/100\n",
      "37686/37686 [==============================] - 2s 61us/step - loss: 0.8525 - acc: 0.6507 - val_loss: 0.9158 - val_acc: 0.6228\n",
      "Epoch 14/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.8526 - acc: 0.6530 - val_loss: 0.9223 - val_acc: 0.6274\n",
      "Epoch 15/100\n",
      "37686/37686 [==============================] - 2s 60us/step - loss: 0.8435 - acc: 0.6557 - val_loss: 0.9143 - val_acc: 0.6463\n",
      "Epoch 16/100\n",
      "37686/37686 [==============================] - 2s 60us/step - loss: 0.8381 - acc: 0.6546 - val_loss: 0.9364 - val_acc: 0.6357\n",
      "Epoch 17/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.8364 - acc: 0.6573 - val_loss: 0.9008 - val_acc: 0.6426\n",
      "Epoch 18/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.8321 - acc: 0.6566 - val_loss: 0.9181 - val_acc: 0.6357\n",
      "Epoch 19/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.8287 - acc: 0.6596 - val_loss: 0.9020 - val_acc: 0.6439\n",
      "Epoch 20/100\n",
      "37686/37686 [==============================] - 2s 60us/step - loss: 0.8203 - acc: 0.6715 - val_loss: 0.9574 - val_acc: 0.6051\n",
      "Epoch 21/100\n",
      "37686/37686 [==============================] - 2s 62us/step - loss: 0.7964 - acc: 0.6958 - val_loss: 0.8481 - val_acc: 0.6953\n",
      "Epoch 22/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.7693 - acc: 0.7071 - val_loss: 0.8318 - val_acc: 0.6938\n",
      "Epoch 23/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.7583 - acc: 0.7111 - val_loss: 0.8576 - val_acc: 0.6838\n",
      "Epoch 24/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.7484 - acc: 0.7171 - val_loss: 0.8449 - val_acc: 0.6876\n",
      "Epoch 25/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.7436 - acc: 0.7157 - val_loss: 0.8200 - val_acc: 0.7047\n",
      "Epoch 26/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.7398 - acc: 0.7198 - val_loss: 0.8193 - val_acc: 0.7025\n",
      "Epoch 27/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.7388 - acc: 0.7201 - val_loss: 0.8235 - val_acc: 0.7048\n",
      "Epoch 28/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.7354 - acc: 0.7219 - val_loss: 0.9166 - val_acc: 0.6695\n",
      "Epoch 29/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.7361 - acc: 0.7223 - val_loss: 0.9166 - val_acc: 0.6607\n",
      "Epoch 30/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.7305 - acc: 0.7241 - val_loss: 0.8393 - val_acc: 0.6986\n",
      "Epoch 31/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.7322 - acc: 0.7232 - val_loss: 0.8262 - val_acc: 0.7031\n",
      "Epoch 32/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.7321 - acc: 0.7215 - val_loss: 0.8107 - val_acc: 0.7036\n",
      "Epoch 33/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.7295 - acc: 0.7240 - val_loss: 0.8352 - val_acc: 0.6925\n",
      "Epoch 34/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.7297 - acc: 0.7234 - val_loss: 0.8641 - val_acc: 0.6863\n",
      "Epoch 35/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.7296 - acc: 0.7226 - val_loss: 0.8227 - val_acc: 0.7030\n",
      "Epoch 36/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.7267 - acc: 0.7254 - val_loss: 0.8265 - val_acc: 0.6971\n",
      "Epoch 37/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.7272 - acc: 0.7255 - val_loss: 0.8336 - val_acc: 0.6896\n",
      "Epoch 38/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.7231 - acc: 0.7262 - val_loss: 0.8375 - val_acc: 0.6997\n",
      "Epoch 39/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.7256 - acc: 0.7264 - val_loss: 0.8263 - val_acc: 0.6994\n",
      "Epoch 40/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.7230 - acc: 0.7272 - val_loss: 0.8783 - val_acc: 0.6880\n",
      "Epoch 41/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.7253 - acc: 0.7268 - val_loss: 0.8267 - val_acc: 0.7077\n",
      "Epoch 42/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.7202 - acc: 0.7262 - val_loss: 0.8460 - val_acc: 0.6976\n",
      "Epoch 43/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.7218 - acc: 0.7262 - val_loss: 0.8289 - val_acc: 0.7033\n",
      "Epoch 44/100\n",
      "37686/37686 [==============================] - 2s 64us/step - loss: 0.7196 - acc: 0.7276 - val_loss: 0.8779 - val_acc: 0.6808\n",
      "Epoch 45/100\n",
      "37686/37686 [==============================] - 3s 68us/step - loss: 0.7180 - acc: 0.7262 - val_loss: 0.8359 - val_acc: 0.6990\n",
      "Epoch 46/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.7195 - acc: 0.7278 - val_loss: 0.8479 - val_acc: 0.6918\n",
      "Epoch 47/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.7196 - acc: 0.7272 - val_loss: 0.8468 - val_acc: 0.6988\n",
      "Epoch 48/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.7189 - acc: 0.7271 - val_loss: 0.8193 - val_acc: 0.7024\n",
      "Epoch 49/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.7133 - acc: 0.7288 - val_loss: 0.8354 - val_acc: 0.6972\n",
      "Epoch 50/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.7097 - acc: 0.7294 - val_loss: 0.8126 - val_acc: 0.7045\n",
      "Epoch 51/100\n",
      "37686/37686 [==============================] - 2s 60us/step - loss: 0.7118 - acc: 0.7301 - val_loss: 0.8757 - val_acc: 0.6757\n",
      "Epoch 52/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.7090 - acc: 0.7324 - val_loss: 0.8645 - val_acc: 0.6829\n",
      "Epoch 53/100\n",
      "37686/37686 [==============================] - 2s 62us/step - loss: 0.7088 - acc: 0.7304 - val_loss: 0.8215 - val_acc: 0.7011\n",
      "Epoch 54/100\n",
      "37686/37686 [==============================] - 2s 60us/step - loss: 0.7105 - acc: 0.7306 - val_loss: 0.8166 - val_acc: 0.7074\n",
      "Epoch 55/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.7126 - acc: 0.7302 - val_loss: 0.8324 - val_acc: 0.6936\n",
      "Epoch 56/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.7122 - acc: 0.7301 - val_loss: 0.8241 - val_acc: 0.7042\n",
      "Epoch 57/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.7081 - acc: 0.7314 - val_loss: 0.8090 - val_acc: 0.7037\n",
      "Epoch 58/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.7066 - acc: 0.7320 - val_loss: 0.8252 - val_acc: 0.7020\n",
      "Epoch 59/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.7102 - acc: 0.7308 - val_loss: 0.8139 - val_acc: 0.7044\n",
      "Epoch 60/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.7039 - acc: 0.7323 - val_loss: 0.8579 - val_acc: 0.6958\n",
      "Epoch 61/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.7090 - acc: 0.7298 - val_loss: 0.8251 - val_acc: 0.6963\n",
      "Epoch 62/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.7060 - acc: 0.7339 - val_loss: 0.8284 - val_acc: 0.6949\n",
      "Epoch 63/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.7051 - acc: 0.7350 - val_loss: 0.8167 - val_acc: 0.7058\n",
      "Epoch 64/100\n",
      "37686/37686 [==============================] - 2s 60us/step - loss: 0.7054 - acc: 0.7330 - val_loss: 0.8176 - val_acc: 0.7029\n",
      "Epoch 65/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.7055 - acc: 0.7334 - val_loss: 0.8146 - val_acc: 0.7009\n",
      "Epoch 66/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.7005 - acc: 0.7346 - val_loss: 0.8234 - val_acc: 0.7054\n",
      "Epoch 67/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.7007 - acc: 0.7338 - val_loss: 0.8245 - val_acc: 0.6998\n",
      "Epoch 68/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.6999 - acc: 0.7339 - val_loss: 0.8413 - val_acc: 0.6960\n",
      "Epoch 69/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.7002 - acc: 0.7344 - val_loss: 0.8159 - val_acc: 0.7096\n",
      "Epoch 70/100\n",
      "37686/37686 [==============================] - 2s 63us/step - loss: 0.6969 - acc: 0.7348 - val_loss: 0.8022 - val_acc: 0.7102\n",
      "Epoch 71/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.6958 - acc: 0.7354 - val_loss: 0.8057 - val_acc: 0.7071\n",
      "Epoch 72/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.6961 - acc: 0.7354 - val_loss: 0.8307 - val_acc: 0.7014\n",
      "Epoch 73/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.6955 - acc: 0.7352 - val_loss: 0.8208 - val_acc: 0.7036\n",
      "Epoch 74/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.6937 - acc: 0.7365 - val_loss: 0.8018 - val_acc: 0.7151\n",
      "Epoch 75/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.6898 - acc: 0.7386 - val_loss: 0.8329 - val_acc: 0.7091\n",
      "Epoch 76/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.6894 - acc: 0.7380 - val_loss: 0.8175 - val_acc: 0.7156\n",
      "Epoch 77/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.6896 - acc: 0.7380 - val_loss: 0.8297 - val_acc: 0.6965\n",
      "Epoch 78/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.6848 - acc: 0.7388 - val_loss: 0.7949 - val_acc: 0.7133\n",
      "Epoch 79/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.6853 - acc: 0.7398 - val_loss: 0.8060 - val_acc: 0.7174\n",
      "Epoch 80/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.6847 - acc: 0.7405 - val_loss: 0.7762 - val_acc: 0.7220\n",
      "Epoch 81/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.6828 - acc: 0.7410 - val_loss: 0.7997 - val_acc: 0.7149\n",
      "Epoch 82/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.6805 - acc: 0.7402 - val_loss: 0.7980 - val_acc: 0.7122\n",
      "Epoch 83/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.6816 - acc: 0.7419 - val_loss: 0.7891 - val_acc: 0.7134\n",
      "Epoch 84/100\n",
      "37686/37686 [==============================] - 2s 63us/step - loss: 0.6781 - acc: 0.7430 - val_loss: 0.7969 - val_acc: 0.7137\n",
      "Epoch 85/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.6813 - acc: 0.7413 - val_loss: 0.7977 - val_acc: 0.7096\n",
      "Epoch 86/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.6759 - acc: 0.7416 - val_loss: 0.7893 - val_acc: 0.7225\n",
      "Epoch 87/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.6776 - acc: 0.7423 - val_loss: 0.8134 - val_acc: 0.7052\n",
      "Epoch 88/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.6773 - acc: 0.7426 - val_loss: 0.7857 - val_acc: 0.7220\n",
      "Epoch 89/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.6748 - acc: 0.7436 - val_loss: 0.7913 - val_acc: 0.7092\n",
      "Epoch 90/100\n",
      "37686/37686 [==============================] - 2s 61us/step - loss: 0.6728 - acc: 0.7456 - val_loss: 0.7815 - val_acc: 0.7156\n",
      "Epoch 91/100\n",
      "37686/37686 [==============================] - 2s 60us/step - loss: 0.6752 - acc: 0.7438 - val_loss: 0.8058 - val_acc: 0.7156\n",
      "Epoch 92/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.6745 - acc: 0.7445 - val_loss: 0.7979 - val_acc: 0.7173\n",
      "Epoch 93/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.6783 - acc: 0.7451 - val_loss: 0.7765 - val_acc: 0.7226\n",
      "Epoch 94/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.6727 - acc: 0.7469 - val_loss: 0.7863 - val_acc: 0.7234\n",
      "Epoch 95/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.6695 - acc: 0.7476 - val_loss: 0.8193 - val_acc: 0.7156\n",
      "Epoch 96/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.6685 - acc: 0.7468 - val_loss: 0.8011 - val_acc: 0.7176\n",
      "Epoch 97/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.6699 - acc: 0.7463 - val_loss: 0.7943 - val_acc: 0.7174\n",
      "Epoch 98/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.6655 - acc: 0.7496 - val_loss: 0.7783 - val_acc: 0.7295\n",
      "Epoch 99/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.6666 - acc: 0.7488 - val_loss: 0.7890 - val_acc: 0.7239\n",
      "Epoch 100/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.6662 - acc: 0.7507 - val_loss: 0.7861 - val_acc: 0.7189\n",
      "Inner step: 8 / 40\n",
      "Train on 37686 samples, validate on 13278 samples\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37686/37686 [==============================] - 3s 68us/step - loss: 1.2341 - acc: 0.4836 - val_loss: 1.0850 - val_acc: 0.5476\n",
      "Epoch 2/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 1.0871 - acc: 0.5415 - val_loss: 1.0887 - val_acc: 0.5471\n",
      "Epoch 3/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 1.0701 - acc: 0.5474 - val_loss: 1.0735 - val_acc: 0.5535\n",
      "Epoch 4/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 1.0633 - acc: 0.5487 - val_loss: 1.0831 - val_acc: 0.5613\n",
      "Epoch 5/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 1.0640 - acc: 0.5497 - val_loss: 1.0595 - val_acc: 0.5564\n",
      "Epoch 6/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 1.0577 - acc: 0.5498 - val_loss: 1.0649 - val_acc: 0.5566\n",
      "Epoch 7/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 1.0567 - acc: 0.5503 - val_loss: 1.0964 - val_acc: 0.5381\n",
      "Epoch 8/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 1.0498 - acc: 0.5536 - val_loss: 1.0538 - val_acc: 0.5632\n",
      "Epoch 9/100\n",
      "37686/37686 [==============================] - 2s 60us/step - loss: 1.0497 - acc: 0.5546 - val_loss: 1.0816 - val_acc: 0.5484\n",
      "Epoch 10/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 1.0517 - acc: 0.5559 - val_loss: 1.0564 - val_acc: 0.5686\n",
      "Epoch 11/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 1.0277 - acc: 0.5738 - val_loss: 1.0218 - val_acc: 0.5850\n",
      "Epoch 12/100\n",
      "37686/37686 [==============================] - 2s 63us/step - loss: 0.9918 - acc: 0.5909 - val_loss: 1.0259 - val_acc: 0.5871\n",
      "Epoch 13/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.9794 - acc: 0.5951 - val_loss: 1.0236 - val_acc: 0.5824\n",
      "Epoch 14/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.9747 - acc: 0.5978 - val_loss: 0.9920 - val_acc: 0.6002\n",
      "Epoch 15/100\n",
      "37686/37686 [==============================] - 2s 61us/step - loss: 0.9697 - acc: 0.5968 - val_loss: 1.0155 - val_acc: 0.5926\n",
      "Epoch 16/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.9568 - acc: 0.6031 - val_loss: 0.9824 - val_acc: 0.6089\n",
      "Epoch 17/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.8872 - acc: 0.6447 - val_loss: 0.9423 - val_acc: 0.6422\n",
      "Epoch 18/100\n",
      "37686/37686 [==============================] - 2s 60us/step - loss: 0.8620 - acc: 0.6549 - val_loss: 1.0015 - val_acc: 0.6051\n",
      "Epoch 19/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.8527 - acc: 0.6599 - val_loss: 0.9264 - val_acc: 0.6534\n",
      "Epoch 20/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.8431 - acc: 0.6639 - val_loss: 0.8993 - val_acc: 0.6638\n",
      "Epoch 21/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.8389 - acc: 0.6667 - val_loss: 0.9211 - val_acc: 0.6530\n",
      "Epoch 22/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.8307 - acc: 0.6734 - val_loss: 0.9105 - val_acc: 0.6506\n",
      "Epoch 23/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.8231 - acc: 0.6768 - val_loss: 0.8887 - val_acc: 0.6647\n",
      "Epoch 24/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.8160 - acc: 0.6813 - val_loss: 0.8811 - val_acc: 0.6685\n",
      "Epoch 25/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.8099 - acc: 0.6839 - val_loss: 0.8919 - val_acc: 0.6654\n",
      "Epoch 26/100\n",
      "37686/37686 [==============================] - 2s 60us/step - loss: 0.8075 - acc: 0.6874 - val_loss: 0.8774 - val_acc: 0.6801\n",
      "Epoch 27/100\n",
      "37686/37686 [==============================] - 2s 64us/step - loss: 0.8026 - acc: 0.6899 - val_loss: 0.8964 - val_acc: 0.6777\n",
      "Epoch 28/100\n",
      "37686/37686 [==============================] - 2s 63us/step - loss: 0.7973 - acc: 0.6941 - val_loss: 0.8898 - val_acc: 0.6716\n",
      "Epoch 29/100\n",
      "37686/37686 [==============================] - 2s 63us/step - loss: 0.7896 - acc: 0.6962 - val_loss: 0.8990 - val_acc: 0.6787\n",
      "Epoch 30/100\n",
      "37686/37686 [==============================] - 2s 64us/step - loss: 0.7843 - acc: 0.7003 - val_loss: 0.8689 - val_acc: 0.6837\n",
      "Epoch 31/100\n",
      "37686/37686 [==============================] - 2s 66us/step - loss: 0.7867 - acc: 0.7012 - val_loss: 0.8730 - val_acc: 0.6778\n",
      "Epoch 32/100\n",
      "37686/37686 [==============================] - 2s 66us/step - loss: 0.7806 - acc: 0.7011 - val_loss: 0.8654 - val_acc: 0.6812\n",
      "Epoch 33/100\n",
      "37686/37686 [==============================] - 2s 62us/step - loss: 0.7786 - acc: 0.7025 - val_loss: 0.9630 - val_acc: 0.6430\n",
      "Epoch 34/100\n",
      "37686/37686 [==============================] - 2s 65us/step - loss: 0.7795 - acc: 0.7018 - val_loss: 0.9029 - val_acc: 0.6719\n",
      "Epoch 35/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.7757 - acc: 0.7047 - val_loss: 0.9103 - val_acc: 0.6787\n",
      "Epoch 36/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.7751 - acc: 0.7068 - val_loss: 0.8634 - val_acc: 0.6774\n",
      "Epoch 37/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.7705 - acc: 0.7063 - val_loss: 0.8838 - val_acc: 0.6810\n",
      "Epoch 38/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.7698 - acc: 0.7088 - val_loss: 0.9070 - val_acc: 0.6734\n",
      "Epoch 39/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.7674 - acc: 0.7092 - val_loss: 0.8802 - val_acc: 0.6792\n",
      "Epoch 40/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.7661 - acc: 0.7092 - val_loss: 0.8643 - val_acc: 0.6814\n",
      "Epoch 41/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.7634 - acc: 0.7089 - val_loss: 0.8845 - val_acc: 0.6798\n",
      "Epoch 42/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.7632 - acc: 0.7086 - val_loss: 0.8665 - val_acc: 0.6773\n",
      "Epoch 43/100\n",
      "37686/37686 [==============================] - 2s 61us/step - loss: 0.7602 - acc: 0.7096 - val_loss: 0.8752 - val_acc: 0.6818\n",
      "Epoch 44/100\n",
      "37686/37686 [==============================] - 2s 60us/step - loss: 0.7563 - acc: 0.7123 - val_loss: 0.8812 - val_acc: 0.6844\n",
      "Epoch 45/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.7547 - acc: 0.7117 - val_loss: 0.8715 - val_acc: 0.6763\n",
      "Epoch 46/100\n",
      "37686/37686 [==============================] - 2s 61us/step - loss: 0.7572 - acc: 0.7089 - val_loss: 0.8561 - val_acc: 0.6770\n",
      "Epoch 47/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.7534 - acc: 0.7119 - val_loss: 0.8671 - val_acc: 0.6820\n",
      "Epoch 48/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.7533 - acc: 0.7113 - val_loss: 0.8596 - val_acc: 0.6864\n",
      "Epoch 49/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.7505 - acc: 0.7114 - val_loss: 0.8714 - val_acc: 0.6808\n",
      "Epoch 50/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.7500 - acc: 0.7122 - val_loss: 0.9122 - val_acc: 0.6719\n",
      "Epoch 51/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.7488 - acc: 0.7140 - val_loss: 0.8624 - val_acc: 0.6846\n",
      "Epoch 52/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.7511 - acc: 0.7141 - val_loss: 0.8445 - val_acc: 0.6920\n",
      "Epoch 53/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.7470 - acc: 0.7141 - val_loss: 0.8634 - val_acc: 0.6777\n",
      "Epoch 54/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.7435 - acc: 0.7153 - val_loss: 0.8743 - val_acc: 0.6790\n",
      "Epoch 55/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.7397 - acc: 0.7172 - val_loss: 0.8460 - val_acc: 0.6859\n",
      "Epoch 56/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.7401 - acc: 0.7165 - val_loss: 0.9037 - val_acc: 0.6754\n",
      "Epoch 57/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.7394 - acc: 0.7157 - val_loss: 0.8564 - val_acc: 0.6798\n",
      "Epoch 58/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.7367 - acc: 0.7190 - val_loss: 0.8627 - val_acc: 0.6893\n",
      "Epoch 59/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.7358 - acc: 0.7165 - val_loss: 0.8637 - val_acc: 0.6875\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.7337 - acc: 0.7161 - val_loss: 0.8464 - val_acc: 0.6911\n",
      "Epoch 61/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.7324 - acc: 0.7184 - val_loss: 0.8556 - val_acc: 0.6912\n",
      "Epoch 62/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.7251 - acc: 0.7203 - val_loss: 0.8763 - val_acc: 0.6832\n",
      "Epoch 63/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.7302 - acc: 0.7172 - val_loss: 0.8486 - val_acc: 0.6860\n",
      "Epoch 64/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.7246 - acc: 0.7170 - val_loss: 0.8708 - val_acc: 0.6835\n",
      "Epoch 65/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.7237 - acc: 0.7235 - val_loss: 0.8863 - val_acc: 0.6898\n",
      "Epoch 66/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.7168 - acc: 0.7225 - val_loss: 0.8842 - val_acc: 0.7009\n",
      "Epoch 67/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.7100 - acc: 0.7323 - val_loss: 0.8367 - val_acc: 0.7167\n",
      "Epoch 68/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.6938 - acc: 0.7488 - val_loss: 0.8560 - val_acc: 0.7107\n",
      "Epoch 69/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.6746 - acc: 0.7656 - val_loss: 0.8399 - val_acc: 0.7247\n",
      "Epoch 70/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.6599 - acc: 0.7742 - val_loss: 0.8154 - val_acc: 0.7399\n",
      "Epoch 71/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.6503 - acc: 0.7813 - val_loss: 0.7725 - val_acc: 0.7570\n",
      "Epoch 72/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.6364 - acc: 0.7922 - val_loss: 0.7854 - val_acc: 0.7585\n",
      "Epoch 73/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.6266 - acc: 0.8005 - val_loss: 0.7542 - val_acc: 0.7693\n",
      "Epoch 74/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.6194 - acc: 0.8058 - val_loss: 0.7591 - val_acc: 0.7701\n",
      "Epoch 75/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.6141 - acc: 0.8085 - val_loss: 0.7569 - val_acc: 0.7715\n",
      "Epoch 76/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.6092 - acc: 0.8109 - val_loss: 0.7571 - val_acc: 0.7701\n",
      "Epoch 77/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.6065 - acc: 0.8118 - val_loss: 0.7429 - val_acc: 0.7765\n",
      "Epoch 78/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.6038 - acc: 0.8119 - val_loss: 0.7763 - val_acc: 0.7604\n",
      "Epoch 79/100\n",
      "37686/37686 [==============================] - 2s 60us/step - loss: 0.5987 - acc: 0.8164 - val_loss: 0.7612 - val_acc: 0.7726\n",
      "Epoch 80/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.5948 - acc: 0.8169 - val_loss: 0.7295 - val_acc: 0.7806\n",
      "Epoch 81/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.5909 - acc: 0.8184 - val_loss: 0.7523 - val_acc: 0.7731\n",
      "Epoch 82/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.5924 - acc: 0.8199 - val_loss: 0.7541 - val_acc: 0.7747\n",
      "Epoch 83/100\n",
      "37686/37686 [==============================] - 3s 66us/step - loss: 0.5854 - acc: 0.8215 - val_loss: 0.7526 - val_acc: 0.7736\n",
      "Epoch 84/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.5922 - acc: 0.8203 - val_loss: 0.7399 - val_acc: 0.7822\n",
      "Epoch 85/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.5873 - acc: 0.8212 - val_loss: 0.7435 - val_acc: 0.7817\n",
      "Epoch 86/100\n",
      "37686/37686 [==============================] - 2s 61us/step - loss: 0.5874 - acc: 0.8222 - val_loss: 0.7303 - val_acc: 0.7827\n",
      "Epoch 87/100\n",
      "37686/37686 [==============================] - 3s 73us/step - loss: 0.5837 - acc: 0.8231 - val_loss: 0.7319 - val_acc: 0.7820\n",
      "Epoch 88/100\n",
      "37686/37686 [==============================] - 2s 62us/step - loss: 0.5823 - acc: 0.8239 - val_loss: 0.7273 - val_acc: 0.7864\n",
      "Epoch 89/100\n",
      "37686/37686 [==============================] - 2s 61us/step - loss: 0.5798 - acc: 0.8249 - val_loss: 0.7514 - val_acc: 0.7798\n",
      "Epoch 90/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.5787 - acc: 0.8246 - val_loss: 0.7412 - val_acc: 0.7823\n",
      "Epoch 91/100\n",
      "37686/37686 [==============================] - 2s 60us/step - loss: 0.5824 - acc: 0.8239 - val_loss: 0.7595 - val_acc: 0.7759\n",
      "Epoch 92/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.5779 - acc: 0.8253 - val_loss: 0.7516 - val_acc: 0.7826\n",
      "Epoch 93/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.5761 - acc: 0.8259 - val_loss: 0.7137 - val_acc: 0.7906\n",
      "Epoch 94/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.5729 - acc: 0.8273 - val_loss: 0.7420 - val_acc: 0.7790\n",
      "Epoch 95/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.5761 - acc: 0.8261 - val_loss: 0.7341 - val_acc: 0.7878\n",
      "Epoch 96/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.5746 - acc: 0.8263 - val_loss: 0.7375 - val_acc: 0.7860\n",
      "Epoch 97/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.5731 - acc: 0.8294 - val_loss: 0.7418 - val_acc: 0.7850\n",
      "Epoch 98/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.5714 - acc: 0.8279 - val_loss: 0.7283 - val_acc: 0.7866\n",
      "Epoch 99/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.5672 - acc: 0.8288 - val_loss: 0.7350 - val_acc: 0.7875\n",
      "Epoch 100/100\n",
      "37686/37686 [==============================] - 2s 61us/step - loss: 0.5679 - acc: 0.8291 - val_loss: 0.7405 - val_acc: 0.7820\n",
      "Inner step: 9 / 40\n",
      "Train on 37686 samples, validate on 13278 samples\n",
      "Epoch 1/100\n",
      "37686/37686 [==============================] - 2s 66us/step - loss: 1.4011 - acc: 0.4128 - val_loss: 1.3602 - val_acc: 0.4345\n",
      "Epoch 2/100\n",
      "37686/37686 [==============================] - 2s 60us/step - loss: 1.3562 - acc: 0.4242 - val_loss: 1.3774 - val_acc: 0.4295\n",
      "Epoch 3/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 1.3470 - acc: 0.4306 - val_loss: 1.3545 - val_acc: 0.4455\n",
      "Epoch 4/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 1.2760 - acc: 0.4788 - val_loss: 1.3101 - val_acc: 0.4806\n",
      "Epoch 5/100\n",
      "37686/37686 [==============================] - 2s 61us/step - loss: 1.2559 - acc: 0.4920 - val_loss: 1.2791 - val_acc: 0.4901\n",
      "Epoch 6/100\n",
      "37686/37686 [==============================] - 2s 61us/step - loss: 1.2483 - acc: 0.4937 - val_loss: 1.2986 - val_acc: 0.4868\n",
      "Epoch 7/100\n",
      "37686/37686 [==============================] - 3s 67us/step - loss: 1.2390 - acc: 0.4986 - val_loss: 1.3466 - val_acc: 0.4652\n",
      "Epoch 8/100\n",
      "37686/37686 [==============================] - 2s 62us/step - loss: 1.2248 - acc: 0.5080 - val_loss: 1.2390 - val_acc: 0.5123\n",
      "Epoch 9/100\n",
      "37686/37686 [==============================] - 2s 64us/step - loss: 1.1851 - acc: 0.5287 - val_loss: 1.2432 - val_acc: 0.5253\n",
      "Epoch 10/100\n",
      "37686/37686 [==============================] - 2s 60us/step - loss: 1.1667 - acc: 0.5381 - val_loss: 1.2433 - val_acc: 0.5233\n",
      "Epoch 11/100\n",
      "37686/37686 [==============================] - 2s 62us/step - loss: 1.1584 - acc: 0.5412 - val_loss: 1.2273 - val_acc: 0.5290\n",
      "Epoch 12/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 1.1511 - acc: 0.5442 - val_loss: 1.2160 - val_acc: 0.5331\n",
      "Epoch 13/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 1.1495 - acc: 0.5456 - val_loss: 1.2147 - val_acc: 0.5331\n",
      "Epoch 14/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 1.1443 - acc: 0.5471 - val_loss: 1.2108 - val_acc: 0.5380\n",
      "Epoch 15/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 1.1400 - acc: 0.5486 - val_loss: 1.2354 - val_acc: 0.5270\n",
      "Epoch 16/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 1.1414 - acc: 0.5478 - val_loss: 1.2116 - val_acc: 0.5369\n",
      "Epoch 17/100\n",
      "37686/37686 [==============================] - 2s 61us/step - loss: 1.1348 - acc: 0.5505 - val_loss: 1.2233 - val_acc: 0.5334\n",
      "Epoch 18/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37686/37686 [==============================] - 2s 60us/step - loss: 1.1341 - acc: 0.5514 - val_loss: 1.2399 - val_acc: 0.5277\n",
      "Epoch 19/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 1.1304 - acc: 0.5528 - val_loss: 1.2022 - val_acc: 0.5477\n",
      "Epoch 20/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 1.0290 - acc: 0.5830 - val_loss: 1.0227 - val_acc: 0.5900\n",
      "Epoch 21/100\n",
      "37686/37686 [==============================] - 3s 66us/step - loss: 0.9062 - acc: 0.6318 - val_loss: 0.9798 - val_acc: 0.6409\n",
      "Epoch 22/100\n",
      "37686/37686 [==============================] - 3s 71us/step - loss: 0.8670 - acc: 0.6535 - val_loss: 0.9487 - val_acc: 0.6312\n",
      "Epoch 23/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.8477 - acc: 0.6664 - val_loss: 0.9692 - val_acc: 0.6463\n",
      "Epoch 24/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.8401 - acc: 0.6706 - val_loss: 0.9585 - val_acc: 0.6420\n",
      "Epoch 25/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.8348 - acc: 0.6752 - val_loss: 0.9708 - val_acc: 0.6681\n",
      "Epoch 26/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.8273 - acc: 0.6887 - val_loss: 0.9272 - val_acc: 0.6444\n",
      "Epoch 27/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.8246 - acc: 0.6937 - val_loss: 0.9929 - val_acc: 0.6163\n",
      "Epoch 28/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.8161 - acc: 0.7011 - val_loss: 0.8921 - val_acc: 0.6828\n",
      "Epoch 29/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.8062 - acc: 0.7034 - val_loss: 0.8878 - val_acc: 0.6887\n",
      "Epoch 30/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.7947 - acc: 0.7042 - val_loss: 0.8938 - val_acc: 0.6605\n",
      "Epoch 31/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.7916 - acc: 0.6985 - val_loss: 0.8701 - val_acc: 0.6720\n",
      "Epoch 32/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.7847 - acc: 0.6972 - val_loss: 0.8730 - val_acc: 0.6606\n",
      "Epoch 33/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.7754 - acc: 0.6991 - val_loss: 0.8683 - val_acc: 0.6787\n",
      "Epoch 34/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.7780 - acc: 0.6985 - val_loss: 0.9412 - val_acc: 0.6337\n",
      "Epoch 35/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.7752 - acc: 0.7011 - val_loss: 0.8703 - val_acc: 0.6726\n",
      "Epoch 36/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.7695 - acc: 0.7036 - val_loss: 0.8645 - val_acc: 0.6678\n",
      "Epoch 37/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.7669 - acc: 0.7016 - val_loss: 0.8954 - val_acc: 0.6827\n",
      "Epoch 38/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.7661 - acc: 0.7056 - val_loss: 0.8515 - val_acc: 0.6788\n",
      "Epoch 39/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.7423 - acc: 0.7292 - val_loss: 0.8290 - val_acc: 0.7228\n",
      "Epoch 40/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.6933 - acc: 0.7588 - val_loss: 0.8040 - val_acc: 0.7275\n",
      "Epoch 41/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.6845 - acc: 0.7616 - val_loss: 0.8524 - val_acc: 0.7235\n",
      "Epoch 42/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.6776 - acc: 0.7657 - val_loss: 0.8327 - val_acc: 0.7174\n",
      "Epoch 43/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.6717 - acc: 0.7670 - val_loss: 0.8067 - val_acc: 0.7287\n",
      "Epoch 44/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.6691 - acc: 0.7677 - val_loss: 0.8052 - val_acc: 0.7282\n",
      "Epoch 45/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.6643 - acc: 0.7681 - val_loss: 0.7984 - val_acc: 0.7331\n",
      "Epoch 46/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.6640 - acc: 0.7708 - val_loss: 0.8302 - val_acc: 0.7225\n",
      "Epoch 47/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.6644 - acc: 0.7685 - val_loss: 0.8213 - val_acc: 0.7245\n",
      "Epoch 48/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.6601 - acc: 0.7701 - val_loss: 0.7909 - val_acc: 0.7362\n",
      "Epoch 49/100\n",
      "37686/37686 [==============================] - 2s 61us/step - loss: 0.6587 - acc: 0.7694 - val_loss: 0.7841 - val_acc: 0.7465\n",
      "Epoch 50/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.6567 - acc: 0.7720 - val_loss: 0.8002 - val_acc: 0.7351\n",
      "Epoch 51/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.6556 - acc: 0.7721 - val_loss: 0.7900 - val_acc: 0.7361\n",
      "Epoch 52/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.6554 - acc: 0.7730 - val_loss: 0.7963 - val_acc: 0.7380\n",
      "Epoch 53/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.6511 - acc: 0.7742 - val_loss: 0.7925 - val_acc: 0.7373\n",
      "Epoch 54/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.6492 - acc: 0.7755 - val_loss: 0.7900 - val_acc: 0.7354\n",
      "Epoch 55/100\n",
      "37686/37686 [==============================] - 2s 61us/step - loss: 0.6510 - acc: 0.7733 - val_loss: 0.7975 - val_acc: 0.7308\n",
      "Epoch 56/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.6487 - acc: 0.7724 - val_loss: 0.7801 - val_acc: 0.7414\n",
      "Epoch 57/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.6447 - acc: 0.7762 - val_loss: 0.8248 - val_acc: 0.7160\n",
      "Epoch 58/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.6434 - acc: 0.7770 - val_loss: 0.7874 - val_acc: 0.7388\n",
      "Epoch 59/100\n",
      "37686/37686 [==============================] - 2s 60us/step - loss: 0.6441 - acc: 0.7745 - val_loss: 0.7702 - val_acc: 0.7461\n",
      "Epoch 60/100\n",
      "37686/37686 [==============================] - 2s 63us/step - loss: 0.6426 - acc: 0.7777 - val_loss: 0.7724 - val_acc: 0.7427\n",
      "Epoch 61/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.6372 - acc: 0.7788 - val_loss: 0.8026 - val_acc: 0.7308\n",
      "Epoch 62/100\n",
      "37686/37686 [==============================] - 2s 60us/step - loss: 0.6415 - acc: 0.7775 - val_loss: 0.7696 - val_acc: 0.7434\n",
      "Epoch 63/100\n",
      "37686/37686 [==============================] - 2s 60us/step - loss: 0.6368 - acc: 0.7786 - val_loss: 0.7811 - val_acc: 0.7391\n",
      "Epoch 64/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.6359 - acc: 0.7804 - val_loss: 0.7572 - val_acc: 0.7482\n",
      "Epoch 65/100\n",
      "37686/37686 [==============================] - 3s 72us/step - loss: 0.6367 - acc: 0.7780 - val_loss: 0.7902 - val_acc: 0.7431\n",
      "Epoch 66/100\n",
      "37686/37686 [==============================] - 3s 72us/step - loss: 0.6330 - acc: 0.7790 - val_loss: 0.7882 - val_acc: 0.7372\n",
      "Epoch 67/100\n",
      "37686/37686 [==============================] - 2s 64us/step - loss: 0.6293 - acc: 0.7807 - val_loss: 0.7754 - val_acc: 0.7420\n",
      "Epoch 68/100\n",
      "37686/37686 [==============================] - 3s 70us/step - loss: 0.6279 - acc: 0.7806 - val_loss: 0.7852 - val_acc: 0.7395\n",
      "Epoch 69/100\n",
      "37686/37686 [==============================] - 2s 60us/step - loss: 0.6259 - acc: 0.7808 - val_loss: 0.7841 - val_acc: 0.7427\n",
      "Epoch 70/100\n",
      "37686/37686 [==============================] - 3s 68us/step - loss: 0.6276 - acc: 0.7804 - val_loss: 0.7740 - val_acc: 0.7382\n",
      "Epoch 71/100\n",
      "37686/37686 [==============================] - 2s 65us/step - loss: 0.6220 - acc: 0.7813 - val_loss: 0.7735 - val_acc: 0.7346\n",
      "Epoch 72/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.6228 - acc: 0.7832 - val_loss: 0.7816 - val_acc: 0.7336\n",
      "Epoch 73/100\n",
      "37686/37686 [==============================] - 2s 65us/step - loss: 0.6199 - acc: 0.7824 - val_loss: 0.7800 - val_acc: 0.7410\n",
      "Epoch 74/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.6207 - acc: 0.7815 - val_loss: 0.7968 - val_acc: 0.7364\n",
      "Epoch 75/100\n",
      "37686/37686 [==============================] - 3s 70us/step - loss: 0.6207 - acc: 0.7825 - val_loss: 0.7563 - val_acc: 0.7459\n",
      "Epoch 76/100\n",
      "37686/37686 [==============================] - 3s 67us/step - loss: 0.6188 - acc: 0.7836 - val_loss: 0.7710 - val_acc: 0.7479\n",
      "Epoch 77/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37686/37686 [==============================] - 3s 71us/step - loss: 0.6168 - acc: 0.7833 - val_loss: 0.7768 - val_acc: 0.7407\n",
      "Epoch 78/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.6175 - acc: 0.7834 - val_loss: 0.8062 - val_acc: 0.7279\n",
      "Epoch 79/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.6149 - acc: 0.7840 - val_loss: 0.7692 - val_acc: 0.7390\n",
      "Epoch 80/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.6148 - acc: 0.7846 - val_loss: 0.7889 - val_acc: 0.7334\n",
      "Epoch 81/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.6107 - acc: 0.7858 - val_loss: 0.7906 - val_acc: 0.7298\n",
      "Epoch 82/100\n",
      "37686/37686 [==============================] - 3s 66us/step - loss: 0.6142 - acc: 0.7844 - val_loss: 0.7943 - val_acc: 0.7329\n",
      "Epoch 83/100\n",
      "37686/37686 [==============================] - 2s 63us/step - loss: 0.6079 - acc: 0.7872 - val_loss: 0.7787 - val_acc: 0.7392\n",
      "Epoch 84/100\n",
      "37686/37686 [==============================] - 2s 62us/step - loss: 0.6124 - acc: 0.7850 - val_loss: 0.7777 - val_acc: 0.7409\n",
      "Epoch 85/100\n",
      "37686/37686 [==============================] - 2s 62us/step - loss: 0.6139 - acc: 0.7841 - val_loss: 0.7515 - val_acc: 0.7492\n",
      "Epoch 86/100\n",
      "37686/37686 [==============================] - 3s 67us/step - loss: 0.6061 - acc: 0.7860 - val_loss: 0.7641 - val_acc: 0.7534\n",
      "Epoch 87/100\n",
      "37686/37686 [==============================] - 3s 69us/step - loss: 0.6087 - acc: 0.7864 - val_loss: 0.7529 - val_acc: 0.7509\n",
      "Epoch 88/100\n",
      "37686/37686 [==============================] - 2s 63us/step - loss: 0.6079 - acc: 0.7855 - val_loss: 0.7613 - val_acc: 0.7466\n",
      "Epoch 89/100\n",
      "37686/37686 [==============================] - 2s 63us/step - loss: 0.6036 - acc: 0.7909 - val_loss: 0.8452 - val_acc: 0.7274\n",
      "Epoch 90/100\n",
      "37686/37686 [==============================] - 2s 60us/step - loss: 0.6045 - acc: 0.7863 - val_loss: 0.8058 - val_acc: 0.7346\n",
      "Epoch 91/100\n",
      "37686/37686 [==============================] - 2s 66us/step - loss: 0.6038 - acc: 0.7885 - val_loss: 0.7658 - val_acc: 0.7520\n",
      "Epoch 92/100\n",
      "37686/37686 [==============================] - 2s 63us/step - loss: 0.6045 - acc: 0.7878 - val_loss: 0.7675 - val_acc: 0.7536\n",
      "Epoch 93/100\n",
      "37686/37686 [==============================] - 3s 75us/step - loss: 0.5995 - acc: 0.7896 - val_loss: 0.7492 - val_acc: 0.7460\n",
      "Epoch 94/100\n",
      "37686/37686 [==============================] - 3s 67us/step - loss: 0.5992 - acc: 0.7899 - val_loss: 0.7594 - val_acc: 0.7503\n",
      "Epoch 95/100\n",
      "37686/37686 [==============================] - 3s 73us/step - loss: 0.6015 - acc: 0.7899 - val_loss: 0.7656 - val_acc: 0.7491\n",
      "Epoch 96/100\n",
      "37686/37686 [==============================] - 3s 71us/step - loss: 0.5981 - acc: 0.7888 - val_loss: 0.7621 - val_acc: 0.7510\n",
      "Epoch 97/100\n",
      "37686/37686 [==============================] - 3s 70us/step - loss: 0.5971 - acc: 0.7891 - val_loss: 0.7522 - val_acc: 0.7582\n",
      "Epoch 98/100\n",
      "37686/37686 [==============================] - 3s 72us/step - loss: 0.5987 - acc: 0.7902 - val_loss: 0.7623 - val_acc: 0.7512\n",
      "Epoch 99/100\n",
      "37686/37686 [==============================] - 3s 71us/step - loss: 0.5955 - acc: 0.7912 - val_loss: 0.7615 - val_acc: 0.7543\n",
      "Epoch 100/100\n",
      "37686/37686 [==============================] - 3s 71us/step - loss: 0.5975 - acc: 0.7915 - val_loss: 0.7856 - val_acc: 0.7432\n",
      "Inner step: 10 / 40\n",
      "Train on 37686 samples, validate on 13278 samples\n",
      "Epoch 1/100\n",
      "37686/37686 [==============================] - 3s 86us/step - loss: 1.2178 - acc: 0.4990 - val_loss: 1.0838 - val_acc: 0.5481\n",
      "Epoch 2/100\n",
      "37686/37686 [==============================] - 3s 68us/step - loss: 1.0801 - acc: 0.5461 - val_loss: 1.0678 - val_acc: 0.5563\n",
      "Epoch 3/100\n",
      "37686/37686 [==============================] - 2s 66us/step - loss: 1.0650 - acc: 0.5535 - val_loss: 1.0638 - val_acc: 0.5553\n",
      "Epoch 4/100\n",
      "37686/37686 [==============================] - 3s 69us/step - loss: 1.0620 - acc: 0.5524 - val_loss: 1.1255 - val_acc: 0.5334\n",
      "Epoch 5/100\n",
      "37686/37686 [==============================] - 2s 64us/step - loss: 1.0584 - acc: 0.5524 - val_loss: 1.0923 - val_acc: 0.5484\n",
      "Epoch 6/100\n",
      "37686/37686 [==============================] - 3s 70us/step - loss: 1.0531 - acc: 0.5555 - val_loss: 1.0588 - val_acc: 0.5591\n",
      "Epoch 7/100\n",
      "37686/37686 [==============================] - 2s 64us/step - loss: 1.0537 - acc: 0.5541 - val_loss: 1.0831 - val_acc: 0.5521\n",
      "Epoch 8/100\n",
      "37686/37686 [==============================] - 2s 63us/step - loss: 1.0530 - acc: 0.5541 - val_loss: 1.0681 - val_acc: 0.5622\n",
      "Epoch 9/100\n",
      "37686/37686 [==============================] - 2s 64us/step - loss: 1.0516 - acc: 0.5556 - val_loss: 1.0908 - val_acc: 0.5434\n",
      "Epoch 10/100\n",
      "37686/37686 [==============================] - 2s 65us/step - loss: 1.0490 - acc: 0.5570 - val_loss: 1.0721 - val_acc: 0.5587\n",
      "Epoch 11/100\n",
      "37686/37686 [==============================] - 2s 60us/step - loss: 1.0445 - acc: 0.5591 - val_loss: 1.0225 - val_acc: 0.6113\n",
      "Epoch 12/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.9682 - acc: 0.6141 - val_loss: 0.9838 - val_acc: 0.6356\n",
      "Epoch 13/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.9098 - acc: 0.6486 - val_loss: 0.9260 - val_acc: 0.6597\n",
      "Epoch 14/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.8808 - acc: 0.6531 - val_loss: 0.9120 - val_acc: 0.6368\n",
      "Epoch 15/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.8657 - acc: 0.6576 - val_loss: 0.9101 - val_acc: 0.6607\n",
      "Epoch 16/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.8596 - acc: 0.6603 - val_loss: 0.9068 - val_acc: 0.6497\n",
      "Epoch 17/100\n",
      "37686/37686 [==============================] - 2s 61us/step - loss: 0.8506 - acc: 0.6661 - val_loss: 0.9049 - val_acc: 0.6231\n",
      "Epoch 18/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.8325 - acc: 0.6783 - val_loss: 0.9779 - val_acc: 0.6670\n",
      "Epoch 19/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.7926 - acc: 0.6982 - val_loss: 0.8329 - val_acc: 0.6975\n",
      "Epoch 20/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.7727 - acc: 0.7102 - val_loss: 0.8213 - val_acc: 0.7082\n",
      "Epoch 21/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.7539 - acc: 0.7177 - val_loss: 0.8179 - val_acc: 0.7131\n",
      "Epoch 22/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.7407 - acc: 0.7250 - val_loss: 0.9544 - val_acc: 0.6411\n",
      "Epoch 23/100\n",
      "37686/37686 [==============================] - 2s 62us/step - loss: 0.7366 - acc: 0.7296 - val_loss: 0.8977 - val_acc: 0.6756\n",
      "Epoch 24/100\n",
      "37686/37686 [==============================] - 3s 72us/step - loss: 0.7284 - acc: 0.7344 - val_loss: 0.8199 - val_acc: 0.7147\n",
      "Epoch 25/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.7264 - acc: 0.7353 - val_loss: 0.8149 - val_acc: 0.7146\n",
      "Epoch 26/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 0.7168 - acc: 0.7416 - val_loss: 0.8145 - val_acc: 0.7221\n",
      "Epoch 27/100\n",
      "37686/37686 [==============================] - 2s 52us/step - loss: 0.7143 - acc: 0.7435 - val_loss: 0.8070 - val_acc: 0.7237\n",
      "Epoch 28/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.7078 - acc: 0.7479 - val_loss: 0.7943 - val_acc: 0.7320\n",
      "Epoch 29/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 0.7052 - acc: 0.7496 - val_loss: 0.8563 - val_acc: 0.6979\n",
      "Epoch 30/100\n",
      "37686/37686 [==============================] - 2s 52us/step - loss: 0.6946 - acc: 0.7532 - val_loss: 0.7857 - val_acc: 0.7335\n",
      "Epoch 31/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.6907 - acc: 0.7564 - val_loss: 0.7926 - val_acc: 0.7329\n",
      "Epoch 32/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.6885 - acc: 0.7565 - val_loss: 0.8267 - val_acc: 0.7194\n",
      "Epoch 33/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.6789 - acc: 0.7613 - val_loss: 0.8012 - val_acc: 0.7306\n",
      "Epoch 34/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 0.6794 - acc: 0.7626 - val_loss: 0.7801 - val_acc: 0.7333\n",
      "Epoch 35/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.6723 - acc: 0.7638 - val_loss: 0.7785 - val_acc: 0.7376\n",
      "Epoch 36/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.6733 - acc: 0.7628 - val_loss: 0.7949 - val_acc: 0.7314\n",
      "Epoch 37/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 0.6686 - acc: 0.7639 - val_loss: 0.7992 - val_acc: 0.7281\n",
      "Epoch 38/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.6593 - acc: 0.7691 - val_loss: 0.7951 - val_acc: 0.7241\n",
      "Epoch 39/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.6606 - acc: 0.7660 - val_loss: 0.8132 - val_acc: 0.7296\n",
      "Epoch 40/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.6544 - acc: 0.7700 - val_loss: 0.7709 - val_acc: 0.7412\n",
      "Epoch 41/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.6577 - acc: 0.7691 - val_loss: 0.7851 - val_acc: 0.7369\n",
      "Epoch 42/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.6540 - acc: 0.7694 - val_loss: 0.7753 - val_acc: 0.7368\n",
      "Epoch 43/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.6526 - acc: 0.7698 - val_loss: 0.7975 - val_acc: 0.7217\n",
      "Epoch 44/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.6437 - acc: 0.7724 - val_loss: 0.7727 - val_acc: 0.7390\n",
      "Epoch 45/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.6443 - acc: 0.7730 - val_loss: 0.7943 - val_acc: 0.7329\n",
      "Epoch 46/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.6437 - acc: 0.7737 - val_loss: 0.7858 - val_acc: 0.7390\n",
      "Epoch 47/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.6404 - acc: 0.7731 - val_loss: 0.8298 - val_acc: 0.7436\n",
      "Epoch 48/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.6369 - acc: 0.7730 - val_loss: 0.7909 - val_acc: 0.7339\n",
      "Epoch 49/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.6342 - acc: 0.7747 - val_loss: 0.7914 - val_acc: 0.7295\n",
      "Epoch 50/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.6371 - acc: 0.7728 - val_loss: 0.7742 - val_acc: 0.7404\n",
      "Epoch 51/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.6337 - acc: 0.7749 - val_loss: 0.7611 - val_acc: 0.7443\n",
      "Epoch 52/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.6293 - acc: 0.7753 - val_loss: 0.8317 - val_acc: 0.7168\n",
      "Epoch 53/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.6269 - acc: 0.7776 - val_loss: 0.7732 - val_acc: 0.7432\n",
      "Epoch 54/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.6256 - acc: 0.7774 - val_loss: 0.7829 - val_acc: 0.7470\n",
      "Epoch 55/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.6199 - acc: 0.7783 - val_loss: 0.8092 - val_acc: 0.7302\n",
      "Epoch 56/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.6229 - acc: 0.7773 - val_loss: 0.7644 - val_acc: 0.7496\n",
      "Epoch 57/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.6174 - acc: 0.7783 - val_loss: 0.7781 - val_acc: 0.7476\n",
      "Epoch 58/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.6158 - acc: 0.7818 - val_loss: 0.7560 - val_acc: 0.7494\n",
      "Epoch 59/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.6172 - acc: 0.7791 - val_loss: 0.7777 - val_acc: 0.7439\n",
      "Epoch 60/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.6154 - acc: 0.7805 - val_loss: 0.7683 - val_acc: 0.7454\n",
      "Epoch 61/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.6109 - acc: 0.7830 - val_loss: 0.7469 - val_acc: 0.7513\n",
      "Epoch 62/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.6114 - acc: 0.7817 - val_loss: 0.7407 - val_acc: 0.7533\n",
      "Epoch 63/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.6050 - acc: 0.7834 - val_loss: 0.7548 - val_acc: 0.7501\n",
      "Epoch 64/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.6040 - acc: 0.7851 - val_loss: 0.7731 - val_acc: 0.7412\n",
      "Epoch 65/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.6010 - acc: 0.7866 - val_loss: 0.7742 - val_acc: 0.7509\n",
      "Epoch 66/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.6052 - acc: 0.7848 - val_loss: 0.7664 - val_acc: 0.7482\n",
      "Epoch 67/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.6008 - acc: 0.7845 - val_loss: 0.7887 - val_acc: 0.7499\n",
      "Epoch 68/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.5991 - acc: 0.7870 - val_loss: 0.7703 - val_acc: 0.7489\n",
      "Epoch 69/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.5982 - acc: 0.7861 - val_loss: 0.7441 - val_acc: 0.7549\n",
      "Epoch 70/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.5914 - acc: 0.7880 - val_loss: 0.7662 - val_acc: 0.7537\n",
      "Epoch 71/100\n",
      "37686/37686 [==============================] - 2s 52us/step - loss: 0.5986 - acc: 0.7874 - val_loss: 0.7698 - val_acc: 0.7495\n",
      "Epoch 72/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 0.5919 - acc: 0.7875 - val_loss: 0.7768 - val_acc: 0.7509\n",
      "Epoch 73/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 0.5930 - acc: 0.7884 - val_loss: 0.7633 - val_acc: 0.7534\n",
      "Epoch 74/100\n",
      "37686/37686 [==============================] - 2s 52us/step - loss: 0.5898 - acc: 0.7881 - val_loss: 0.7716 - val_acc: 0.7473\n",
      "Epoch 75/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.5866 - acc: 0.7904 - val_loss: 0.7951 - val_acc: 0.7483\n",
      "Epoch 76/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.5859 - acc: 0.7894 - val_loss: 0.7638 - val_acc: 0.7583\n",
      "Epoch 77/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.5890 - acc: 0.7887 - val_loss: 0.7701 - val_acc: 0.7494\n",
      "Epoch 78/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.5816 - acc: 0.7918 - val_loss: 0.7753 - val_acc: 0.7466\n",
      "Epoch 79/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.5894 - acc: 0.7894 - val_loss: 0.7533 - val_acc: 0.7537\n",
      "Epoch 80/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.5829 - acc: 0.7911 - val_loss: 0.7487 - val_acc: 0.7559\n",
      "Epoch 81/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.5846 - acc: 0.7890 - val_loss: 0.7671 - val_acc: 0.7472\n",
      "Epoch 82/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.5815 - acc: 0.7918 - val_loss: 0.7410 - val_acc: 0.7527\n",
      "Epoch 83/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.5820 - acc: 0.7915 - val_loss: 0.7476 - val_acc: 0.7531\n",
      "Epoch 84/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.5773 - acc: 0.7932 - val_loss: 0.7504 - val_acc: 0.7485\n",
      "Epoch 85/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.5748 - acc: 0.7930 - val_loss: 0.7845 - val_acc: 0.7482\n",
      "Epoch 86/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.5759 - acc: 0.7925 - val_loss: 0.7770 - val_acc: 0.7581\n",
      "Epoch 87/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.5754 - acc: 0.7925 - val_loss: 0.7830 - val_acc: 0.7466\n",
      "Epoch 88/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 0.5746 - acc: 0.7914 - val_loss: 0.7851 - val_acc: 0.7519\n",
      "Epoch 89/100\n",
      "37686/37686 [==============================] - 2s 52us/step - loss: 0.5725 - acc: 0.7937 - val_loss: 0.7502 - val_acc: 0.7549\n",
      "Epoch 90/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 0.5715 - acc: 0.7947 - val_loss: 0.7805 - val_acc: 0.7402\n",
      "Epoch 91/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 0.5718 - acc: 0.7962 - val_loss: 0.7574 - val_acc: 0.7445\n",
      "Epoch 92/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.5736 - acc: 0.7938 - val_loss: 0.7645 - val_acc: 0.7520\n",
      "Epoch 93/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.5662 - acc: 0.7966 - val_loss: 0.7403 - val_acc: 0.7536\n",
      "Epoch 94/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.5624 - acc: 0.7971 - val_loss: 0.7664 - val_acc: 0.7553\n",
      "Epoch 95/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.5693 - acc: 0.7935 - val_loss: 0.7352 - val_acc: 0.7591\n",
      "Epoch 96/100\n",
      "37686/37686 [==============================] - 2s 52us/step - loss: 0.5680 - acc: 0.7964 - val_loss: 0.7348 - val_acc: 0.7588\n",
      "Epoch 97/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 0.5632 - acc: 0.7952 - val_loss: 0.7548 - val_acc: 0.7573\n",
      "Epoch 98/100\n",
      "37686/37686 [==============================] - 2s 52us/step - loss: 0.5610 - acc: 0.7951 - val_loss: 0.7387 - val_acc: 0.7558\n",
      "Epoch 99/100\n",
      "37686/37686 [==============================] - 2s 52us/step - loss: 0.5644 - acc: 0.7949 - val_loss: 0.7522 - val_acc: 0.7568\n",
      "Epoch 100/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 0.5609 - acc: 0.7958 - val_loss: 0.7668 - val_acc: 0.7401\n",
      "Inner step: 11 / 40\n",
      "Train on 37686 samples, validate on 13278 samples\n",
      "Epoch 1/100\n",
      "37686/37686 [==============================] - 3s 73us/step - loss: 1.3441 - acc: 0.4447 - val_loss: 1.2913 - val_acc: 0.4746\n",
      "Epoch 2/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 1.2811 - acc: 0.4696 - val_loss: 1.3082 - val_acc: 0.4715\n",
      "Epoch 3/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 1.2716 - acc: 0.4735 - val_loss: 1.2890 - val_acc: 0.4774\n",
      "Epoch 4/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 1.2672 - acc: 0.4763 - val_loss: 1.3016 - val_acc: 0.4745\n",
      "Epoch 5/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 1.2666 - acc: 0.4771 - val_loss: 1.2925 - val_acc: 0.4837\n",
      "Epoch 6/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 1.2639 - acc: 0.4807 - val_loss: 1.2948 - val_acc: 0.4800\n",
      "Epoch 7/100\n",
      "37686/37686 [==============================] - 2s 61us/step - loss: 1.2616 - acc: 0.4819 - val_loss: 1.2991 - val_acc: 0.4867\n",
      "Epoch 8/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 1.2394 - acc: 0.5028 - val_loss: 1.2562 - val_acc: 0.5059\n",
      "Epoch 9/100\n",
      "37686/37686 [==============================] - 2s 60us/step - loss: 1.2049 - acc: 0.5201 - val_loss: 1.2572 - val_acc: 0.5175\n",
      "Epoch 10/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 1.1783 - acc: 0.5338 - val_loss: 1.2176 - val_acc: 0.5334\n",
      "Epoch 11/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 1.1581 - acc: 0.5414 - val_loss: 1.2072 - val_acc: 0.5364\n",
      "Epoch 12/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 1.0582 - acc: 0.5824 - val_loss: 1.0151 - val_acc: 0.6054\n",
      "Epoch 13/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.9606 - acc: 0.6194 - val_loss: 1.0194 - val_acc: 0.5984\n",
      "Epoch 14/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.9447 - acc: 0.6242 - val_loss: 1.0566 - val_acc: 0.5905\n",
      "Epoch 15/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.9191 - acc: 0.6414 - val_loss: 0.9279 - val_acc: 0.6600\n",
      "Epoch 16/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.8445 - acc: 0.6834 - val_loss: 0.9175 - val_acc: 0.6647\n",
      "Epoch 17/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.8253 - acc: 0.6909 - val_loss: 0.8934 - val_acc: 0.6756\n",
      "Epoch 18/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.8183 - acc: 0.6938 - val_loss: 0.8935 - val_acc: 0.6661\n",
      "Epoch 19/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.8128 - acc: 0.6948 - val_loss: 0.8909 - val_acc: 0.6746\n",
      "Epoch 20/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.8114 - acc: 0.6949 - val_loss: 0.9024 - val_acc: 0.6646\n",
      "Epoch 21/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.8018 - acc: 0.6986 - val_loss: 0.9076 - val_acc: 0.6612\n",
      "Epoch 22/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.7976 - acc: 0.6983 - val_loss: 0.9018 - val_acc: 0.6655\n",
      "Epoch 23/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.7871 - acc: 0.7021 - val_loss: 0.9079 - val_acc: 0.6655\n",
      "Epoch 24/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.7856 - acc: 0.7030 - val_loss: 0.8699 - val_acc: 0.6821\n",
      "Epoch 25/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.7834 - acc: 0.7044 - val_loss: 0.8670 - val_acc: 0.6837\n",
      "Epoch 26/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.7785 - acc: 0.7043 - val_loss: 0.8707 - val_acc: 0.6812\n",
      "Epoch 27/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.7703 - acc: 0.7058 - val_loss: 0.8813 - val_acc: 0.6788\n",
      "Epoch 28/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.7704 - acc: 0.7068 - val_loss: 0.8832 - val_acc: 0.6781\n",
      "Epoch 29/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.7616 - acc: 0.7103 - val_loss: 0.8809 - val_acc: 0.6802\n",
      "Epoch 30/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.7597 - acc: 0.7104 - val_loss: 0.8564 - val_acc: 0.6927\n",
      "Epoch 31/100\n",
      "37686/37686 [==============================] - 2s 62us/step - loss: 0.7527 - acc: 0.7123 - val_loss: 0.8790 - val_acc: 0.6748\n",
      "Epoch 32/100\n",
      "37686/37686 [==============================] - 2s 66us/step - loss: 0.7500 - acc: 0.7124 - val_loss: 0.8819 - val_acc: 0.6740\n",
      "Epoch 33/100\n",
      "37686/37686 [==============================] - 3s 66us/step - loss: 0.7500 - acc: 0.7130 - val_loss: 0.8550 - val_acc: 0.6897\n",
      "Epoch 34/100\n",
      "37686/37686 [==============================] - 3s 71us/step - loss: 0.7435 - acc: 0.7164 - val_loss: 0.8278 - val_acc: 0.6973\n",
      "Epoch 35/100\n",
      "37686/37686 [==============================] - 3s 77us/step - loss: 0.7366 - acc: 0.7164 - val_loss: 0.8651 - val_acc: 0.6834\n",
      "Epoch 36/100\n",
      "37686/37686 [==============================] - 3s 68us/step - loss: 0.7329 - acc: 0.7158 - val_loss: 0.8429 - val_acc: 0.6856\n",
      "Epoch 37/100\n",
      "37686/37686 [==============================] - 2s 65us/step - loss: 0.7238 - acc: 0.7198 - val_loss: 0.8624 - val_acc: 0.6802\n",
      "Epoch 38/100\n",
      "37686/37686 [==============================] - 3s 73us/step - loss: 0.7240 - acc: 0.7187 - val_loss: 0.8433 - val_acc: 0.6981\n",
      "Epoch 39/100\n",
      "37686/37686 [==============================] - 3s 74us/step - loss: 0.7165 - acc: 0.7226 - val_loss: 0.8360 - val_acc: 0.6911\n",
      "Epoch 40/100\n",
      "37686/37686 [==============================] - 3s 79us/step - loss: 0.7013 - acc: 0.7324 - val_loss: 0.8211 - val_acc: 0.7212\n",
      "Epoch 41/100\n",
      "37686/37686 [==============================] - 3s 69us/step - loss: 0.6757 - acc: 0.7620 - val_loss: 0.7665 - val_acc: 0.7421\n",
      "Epoch 42/100\n",
      "37686/37686 [==============================] - 3s 70us/step - loss: 0.6443 - acc: 0.7756 - val_loss: 0.7793 - val_acc: 0.7405\n",
      "Epoch 43/100\n",
      "37686/37686 [==============================] - 3s 72us/step - loss: 0.6320 - acc: 0.7807 - val_loss: 0.7489 - val_acc: 0.7545\n",
      "Epoch 44/100\n",
      "37686/37686 [==============================] - 2s 64us/step - loss: 0.6271 - acc: 0.7792 - val_loss: 0.7470 - val_acc: 0.7527\n",
      "Epoch 45/100\n",
      "37686/37686 [==============================] - 2s 66us/step - loss: 0.6152 - acc: 0.7848 - val_loss: 0.7696 - val_acc: 0.7521\n",
      "Epoch 46/100\n",
      "37686/37686 [==============================] - 2s 65us/step - loss: 0.6123 - acc: 0.7842 - val_loss: 0.7454 - val_acc: 0.7585\n",
      "Epoch 47/100\n",
      "37686/37686 [==============================] - 2s 63us/step - loss: 0.6089 - acc: 0.7867 - val_loss: 0.7667 - val_acc: 0.7460\n",
      "Epoch 48/100\n",
      "37686/37686 [==============================] - 3s 71us/step - loss: 0.6047 - acc: 0.7872 - val_loss: 0.7563 - val_acc: 0.7530\n",
      "Epoch 49/100\n",
      "37686/37686 [==============================] - 3s 80us/step - loss: 0.5977 - acc: 0.7896 - val_loss: 0.7415 - val_acc: 0.7585\n",
      "Epoch 50/100\n",
      "37686/37686 [==============================] - 3s 67us/step - loss: 0.6040 - acc: 0.7879 - val_loss: 0.7401 - val_acc: 0.7612\n",
      "Epoch 51/100\n",
      "37686/37686 [==============================] - 3s 71us/step - loss: 0.5997 - acc: 0.7881 - val_loss: 0.7255 - val_acc: 0.7546\n",
      "Epoch 52/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37686/37686 [==============================] - 3s 77us/step - loss: 0.5913 - acc: 0.7917 - val_loss: 0.7514 - val_acc: 0.7548\n",
      "Epoch 53/100\n",
      "37686/37686 [==============================] - 2s 63us/step - loss: 0.5941 - acc: 0.7891 - val_loss: 0.7526 - val_acc: 0.7578\n",
      "Epoch 54/100\n",
      "37686/37686 [==============================] - 2s 62us/step - loss: 0.5924 - acc: 0.7903 - val_loss: 0.7313 - val_acc: 0.7615\n",
      "Epoch 55/100\n",
      "37686/37686 [==============================] - 2s 60us/step - loss: 0.5879 - acc: 0.7925 - val_loss: 0.7674 - val_acc: 0.7476\n",
      "Epoch 56/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.5790 - acc: 0.7955 - val_loss: 0.7701 - val_acc: 0.7629\n",
      "Epoch 57/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.5870 - acc: 0.7939 - val_loss: 0.7532 - val_acc: 0.7549\n",
      "Epoch 58/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.5797 - acc: 0.7944 - val_loss: 0.7727 - val_acc: 0.7451\n",
      "Epoch 59/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.5763 - acc: 0.7954 - val_loss: 0.7784 - val_acc: 0.7556\n",
      "Epoch 60/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.5768 - acc: 0.7959 - val_loss: 0.7644 - val_acc: 0.7540\n",
      "Epoch 61/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.5739 - acc: 0.7974 - val_loss: 0.7508 - val_acc: 0.7628\n",
      "Epoch 62/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.5794 - acc: 0.7963 - val_loss: 0.7284 - val_acc: 0.7616\n",
      "Epoch 63/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.5716 - acc: 0.7975 - val_loss: 0.7344 - val_acc: 0.7660\n",
      "Epoch 64/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.5750 - acc: 0.7972 - val_loss: 0.7354 - val_acc: 0.7646\n",
      "Epoch 65/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.5638 - acc: 0.8002 - val_loss: 0.7358 - val_acc: 0.7676\n",
      "Epoch 66/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.5651 - acc: 0.7998 - val_loss: 0.7076 - val_acc: 0.7678\n",
      "Epoch 67/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.5612 - acc: 0.7983 - val_loss: 0.7239 - val_acc: 0.7707\n",
      "Epoch 68/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.5584 - acc: 0.8027 - val_loss: 0.7124 - val_acc: 0.7777\n",
      "Epoch 69/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.5626 - acc: 0.8024 - val_loss: 0.6981 - val_acc: 0.7785\n",
      "Epoch 70/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.5566 - acc: 0.8045 - val_loss: 0.7089 - val_acc: 0.7786\n",
      "Epoch 71/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.5559 - acc: 0.8033 - val_loss: 0.7014 - val_acc: 0.7689\n",
      "Epoch 72/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.5625 - acc: 0.8015 - val_loss: 0.7205 - val_acc: 0.7674\n",
      "Epoch 73/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.5534 - acc: 0.8032 - val_loss: 0.7169 - val_acc: 0.7618\n",
      "Epoch 74/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.5499 - acc: 0.8044 - val_loss: 0.7044 - val_acc: 0.7811\n",
      "Epoch 75/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.5499 - acc: 0.8028 - val_loss: 0.7305 - val_acc: 0.7676\n",
      "Epoch 76/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.5442 - acc: 0.8081 - val_loss: 0.7390 - val_acc: 0.7744\n",
      "Epoch 77/100\n",
      "37686/37686 [==============================] - 2s 60us/step - loss: 0.5473 - acc: 0.8067 - val_loss: 0.7210 - val_acc: 0.7745\n",
      "Epoch 78/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.5446 - acc: 0.8057 - val_loss: 0.7596 - val_acc: 0.7537\n",
      "Epoch 79/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.5442 - acc: 0.8062 - val_loss: 0.6931 - val_acc: 0.7860\n",
      "Epoch 80/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.5348 - acc: 0.8095 - val_loss: 0.7248 - val_acc: 0.7794\n",
      "Epoch 81/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.5438 - acc: 0.8075 - val_loss: 0.7175 - val_acc: 0.7784\n",
      "Epoch 82/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.5393 - acc: 0.8082 - val_loss: 0.7211 - val_acc: 0.7788\n",
      "Epoch 83/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.5364 - acc: 0.8090 - val_loss: 0.7707 - val_acc: 0.7615\n",
      "Epoch 84/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.5329 - acc: 0.8101 - val_loss: 0.7047 - val_acc: 0.7761\n",
      "Epoch 85/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.5338 - acc: 0.8093 - val_loss: 0.7273 - val_acc: 0.7665\n",
      "Epoch 86/100\n",
      "37686/37686 [==============================] - 2s 62us/step - loss: 0.5296 - acc: 0.8131 - val_loss: 0.7113 - val_acc: 0.7774\n",
      "Epoch 87/100\n",
      "37686/37686 [==============================] - 2s 60us/step - loss: 0.5326 - acc: 0.8119 - val_loss: 0.7025 - val_acc: 0.7765\n",
      "Epoch 88/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.5283 - acc: 0.8124 - val_loss: 0.7079 - val_acc: 0.7810\n",
      "Epoch 89/100\n",
      "37686/37686 [==============================] - 2s 60us/step - loss: 0.5279 - acc: 0.8128 - val_loss: 0.7064 - val_acc: 0.7698\n",
      "Epoch 90/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.5277 - acc: 0.8112 - val_loss: 0.7630 - val_acc: 0.7616\n",
      "Epoch 91/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.5260 - acc: 0.8133 - val_loss: 0.7321 - val_acc: 0.7666\n",
      "Epoch 92/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.5230 - acc: 0.8131 - val_loss: 0.7204 - val_acc: 0.7809\n",
      "Epoch 93/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.5226 - acc: 0.8143 - val_loss: 0.7542 - val_acc: 0.7663\n",
      "Epoch 94/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.5182 - acc: 0.8136 - val_loss: 0.7597 - val_acc: 0.7651\n",
      "Epoch 95/100\n",
      "37686/37686 [==============================] - 2s 60us/step - loss: 0.5204 - acc: 0.8142 - val_loss: 0.7128 - val_acc: 0.7747\n",
      "Epoch 96/100\n",
      "37686/37686 [==============================] - 2s 63us/step - loss: 0.5234 - acc: 0.8129 - val_loss: 0.7311 - val_acc: 0.7792\n",
      "Epoch 97/100\n",
      "37686/37686 [==============================] - 2s 64us/step - loss: 0.5121 - acc: 0.8170 - val_loss: 0.7277 - val_acc: 0.7856\n",
      "Epoch 98/100\n",
      "37686/37686 [==============================] - 2s 61us/step - loss: 0.5184 - acc: 0.8157 - val_loss: 0.7304 - val_acc: 0.7720\n",
      "Epoch 99/100\n",
      "37686/37686 [==============================] - 2s 60us/step - loss: 0.5179 - acc: 0.8141 - val_loss: 0.7040 - val_acc: 0.7848\n",
      "Epoch 100/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.5143 - acc: 0.8164 - val_loss: 0.7320 - val_acc: 0.7704\n",
      "Inner step: 12 / 40\n",
      "Train on 37686 samples, validate on 13278 samples\n",
      "Epoch 1/100\n",
      "37686/37686 [==============================] - 2s 64us/step - loss: 1.3485 - acc: 0.4407 - val_loss: 1.3042 - val_acc: 0.4714\n",
      "Epoch 2/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 1.2795 - acc: 0.4681 - val_loss: 1.2809 - val_acc: 0.4757\n",
      "Epoch 3/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 1.2705 - acc: 0.4706 - val_loss: 1.2808 - val_acc: 0.4818\n",
      "Epoch 4/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 1.2680 - acc: 0.4712 - val_loss: 1.2872 - val_acc: 0.4771\n",
      "Epoch 5/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 1.2657 - acc: 0.4727 - val_loss: 1.3271 - val_acc: 0.4748\n",
      "Epoch 6/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 1.2639 - acc: 0.4718 - val_loss: 1.3092 - val_acc: 0.4793\n",
      "Epoch 7/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 1.2641 - acc: 0.4740 - val_loss: 1.3125 - val_acc: 0.4740\n",
      "Epoch 8/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 1.2534 - acc: 0.4848 - val_loss: 1.2590 - val_acc: 0.5059\n",
      "Epoch 9/100\n",
      "37686/37686 [==============================] - 2s 66us/step - loss: 1.2097 - acc: 0.5117 - val_loss: 1.2415 - val_acc: 0.5108\n",
      "Epoch 10/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37686/37686 [==============================] - 2s 53us/step - loss: 1.1914 - acc: 0.5156 - val_loss: 1.2471 - val_acc: 0.5114\n",
      "Epoch 11/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 1.1839 - acc: 0.5155 - val_loss: 1.2357 - val_acc: 0.5191\n",
      "Epoch 12/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 1.1771 - acc: 0.5208 - val_loss: 1.2203 - val_acc: 0.5171\n",
      "Epoch 13/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 1.1697 - acc: 0.5294 - val_loss: 1.2158 - val_acc: 0.5273\n",
      "Epoch 14/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 1.1627 - acc: 0.5378 - val_loss: 1.2285 - val_acc: 0.5345\n",
      "Epoch 15/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 1.1523 - acc: 0.5423 - val_loss: 1.2102 - val_acc: 0.5331\n",
      "Epoch 16/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 1.1444 - acc: 0.5472 - val_loss: 1.2244 - val_acc: 0.5350\n",
      "Epoch 17/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 1.1397 - acc: 0.5482 - val_loss: 1.2323 - val_acc: 0.5355\n",
      "Epoch 18/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 1.1375 - acc: 0.5502 - val_loss: 1.2057 - val_acc: 0.5395\n",
      "Epoch 19/100\n",
      "37686/37686 [==============================] - 2s 62us/step - loss: 1.1326 - acc: 0.5516 - val_loss: 1.2065 - val_acc: 0.5410\n",
      "Epoch 20/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 1.1336 - acc: 0.5526 - val_loss: 1.2173 - val_acc: 0.5382\n",
      "Epoch 21/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 1.1322 - acc: 0.5519 - val_loss: 1.1966 - val_acc: 0.5413\n",
      "Epoch 22/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 1.1291 - acc: 0.5537 - val_loss: 1.1966 - val_acc: 0.5435\n",
      "Epoch 23/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 1.1259 - acc: 0.5544 - val_loss: 1.2075 - val_acc: 0.5407\n",
      "Epoch 24/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 1.1217 - acc: 0.5561 - val_loss: 1.1964 - val_acc: 0.5372\n",
      "Epoch 25/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 1.1156 - acc: 0.5590 - val_loss: 1.2038 - val_acc: 0.5580\n",
      "Epoch 26/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 1.0670 - acc: 0.5898 - val_loss: 1.0536 - val_acc: 0.6168\n",
      "Epoch 27/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.9587 - acc: 0.6240 - val_loss: 0.9969 - val_acc: 0.6195\n",
      "Epoch 28/100\n",
      "37686/37686 [==============================] - 2s 52us/step - loss: 0.9288 - acc: 0.6317 - val_loss: 1.0693 - val_acc: 0.5892\n",
      "Epoch 29/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 0.9184 - acc: 0.6349 - val_loss: 0.9766 - val_acc: 0.6261\n",
      "Epoch 30/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 0.9147 - acc: 0.6369 - val_loss: 0.9681 - val_acc: 0.6277\n",
      "Epoch 31/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 0.9079 - acc: 0.6380 - val_loss: 0.9836 - val_acc: 0.6223\n",
      "Epoch 32/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 0.9051 - acc: 0.6394 - val_loss: 0.9713 - val_acc: 0.6310\n",
      "Epoch 33/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 0.9016 - acc: 0.6416 - val_loss: 0.9846 - val_acc: 0.6235\n",
      "Epoch 34/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 0.8985 - acc: 0.6404 - val_loss: 0.9891 - val_acc: 0.6167\n",
      "Epoch 35/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 0.8965 - acc: 0.6412 - val_loss: 1.0043 - val_acc: 0.6075\n",
      "Epoch 36/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 0.8908 - acc: 0.6423 - val_loss: 0.9884 - val_acc: 0.6192\n",
      "Epoch 37/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 0.8928 - acc: 0.6422 - val_loss: 1.0133 - val_acc: 0.6042\n",
      "Epoch 38/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 0.8879 - acc: 0.6448 - val_loss: 1.0102 - val_acc: 0.6054\n",
      "Epoch 39/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 0.8899 - acc: 0.6442 - val_loss: 0.9769 - val_acc: 0.6225\n",
      "Epoch 40/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 0.8852 - acc: 0.6462 - val_loss: 0.9691 - val_acc: 0.6332\n",
      "Epoch 41/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.8680 - acc: 0.6606 - val_loss: 0.9247 - val_acc: 0.6617\n",
      "Epoch 42/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.8297 - acc: 0.6871 - val_loss: 0.9132 - val_acc: 0.6573\n",
      "Epoch 43/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.8020 - acc: 0.6981 - val_loss: 0.8889 - val_acc: 0.6764\n",
      "Epoch 44/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.7929 - acc: 0.6989 - val_loss: 0.8889 - val_acc: 0.6738\n",
      "Epoch 45/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 0.7782 - acc: 0.7077 - val_loss: 0.8754 - val_acc: 0.6816\n",
      "Epoch 46/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.7772 - acc: 0.7061 - val_loss: 0.9160 - val_acc: 0.6679\n",
      "Epoch 47/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 0.7491 - acc: 0.7324 - val_loss: 0.8050 - val_acc: 0.7271\n",
      "Epoch 48/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.6854 - acc: 0.7604 - val_loss: 0.8755 - val_acc: 0.6948\n",
      "Epoch 49/100\n",
      "37686/37686 [==============================] - 2s 63us/step - loss: 0.6737 - acc: 0.7650 - val_loss: 0.7782 - val_acc: 0.7392\n",
      "Epoch 50/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.6707 - acc: 0.7658 - val_loss: 0.7953 - val_acc: 0.7296\n",
      "Epoch 51/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.6639 - acc: 0.7687 - val_loss: 0.8093 - val_acc: 0.7322\n",
      "Epoch 52/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 0.6645 - acc: 0.7684 - val_loss: 0.8014 - val_acc: 0.7393\n",
      "Epoch 53/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 0.6579 - acc: 0.7696 - val_loss: 0.7898 - val_acc: 0.7335\n",
      "Epoch 54/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 0.6508 - acc: 0.7736 - val_loss: 0.7934 - val_acc: 0.7350\n",
      "Epoch 55/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.6533 - acc: 0.7730 - val_loss: 0.8000 - val_acc: 0.7298\n",
      "Epoch 56/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.6530 - acc: 0.7722 - val_loss: 0.7833 - val_acc: 0.7412\n",
      "Epoch 57/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.6468 - acc: 0.7741 - val_loss: 0.7834 - val_acc: 0.7375\n",
      "Epoch 58/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.6446 - acc: 0.7758 - val_loss: 0.7708 - val_acc: 0.7419\n",
      "Epoch 59/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 0.6430 - acc: 0.7726 - val_loss: 0.7788 - val_acc: 0.7402\n",
      "Epoch 60/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 0.6395 - acc: 0.7756 - val_loss: 0.7978 - val_acc: 0.7390\n",
      "Epoch 61/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 0.6407 - acc: 0.7725 - val_loss: 0.7773 - val_acc: 0.7381\n",
      "Epoch 62/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 0.6358 - acc: 0.7736 - val_loss: 0.7866 - val_acc: 0.7403\n",
      "Epoch 63/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 0.6352 - acc: 0.7747 - val_loss: 0.7883 - val_acc: 0.7464\n",
      "Epoch 64/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 0.6327 - acc: 0.7761 - val_loss: 0.7820 - val_acc: 0.7407\n",
      "Epoch 65/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 0.6326 - acc: 0.7764 - val_loss: 0.7827 - val_acc: 0.7402\n",
      "Epoch 66/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.6271 - acc: 0.7793 - val_loss: 0.7820 - val_acc: 0.7386\n",
      "Epoch 67/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.6295 - acc: 0.7767 - val_loss: 0.7790 - val_acc: 0.7386\n",
      "Epoch 68/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.6260 - acc: 0.7775 - val_loss: 0.7673 - val_acc: 0.7412\n",
      "Epoch 69/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.6229 - acc: 0.7800 - val_loss: 0.7894 - val_acc: 0.7387\n",
      "Epoch 70/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.6216 - acc: 0.7782 - val_loss: 0.7954 - val_acc: 0.7308\n",
      "Epoch 71/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.6238 - acc: 0.7778 - val_loss: 0.7806 - val_acc: 0.7360\n",
      "Epoch 72/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.6187 - acc: 0.7799 - val_loss: 0.7711 - val_acc: 0.7362\n",
      "Epoch 73/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.6177 - acc: 0.7818 - val_loss: 0.7895 - val_acc: 0.7405\n",
      "Epoch 74/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.6184 - acc: 0.7800 - val_loss: 0.7774 - val_acc: 0.7414\n",
      "Epoch 75/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.6162 - acc: 0.7821 - val_loss: 0.7572 - val_acc: 0.7436\n",
      "Epoch 76/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.6140 - acc: 0.7824 - val_loss: 0.7580 - val_acc: 0.7430\n",
      "Epoch 77/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.6187 - acc: 0.7789 - val_loss: 0.7495 - val_acc: 0.7502\n",
      "Epoch 78/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.6116 - acc: 0.7814 - val_loss: 0.7646 - val_acc: 0.7425\n",
      "Epoch 79/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.6104 - acc: 0.7842 - val_loss: 0.7753 - val_acc: 0.7413\n",
      "Epoch 80/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.6128 - acc: 0.7817 - val_loss: 0.8115 - val_acc: 0.7313\n",
      "Epoch 81/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.6082 - acc: 0.7832 - val_loss: 0.7866 - val_acc: 0.7335\n",
      "Epoch 82/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.6068 - acc: 0.7828 - val_loss: 0.7708 - val_acc: 0.7469\n",
      "Epoch 83/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.6056 - acc: 0.7849 - val_loss: 0.7459 - val_acc: 0.7434\n",
      "Epoch 84/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.6022 - acc: 0.7841 - val_loss: 0.7689 - val_acc: 0.7427\n",
      "Epoch 85/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.6046 - acc: 0.7839 - val_loss: 0.7676 - val_acc: 0.7393\n",
      "Epoch 86/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.6033 - acc: 0.7839 - val_loss: 0.7463 - val_acc: 0.7512\n",
      "Epoch 87/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.6005 - acc: 0.7841 - val_loss: 0.7489 - val_acc: 0.7486\n",
      "Epoch 88/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.6044 - acc: 0.7837 - val_loss: 0.8034 - val_acc: 0.7290\n",
      "Epoch 89/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 0.5975 - acc: 0.7865 - val_loss: 0.7833 - val_acc: 0.7320\n",
      "Epoch 90/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 0.6034 - acc: 0.7849 - val_loss: 0.7481 - val_acc: 0.7483\n",
      "Epoch 91/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 0.5952 - acc: 0.7874 - val_loss: 0.8343 - val_acc: 0.7177\n",
      "Epoch 92/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 0.6010 - acc: 0.7856 - val_loss: 0.7557 - val_acc: 0.7437\n",
      "Epoch 93/100\n",
      "37686/37686 [==============================] - 2s 52us/step - loss: 0.5977 - acc: 0.7857 - val_loss: 0.7531 - val_acc: 0.7442\n",
      "Epoch 94/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 0.5952 - acc: 0.7870 - val_loss: 0.7728 - val_acc: 0.7432\n",
      "Epoch 95/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.5947 - acc: 0.7889 - val_loss: 0.7910 - val_acc: 0.7316\n",
      "Epoch 96/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 0.5942 - acc: 0.7871 - val_loss: 0.7805 - val_acc: 0.7402\n",
      "Epoch 97/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 0.5974 - acc: 0.7854 - val_loss: 0.7731 - val_acc: 0.7515\n",
      "Epoch 98/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 0.5891 - acc: 0.7892 - val_loss: 0.7753 - val_acc: 0.7441\n",
      "Epoch 99/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.5906 - acc: 0.7878 - val_loss: 0.7643 - val_acc: 0.7413\n",
      "Epoch 100/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 0.5901 - acc: 0.7879 - val_loss: 0.7649 - val_acc: 0.7496\n",
      "Inner step: 13 / 40\n",
      "Train on 37686 samples, validate on 13278 samples\n",
      "Epoch 1/100\n",
      "37686/37686 [==============================] - 2s 65us/step - loss: 1.3525 - acc: 0.4386 - val_loss: 1.2944 - val_acc: 0.4752\n",
      "Epoch 2/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 1.2778 - acc: 0.4687 - val_loss: 1.2899 - val_acc: 0.4784\n",
      "Epoch 3/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 1.2737 - acc: 0.4705 - val_loss: 1.2942 - val_acc: 0.4803\n",
      "Epoch 4/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 1.2680 - acc: 0.4740 - val_loss: 1.3058 - val_acc: 0.4815\n",
      "Epoch 5/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 1.2653 - acc: 0.4783 - val_loss: 1.2851 - val_acc: 0.4815\n",
      "Epoch 6/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 1.2648 - acc: 0.4782 - val_loss: 1.2969 - val_acc: 0.4748\n",
      "Epoch 7/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 1.2614 - acc: 0.4806 - val_loss: 1.3294 - val_acc: 0.4765\n",
      "Epoch 8/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 1.2506 - acc: 0.4906 - val_loss: 1.2558 - val_acc: 0.5073\n",
      "Epoch 9/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 1.2039 - acc: 0.5147 - val_loss: 1.2509 - val_acc: 0.5090\n",
      "Epoch 10/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 1.1873 - acc: 0.5187 - val_loss: 1.2149 - val_acc: 0.5252\n",
      "Epoch 11/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 1.1667 - acc: 0.5346 - val_loss: 1.2142 - val_acc: 0.5347\n",
      "Epoch 12/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 1.1512 - acc: 0.5443 - val_loss: 1.1958 - val_acc: 0.5419\n",
      "Epoch 13/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 1.1392 - acc: 0.5487 - val_loss: 1.2093 - val_acc: 0.5413\n",
      "Epoch 14/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 1.1348 - acc: 0.5513 - val_loss: 1.2048 - val_acc: 0.5429\n",
      "Epoch 15/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 1.1308 - acc: 0.5535 - val_loss: 1.2225 - val_acc: 0.5377\n",
      "Epoch 16/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 1.1271 - acc: 0.5532 - val_loss: 1.2256 - val_acc: 0.5403\n",
      "Epoch 17/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 1.1262 - acc: 0.5545 - val_loss: 1.2043 - val_acc: 0.5431\n",
      "Epoch 18/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 1.1205 - acc: 0.5562 - val_loss: 1.1941 - val_acc: 0.5472\n",
      "Epoch 19/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 1.1212 - acc: 0.5559 - val_loss: 1.1877 - val_acc: 0.5453\n",
      "Epoch 20/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 1.1194 - acc: 0.5557 - val_loss: 1.1968 - val_acc: 0.5469\n",
      "Epoch 21/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 1.1155 - acc: 0.5587 - val_loss: 1.1998 - val_acc: 0.5454\n",
      "Epoch 22/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 1.1095 - acc: 0.5601 - val_loss: 1.1944 - val_acc: 0.5473\n",
      "Epoch 23/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 1.1130 - acc: 0.5598 - val_loss: 1.1849 - val_acc: 0.5479\n",
      "Epoch 24/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 1.1109 - acc: 0.5610 - val_loss: 1.2046 - val_acc: 0.5496\n",
      "Epoch 25/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 1.1093 - acc: 0.5606 - val_loss: 1.1979 - val_acc: 0.5454\n",
      "Epoch 26/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 1.1058 - acc: 0.5620 - val_loss: 1.1797 - val_acc: 0.5471\n",
      "Epoch 27/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37686/37686 [==============================] - 2s 58us/step - loss: 1.1074 - acc: 0.5607 - val_loss: 1.1999 - val_acc: 0.5453\n",
      "Epoch 28/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 1.1038 - acc: 0.5624 - val_loss: 1.2058 - val_acc: 0.5461\n",
      "Epoch 29/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 1.1051 - acc: 0.5628 - val_loss: 1.1988 - val_acc: 0.5443\n",
      "Epoch 30/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 1.1031 - acc: 0.5621 - val_loss: 1.1975 - val_acc: 0.5482\n",
      "Epoch 31/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 1.1045 - acc: 0.5617 - val_loss: 1.1972 - val_acc: 0.5447\n",
      "Epoch 32/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 1.0970 - acc: 0.5645 - val_loss: 1.1874 - val_acc: 0.5484\n",
      "Epoch 33/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 1.0963 - acc: 0.5654 - val_loss: 1.1912 - val_acc: 0.5459\n",
      "Epoch 34/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 1.0643 - acc: 0.5825 - val_loss: 1.0913 - val_acc: 0.5907\n",
      "Epoch 35/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.9955 - acc: 0.6092 - val_loss: 1.0337 - val_acc: 0.6139\n",
      "Epoch 36/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.9408 - acc: 0.6251 - val_loss: 0.9849 - val_acc: 0.6201\n",
      "Epoch 37/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.9173 - acc: 0.6293 - val_loss: 0.9802 - val_acc: 0.6243\n",
      "Epoch 38/100\n",
      "37686/37686 [==============================] - 2s 60us/step - loss: 0.8999 - acc: 0.6359 - val_loss: 1.0062 - val_acc: 0.6078\n",
      "Epoch 39/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.8905 - acc: 0.6418 - val_loss: 0.9724 - val_acc: 0.6292\n",
      "Epoch 40/100\n",
      "37686/37686 [==============================] - 2s 60us/step - loss: 0.8859 - acc: 0.6444 - val_loss: 0.9775 - val_acc: 0.6249\n",
      "Epoch 41/100\n",
      "37686/37686 [==============================] - 2s 64us/step - loss: 0.8841 - acc: 0.6443 - val_loss: 1.0032 - val_acc: 0.6069\n",
      "Epoch 42/100\n",
      "37686/37686 [==============================] - 2s 60us/step - loss: 0.8807 - acc: 0.6466 - val_loss: 0.9975 - val_acc: 0.6203\n",
      "Epoch 43/100\n",
      "37686/37686 [==============================] - 2s 60us/step - loss: 0.8804 - acc: 0.6476 - val_loss: 0.9751 - val_acc: 0.6249\n",
      "Epoch 44/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.8711 - acc: 0.6532 - val_loss: 0.9553 - val_acc: 0.6381\n",
      "Epoch 45/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.8618 - acc: 0.6619 - val_loss: 0.9452 - val_acc: 0.6502\n",
      "Epoch 46/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.8304 - acc: 0.6832 - val_loss: 0.9258 - val_acc: 0.6704\n",
      "Epoch 47/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.7824 - acc: 0.7132 - val_loss: 0.8602 - val_acc: 0.7022\n",
      "Epoch 48/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 0.7366 - acc: 0.7402 - val_loss: 0.8361 - val_acc: 0.7155\n",
      "Epoch 49/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 0.7146 - acc: 0.7520 - val_loss: 0.8295 - val_acc: 0.7188\n",
      "Epoch 50/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 0.6948 - acc: 0.7593 - val_loss: 0.8885 - val_acc: 0.6981\n",
      "Epoch 51/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 0.6802 - acc: 0.7626 - val_loss: 0.8161 - val_acc: 0.7281\n",
      "Epoch 52/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.6747 - acc: 0.7661 - val_loss: 0.8141 - val_acc: 0.7292\n",
      "Epoch 53/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.6663 - acc: 0.7700 - val_loss: 0.8029 - val_acc: 0.7312\n",
      "Epoch 54/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.6613 - acc: 0.7720 - val_loss: 0.8186 - val_acc: 0.7249\n",
      "Epoch 55/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 0.6556 - acc: 0.7721 - val_loss: 0.8313 - val_acc: 0.7154\n",
      "Epoch 56/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 0.6534 - acc: 0.7740 - val_loss: 0.8200 - val_acc: 0.7305\n",
      "Epoch 57/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 0.6513 - acc: 0.7744 - val_loss: 0.7809 - val_acc: 0.7402\n",
      "Epoch 58/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 0.6479 - acc: 0.7761 - val_loss: 0.7896 - val_acc: 0.7424\n",
      "Epoch 59/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.6462 - acc: 0.7767 - val_loss: 0.7985 - val_acc: 0.7348\n",
      "Epoch 60/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.6408 - acc: 0.7765 - val_loss: 0.7905 - val_acc: 0.7378\n",
      "Epoch 61/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.6416 - acc: 0.7763 - val_loss: 0.8138 - val_acc: 0.7255\n",
      "Epoch 62/100\n",
      "37686/37686 [==============================] - 2s 61us/step - loss: 0.6362 - acc: 0.7797 - val_loss: 0.8162 - val_acc: 0.7374\n",
      "Epoch 63/100\n",
      "37686/37686 [==============================] - 2s 60us/step - loss: 0.6382 - acc: 0.7780 - val_loss: 0.8239 - val_acc: 0.7332\n",
      "Epoch 64/100\n",
      "37686/37686 [==============================] - 2s 60us/step - loss: 0.6350 - acc: 0.7781 - val_loss: 0.7772 - val_acc: 0.7439\n",
      "Epoch 65/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.6305 - acc: 0.7810 - val_loss: 0.8125 - val_acc: 0.7361\n",
      "Epoch 66/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.6284 - acc: 0.7801 - val_loss: 0.7758 - val_acc: 0.7504\n",
      "Epoch 67/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.6270 - acc: 0.7814 - val_loss: 0.7828 - val_acc: 0.7379\n",
      "Epoch 68/100\n",
      "37686/37686 [==============================] - 3s 71us/step - loss: 0.6273 - acc: 0.7821 - val_loss: 0.7654 - val_acc: 0.7485\n",
      "Epoch 69/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.6257 - acc: 0.7817 - val_loss: 0.8173 - val_acc: 0.7387\n",
      "Epoch 70/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.6203 - acc: 0.7825 - val_loss: 0.7983 - val_acc: 0.7458\n",
      "Epoch 71/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.6235 - acc: 0.7837 - val_loss: 0.7766 - val_acc: 0.7442\n",
      "Epoch 72/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.6180 - acc: 0.7835 - val_loss: 0.8092 - val_acc: 0.7343\n",
      "Epoch 73/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.6172 - acc: 0.7842 - val_loss: 0.8004 - val_acc: 0.7325\n",
      "Epoch 74/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.6212 - acc: 0.7835 - val_loss: 0.8163 - val_acc: 0.7299\n",
      "Epoch 75/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.6187 - acc: 0.7830 - val_loss: 0.7936 - val_acc: 0.7380\n",
      "Epoch 76/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.6140 - acc: 0.7851 - val_loss: 0.7948 - val_acc: 0.7416\n",
      "Epoch 77/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.6086 - acc: 0.7886 - val_loss: 0.7771 - val_acc: 0.7459\n",
      "Epoch 78/100\n",
      "37686/37686 [==============================] - 2s 52us/step - loss: 0.6120 - acc: 0.7849 - val_loss: 0.7573 - val_acc: 0.7553\n",
      "Epoch 79/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 0.6091 - acc: 0.7862 - val_loss: 0.7641 - val_acc: 0.7487\n",
      "Epoch 80/100\n",
      "37686/37686 [==============================] - 2s 52us/step - loss: 0.6095 - acc: 0.7850 - val_loss: 0.7815 - val_acc: 0.7463\n",
      "Epoch 81/100\n",
      "37686/37686 [==============================] - 2s 52us/step - loss: 0.6072 - acc: 0.7864 - val_loss: 0.7558 - val_acc: 0.7464\n",
      "Epoch 82/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 0.6044 - acc: 0.7881 - val_loss: 0.7591 - val_acc: 0.7515\n",
      "Epoch 83/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.6060 - acc: 0.7879 - val_loss: 0.7740 - val_acc: 0.7510\n",
      "Epoch 84/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.6036 - acc: 0.7877 - val_loss: 0.8118 - val_acc: 0.7344\n",
      "Epoch 85/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.6023 - acc: 0.7877 - val_loss: 0.8023 - val_acc: 0.7287\n",
      "Epoch 86/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.6004 - acc: 0.7871 - val_loss: 0.7701 - val_acc: 0.7562\n",
      "Epoch 87/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.5964 - acc: 0.7892 - val_loss: 0.7635 - val_acc: 0.7474\n",
      "Epoch 88/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.5943 - acc: 0.7914 - val_loss: 0.7572 - val_acc: 0.7549\n",
      "Epoch 89/100\n",
      "37686/37686 [==============================] - 2s 63us/step - loss: 0.5982 - acc: 0.7884 - val_loss: 0.7524 - val_acc: 0.7458\n",
      "Epoch 90/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.5964 - acc: 0.7905 - val_loss: 0.7933 - val_acc: 0.7332\n",
      "Epoch 91/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.5967 - acc: 0.7892 - val_loss: 0.7710 - val_acc: 0.7463\n",
      "Epoch 92/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.5927 - acc: 0.7893 - val_loss: 0.7416 - val_acc: 0.7534\n",
      "Epoch 93/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.5900 - acc: 0.7925 - val_loss: 0.7832 - val_acc: 0.7374\n",
      "Epoch 94/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.5945 - acc: 0.7905 - val_loss: 0.7783 - val_acc: 0.7469\n",
      "Epoch 95/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.5886 - acc: 0.7924 - val_loss: 0.7742 - val_acc: 0.7577\n",
      "Epoch 96/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.5878 - acc: 0.7927 - val_loss: 0.7528 - val_acc: 0.7555\n",
      "Epoch 97/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.5908 - acc: 0.7921 - val_loss: 0.7576 - val_acc: 0.7555\n",
      "Epoch 98/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.5891 - acc: 0.7931 - val_loss: 0.7542 - val_acc: 0.7473\n",
      "Epoch 99/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.5886 - acc: 0.7932 - val_loss: 0.7771 - val_acc: 0.7521\n",
      "Epoch 100/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.5812 - acc: 0.7942 - val_loss: 0.7610 - val_acc: 0.7531\n",
      "Inner step: 14 / 40\n",
      "Train on 37686 samples, validate on 13278 samples\n",
      "Epoch 1/100\n",
      "37686/37686 [==============================] - 3s 70us/step - loss: 1.1763 - acc: 0.5120 - val_loss: 1.0346 - val_acc: 0.5728\n",
      "Epoch 2/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 1.0088 - acc: 0.5770 - val_loss: 1.0125 - val_acc: 0.5865\n",
      "Epoch 3/100\n",
      "37686/37686 [==============================] - 2s 60us/step - loss: 0.9936 - acc: 0.5851 - val_loss: 1.0369 - val_acc: 0.5747\n",
      "Epoch 4/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.9804 - acc: 0.5916 - val_loss: 1.0128 - val_acc: 0.5829\n",
      "Epoch 5/100\n",
      "37686/37686 [==============================] - 2s 64us/step - loss: 0.9769 - acc: 0.5911 - val_loss: 0.9924 - val_acc: 0.5929\n",
      "Epoch 6/100\n",
      "37686/37686 [==============================] - 3s 70us/step - loss: 0.9676 - acc: 0.5952 - val_loss: 1.0671 - val_acc: 0.5717\n",
      "Epoch 7/100\n",
      "37686/37686 [==============================] - 2s 60us/step - loss: 0.9643 - acc: 0.5987 - val_loss: 0.9918 - val_acc: 0.6062\n",
      "Epoch 8/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.9628 - acc: 0.6039 - val_loss: 0.9814 - val_acc: 0.6095\n",
      "Epoch 9/100\n",
      "37686/37686 [==============================] - 3s 69us/step - loss: 0.9491 - acc: 0.6136 - val_loss: 1.0113 - val_acc: 0.5983\n",
      "Epoch 10/100\n",
      "37686/37686 [==============================] - 3s 71us/step - loss: 0.9379 - acc: 0.6189 - val_loss: 1.0065 - val_acc: 0.6178\n",
      "Epoch 11/100\n",
      "37686/37686 [==============================] - 3s 68us/step - loss: 0.9301 - acc: 0.6233 - val_loss: 0.9785 - val_acc: 0.6152\n",
      "Epoch 12/100\n",
      "37686/37686 [==============================] - 2s 62us/step - loss: 0.9259 - acc: 0.6292 - val_loss: 0.9974 - val_acc: 0.6131\n",
      "Epoch 13/100\n",
      "37686/37686 [==============================] - 2s 60us/step - loss: 0.9169 - acc: 0.6321 - val_loss: 0.9769 - val_acc: 0.6302\n",
      "Epoch 14/100\n",
      "37686/37686 [==============================] - 2s 62us/step - loss: 0.9130 - acc: 0.6370 - val_loss: 1.0373 - val_acc: 0.5827\n",
      "Epoch 15/100\n",
      "37686/37686 [==============================] - 2s 60us/step - loss: 0.9124 - acc: 0.6399 - val_loss: 0.9978 - val_acc: 0.6116\n",
      "Epoch 16/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.9044 - acc: 0.6403 - val_loss: 0.9966 - val_acc: 0.6252\n",
      "Epoch 17/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.9048 - acc: 0.6424 - val_loss: 1.0032 - val_acc: 0.6158\n",
      "Epoch 18/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.8978 - acc: 0.6455 - val_loss: 0.9722 - val_acc: 0.6289\n",
      "Epoch 19/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.8963 - acc: 0.6478 - val_loss: 0.9655 - val_acc: 0.6408\n",
      "Epoch 20/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.8961 - acc: 0.6493 - val_loss: 0.9788 - val_acc: 0.6288\n",
      "Epoch 21/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.8943 - acc: 0.6564 - val_loss: 0.9952 - val_acc: 0.6473\n",
      "Epoch 22/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.8754 - acc: 0.6696 - val_loss: 0.9601 - val_acc: 0.6411\n",
      "Epoch 23/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.8643 - acc: 0.6733 - val_loss: 0.9301 - val_acc: 0.6603\n",
      "Epoch 24/100\n",
      "37686/37686 [==============================] - 2s 61us/step - loss: 0.8533 - acc: 0.6778 - val_loss: 0.9559 - val_acc: 0.6529\n",
      "Epoch 25/100\n",
      "37686/37686 [==============================] - 2s 60us/step - loss: 0.8530 - acc: 0.6752 - val_loss: 0.9144 - val_acc: 0.6609\n",
      "Epoch 26/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.8410 - acc: 0.6795 - val_loss: 0.9122 - val_acc: 0.6648\n",
      "Epoch 27/100\n",
      "37686/37686 [==============================] - 2s 63us/step - loss: 0.8373 - acc: 0.6798 - val_loss: 0.9019 - val_acc: 0.6692\n",
      "Epoch 28/100\n",
      "37686/37686 [==============================] - 2s 61us/step - loss: 0.8363 - acc: 0.6789 - val_loss: 0.8953 - val_acc: 0.6697\n",
      "Epoch 29/100\n",
      "37686/37686 [==============================] - 3s 67us/step - loss: 0.8317 - acc: 0.6801 - val_loss: 0.9141 - val_acc: 0.6628\n",
      "Epoch 30/100\n",
      "37686/37686 [==============================] - 2s 62us/step - loss: 0.8289 - acc: 0.6789 - val_loss: 0.8955 - val_acc: 0.6722\n",
      "Epoch 31/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.8300 - acc: 0.6757 - val_loss: 0.9161 - val_acc: 0.6628\n",
      "Epoch 32/100\n",
      "37686/37686 [==============================] - 3s 78us/step - loss: 0.8283 - acc: 0.6736 - val_loss: 0.9064 - val_acc: 0.6686\n",
      "Epoch 33/100\n",
      "37686/37686 [==============================] - 3s 75us/step - loss: 0.8240 - acc: 0.6736 - val_loss: 0.8952 - val_acc: 0.6664\n",
      "Epoch 34/100\n",
      "37686/37686 [==============================] - 2s 62us/step - loss: 0.7795 - acc: 0.7069 - val_loss: 0.8339 - val_acc: 0.6993\n",
      "Epoch 35/100\n",
      "37686/37686 [==============================] - 3s 67us/step - loss: 0.7237 - acc: 0.7320 - val_loss: 0.8194 - val_acc: 0.7066\n",
      "Epoch 36/100\n",
      "37686/37686 [==============================] - 3s 68us/step - loss: 0.7143 - acc: 0.7329 - val_loss: 0.8192 - val_acc: 0.7016\n",
      "Epoch 37/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.7089 - acc: 0.7327 - val_loss: 0.7963 - val_acc: 0.7121\n",
      "Epoch 38/100\n",
      "37686/37686 [==============================] - 2s 62us/step - loss: 0.7050 - acc: 0.7348 - val_loss: 0.8309 - val_acc: 0.6981\n",
      "Epoch 39/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.7033 - acc: 0.7353 - val_loss: 0.8020 - val_acc: 0.7146\n",
      "Epoch 40/100\n",
      "37686/37686 [==============================] - 3s 72us/step - loss: 0.7021 - acc: 0.7349 - val_loss: 0.7922 - val_acc: 0.7104\n",
      "Epoch 41/100\n",
      "37686/37686 [==============================] - 3s 70us/step - loss: 0.6984 - acc: 0.7353 - val_loss: 0.7989 - val_acc: 0.7106\n",
      "Epoch 42/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.6971 - acc: 0.7362 - val_loss: 0.7897 - val_acc: 0.7093\n",
      "Epoch 43/100\n",
      "37686/37686 [==============================] - 2s 64us/step - loss: 0.6952 - acc: 0.7364 - val_loss: 0.7927 - val_acc: 0.7126\n",
      "Epoch 44/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37686/37686 [==============================] - 2s 60us/step - loss: 0.6911 - acc: 0.7369 - val_loss: 0.8153 - val_acc: 0.7054\n",
      "Epoch 45/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.6958 - acc: 0.7350 - val_loss: 0.7902 - val_acc: 0.7175\n",
      "Epoch 46/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.6924 - acc: 0.7375 - val_loss: 0.7920 - val_acc: 0.7155\n",
      "Epoch 47/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.6846 - acc: 0.7400 - val_loss: 0.8042 - val_acc: 0.7113\n",
      "Epoch 48/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.6844 - acc: 0.7394 - val_loss: 0.8056 - val_acc: 0.7190\n",
      "Epoch 49/100\n",
      "37686/37686 [==============================] - 2s 66us/step - loss: 0.6856 - acc: 0.7400 - val_loss: 0.7905 - val_acc: 0.7236\n",
      "Epoch 50/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.6856 - acc: 0.7395 - val_loss: 0.8144 - val_acc: 0.7080\n",
      "Epoch 51/100\n",
      "37686/37686 [==============================] - 3s 69us/step - loss: 0.6881 - acc: 0.7377 - val_loss: 0.8072 - val_acc: 0.7085\n",
      "Epoch 52/100\n",
      "37686/37686 [==============================] - 2s 61us/step - loss: 0.6832 - acc: 0.7380 - val_loss: 0.8134 - val_acc: 0.7094\n",
      "Epoch 53/100\n",
      "37686/37686 [==============================] - 2s 62us/step - loss: 0.6820 - acc: 0.7405 - val_loss: 0.7932 - val_acc: 0.7180\n",
      "Epoch 54/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.6787 - acc: 0.7416 - val_loss: 0.7937 - val_acc: 0.7239\n",
      "Epoch 55/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.6795 - acc: 0.7418 - val_loss: 0.7973 - val_acc: 0.7164\n",
      "Epoch 56/100\n",
      "37686/37686 [==============================] - 2s 62us/step - loss: 0.6735 - acc: 0.7410 - val_loss: 0.8017 - val_acc: 0.7043\n",
      "Epoch 57/100\n",
      "37686/37686 [==============================] - 3s 69us/step - loss: 0.6758 - acc: 0.7453 - val_loss: 0.7689 - val_acc: 0.7293\n",
      "Epoch 58/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.6694 - acc: 0.7468 - val_loss: 0.7953 - val_acc: 0.7246\n",
      "Epoch 59/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.6658 - acc: 0.7509 - val_loss: 0.7871 - val_acc: 0.7205\n",
      "Epoch 60/100\n",
      "37686/37686 [==============================] - 2s 61us/step - loss: 0.6624 - acc: 0.7524 - val_loss: 0.7870 - val_acc: 0.7298\n",
      "Epoch 61/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.6634 - acc: 0.7508 - val_loss: 0.8121 - val_acc: 0.7251\n",
      "Epoch 62/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.6614 - acc: 0.7536 - val_loss: 0.7844 - val_acc: 0.7324\n",
      "Epoch 63/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.6567 - acc: 0.7575 - val_loss: 0.7861 - val_acc: 0.7272\n",
      "Epoch 64/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.6561 - acc: 0.7561 - val_loss: 0.7737 - val_acc: 0.7271\n",
      "Epoch 65/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.6494 - acc: 0.7593 - val_loss: 0.7972 - val_acc: 0.7205\n",
      "Epoch 66/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.6529 - acc: 0.7581 - val_loss: 0.7460 - val_acc: 0.7424\n",
      "Epoch 67/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.6498 - acc: 0.7589 - val_loss: 0.7555 - val_acc: 0.7409\n",
      "Epoch 68/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.6455 - acc: 0.7608 - val_loss: 0.7690 - val_acc: 0.7320\n",
      "Epoch 69/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.6441 - acc: 0.7628 - val_loss: 0.7684 - val_acc: 0.7389\n",
      "Epoch 70/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.6445 - acc: 0.7618 - val_loss: 0.7616 - val_acc: 0.7352\n",
      "Epoch 71/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.6443 - acc: 0.7651 - val_loss: 0.7833 - val_acc: 0.7322\n",
      "Epoch 72/100\n",
      "37686/37686 [==============================] - 2s 63us/step - loss: 0.6396 - acc: 0.7681 - val_loss: 0.7728 - val_acc: 0.7356\n",
      "Epoch 73/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.6368 - acc: 0.7665 - val_loss: 0.7644 - val_acc: 0.7469\n",
      "Epoch 74/100\n",
      "37686/37686 [==============================] - 2s 62us/step - loss: 0.6375 - acc: 0.7687 - val_loss: 0.7759 - val_acc: 0.7471\n",
      "Epoch 75/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.6359 - acc: 0.7704 - val_loss: 0.7572 - val_acc: 0.7430\n",
      "Epoch 76/100\n",
      "37686/37686 [==============================] - 2s 63us/step - loss: 0.6288 - acc: 0.7722 - val_loss: 0.7535 - val_acc: 0.7424\n",
      "Epoch 77/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.6282 - acc: 0.7752 - val_loss: 0.7549 - val_acc: 0.7399\n",
      "Epoch 78/100\n",
      "37686/37686 [==============================] - 2s 61us/step - loss: 0.6250 - acc: 0.7745 - val_loss: 0.7542 - val_acc: 0.7456\n",
      "Epoch 79/100\n",
      "37686/37686 [==============================] - 2s 66us/step - loss: 0.6259 - acc: 0.7763 - val_loss: 0.7849 - val_acc: 0.7235\n",
      "Epoch 80/100\n",
      "37686/37686 [==============================] - 2s 60us/step - loss: 0.6197 - acc: 0.7808 - val_loss: 0.7562 - val_acc: 0.7439\n",
      "Epoch 81/100\n",
      "37686/37686 [==============================] - 2s 65us/step - loss: 0.6183 - acc: 0.7789 - val_loss: 0.7667 - val_acc: 0.7417\n",
      "Epoch 82/100\n",
      "37686/37686 [==============================] - 2s 64us/step - loss: 0.6212 - acc: 0.7762 - val_loss: 0.7453 - val_acc: 0.7453\n",
      "Epoch 83/100\n",
      "37686/37686 [==============================] - 2s 63us/step - loss: 0.6194 - acc: 0.7789 - val_loss: 0.7512 - val_acc: 0.7457\n",
      "Epoch 84/100\n",
      "37686/37686 [==============================] - 2s 66us/step - loss: 0.6145 - acc: 0.7808 - val_loss: 0.7378 - val_acc: 0.7511\n",
      "Epoch 85/100\n",
      "37686/37686 [==============================] - 2s 65us/step - loss: 0.6142 - acc: 0.7794 - val_loss: 0.7696 - val_acc: 0.7430\n",
      "Epoch 86/100\n",
      "37686/37686 [==============================] - 2s 62us/step - loss: 0.6130 - acc: 0.7813 - val_loss: 0.7399 - val_acc: 0.7508\n",
      "Epoch 87/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.6108 - acc: 0.7830 - val_loss: 0.7499 - val_acc: 0.7454\n",
      "Epoch 88/100\n",
      "37686/37686 [==============================] - 3s 78us/step - loss: 0.6131 - acc: 0.7808 - val_loss: 0.7726 - val_acc: 0.7363\n",
      "Epoch 89/100\n",
      "37686/37686 [==============================] - 3s 71us/step - loss: 0.6084 - acc: 0.7847 - val_loss: 0.7586 - val_acc: 0.7436\n",
      "Epoch 90/100\n",
      "37686/37686 [==============================] - 3s 74us/step - loss: 0.6098 - acc: 0.7846 - val_loss: 0.7449 - val_acc: 0.7472\n",
      "Epoch 91/100\n",
      "37686/37686 [==============================] - 2s 61us/step - loss: 0.6079 - acc: 0.7830 - val_loss: 0.7432 - val_acc: 0.7503\n",
      "Epoch 92/100\n",
      "37686/37686 [==============================] - 2s 60us/step - loss: 0.6071 - acc: 0.7841 - val_loss: 0.7383 - val_acc: 0.7483\n",
      "Epoch 93/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.6067 - acc: 0.7849 - val_loss: 0.7489 - val_acc: 0.7460\n",
      "Epoch 94/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.6017 - acc: 0.7858 - val_loss: 0.7531 - val_acc: 0.7556\n",
      "Epoch 95/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.6005 - acc: 0.7863 - val_loss: 0.7407 - val_acc: 0.7515\n",
      "Epoch 96/100\n",
      "37686/37686 [==============================] - 2s 62us/step - loss: 0.6001 - acc: 0.7877 - val_loss: 0.7383 - val_acc: 0.7512\n",
      "Epoch 97/100\n",
      "37686/37686 [==============================] - 2s 63us/step - loss: 0.6052 - acc: 0.7843 - val_loss: 0.7283 - val_acc: 0.7546\n",
      "Epoch 98/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.5999 - acc: 0.7872 - val_loss: 0.7570 - val_acc: 0.7458\n",
      "Epoch 99/100\n",
      "37686/37686 [==============================] - 2s 62us/step - loss: 0.5993 - acc: 0.7859 - val_loss: 0.7701 - val_acc: 0.7448\n",
      "Epoch 100/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.5988 - acc: 0.7882 - val_loss: 0.7668 - val_acc: 0.7470\n",
      "Inner step: 15 / 40\n",
      "Train on 37686 samples, validate on 13278 samples\n",
      "Epoch 1/100\n",
      "37686/37686 [==============================] - 3s 75us/step - loss: 1.4022 - acc: 0.4165 - val_loss: 1.3683 - val_acc: 0.4318\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37686/37686 [==============================] - 2s 57us/step - loss: 1.2858 - acc: 0.4691 - val_loss: 1.2867 - val_acc: 0.4791\n",
      "Epoch 3/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 1.2548 - acc: 0.4835 - val_loss: 1.3332 - val_acc: 0.4496\n",
      "Epoch 4/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 1.2259 - acc: 0.4990 - val_loss: 1.2710 - val_acc: 0.4979\n",
      "Epoch 5/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 1.2019 - acc: 0.5088 - val_loss: 1.2679 - val_acc: 0.4980\n",
      "Epoch 6/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 1.1932 - acc: 0.5123 - val_loss: 1.2790 - val_acc: 0.4975\n",
      "Epoch 7/100\n",
      "37686/37686 [==============================] - 2s 60us/step - loss: 1.1876 - acc: 0.5150 - val_loss: 1.2573 - val_acc: 0.5035\n",
      "Epoch 8/100\n",
      "37686/37686 [==============================] - 3s 67us/step - loss: 1.1849 - acc: 0.5158 - val_loss: 1.2743 - val_acc: 0.4996\n",
      "Epoch 9/100\n",
      "37686/37686 [==============================] - 2s 60us/step - loss: 1.1814 - acc: 0.5179 - val_loss: 1.2389 - val_acc: 0.5028\n",
      "Epoch 10/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 1.0634 - acc: 0.5633 - val_loss: 1.0904 - val_acc: 0.5575\n",
      "Epoch 11/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.9935 - acc: 0.5884 - val_loss: 1.0257 - val_acc: 0.5798\n",
      "Epoch 12/100\n",
      "37686/37686 [==============================] - 2s 60us/step - loss: 0.9769 - acc: 0.5944 - val_loss: 1.0089 - val_acc: 0.5907\n",
      "Epoch 13/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.9742 - acc: 0.5942 - val_loss: 1.0467 - val_acc: 0.5668\n",
      "Epoch 14/100\n",
      "37686/37686 [==============================] - 2s 66us/step - loss: 0.9665 - acc: 0.5965 - val_loss: 1.0223 - val_acc: 0.5858\n",
      "Epoch 15/100\n",
      "37686/37686 [==============================] - 2s 61us/step - loss: 0.9648 - acc: 0.5962 - val_loss: 1.0050 - val_acc: 0.5896\n",
      "Epoch 16/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.9602 - acc: 0.5980 - val_loss: 1.0092 - val_acc: 0.5882\n",
      "Epoch 17/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.9603 - acc: 0.5975 - val_loss: 1.0243 - val_acc: 0.5802\n",
      "Epoch 18/100\n",
      "37686/37686 [==============================] - 2s 62us/step - loss: 0.9533 - acc: 0.5995 - val_loss: 1.0540 - val_acc: 0.5490\n",
      "Epoch 19/100\n",
      "37686/37686 [==============================] - 2s 61us/step - loss: 0.8941 - acc: 0.6354 - val_loss: 0.9239 - val_acc: 0.6418\n",
      "Epoch 20/100\n",
      "37686/37686 [==============================] - 2s 60us/step - loss: 0.8545 - acc: 0.6573 - val_loss: 0.9188 - val_acc: 0.6307\n",
      "Epoch 21/100\n",
      "37686/37686 [==============================] - 2s 60us/step - loss: 0.8437 - acc: 0.6590 - val_loss: 0.9662 - val_acc: 0.6144\n",
      "Epoch 22/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.8403 - acc: 0.6603 - val_loss: 0.9347 - val_acc: 0.6394\n",
      "Epoch 23/100\n",
      "37686/37686 [==============================] - 3s 68us/step - loss: 0.8368 - acc: 0.6624 - val_loss: 0.8943 - val_acc: 0.6475\n",
      "Epoch 24/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.8296 - acc: 0.6632 - val_loss: 0.9422 - val_acc: 0.6221\n",
      "Epoch 25/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.8269 - acc: 0.6657 - val_loss: 0.9036 - val_acc: 0.6479\n",
      "Epoch 26/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.8274 - acc: 0.6655 - val_loss: 0.9075 - val_acc: 0.6432\n",
      "Epoch 27/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.8257 - acc: 0.6636 - val_loss: 0.8994 - val_acc: 0.6470\n",
      "Epoch 28/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.8225 - acc: 0.6647 - val_loss: 0.9013 - val_acc: 0.6431\n",
      "Epoch 29/100\n",
      "37686/37686 [==============================] - 2s 62us/step - loss: 0.8165 - acc: 0.6652 - val_loss: 0.9180 - val_acc: 0.6284\n",
      "Epoch 30/100\n",
      "37686/37686 [==============================] - 2s 63us/step - loss: 0.8120 - acc: 0.6688 - val_loss: 0.8968 - val_acc: 0.6478\n",
      "Epoch 31/100\n",
      "37686/37686 [==============================] - 2s 61us/step - loss: 0.8060 - acc: 0.6685 - val_loss: 0.8737 - val_acc: 0.6530\n",
      "Epoch 32/100\n",
      "37686/37686 [==============================] - 2s 63us/step - loss: 0.7962 - acc: 0.6691 - val_loss: 0.8885 - val_acc: 0.6484\n",
      "Epoch 33/100\n",
      "37686/37686 [==============================] - 2s 60us/step - loss: 0.7958 - acc: 0.6682 - val_loss: 0.8677 - val_acc: 0.6539\n",
      "Epoch 34/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.7704 - acc: 0.6908 - val_loss: 0.8304 - val_acc: 0.6900\n",
      "Epoch 35/100\n",
      "37686/37686 [==============================] - 2s 64us/step - loss: 0.7345 - acc: 0.7183 - val_loss: 0.8072 - val_acc: 0.7000\n",
      "Epoch 36/100\n",
      "37686/37686 [==============================] - 2s 61us/step - loss: 0.7187 - acc: 0.7258 - val_loss: 0.8045 - val_acc: 0.7044\n",
      "Epoch 37/100\n",
      "37686/37686 [==============================] - 3s 68us/step - loss: 0.7131 - acc: 0.7304 - val_loss: 0.8054 - val_acc: 0.7046\n",
      "Epoch 38/100\n",
      "37686/37686 [==============================] - 2s 61us/step - loss: 0.7067 - acc: 0.7306 - val_loss: 0.8006 - val_acc: 0.7177\n",
      "Epoch 39/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.7059 - acc: 0.7311 - val_loss: 0.7989 - val_acc: 0.7140\n",
      "Epoch 40/100\n",
      "37686/37686 [==============================] - 3s 77us/step - loss: 0.7025 - acc: 0.7323 - val_loss: 0.7955 - val_acc: 0.7115\n",
      "Epoch 41/100\n",
      "37686/37686 [==============================] - 3s 70us/step - loss: 0.6955 - acc: 0.7358 - val_loss: 0.7956 - val_acc: 0.7090\n",
      "Epoch 42/100\n",
      "37686/37686 [==============================] - 3s 68us/step - loss: 0.6975 - acc: 0.7338 - val_loss: 0.7934 - val_acc: 0.7120\n",
      "Epoch 43/100\n",
      "37686/37686 [==============================] - 3s 70us/step - loss: 0.6923 - acc: 0.7370 - val_loss: 0.8213 - val_acc: 0.7079\n",
      "Epoch 44/100\n",
      "37686/37686 [==============================] - 3s 69us/step - loss: 0.6942 - acc: 0.7366 - val_loss: 0.7883 - val_acc: 0.7203\n",
      "Epoch 45/100\n",
      "37686/37686 [==============================] - 2s 61us/step - loss: 0.6913 - acc: 0.7348 - val_loss: 0.7919 - val_acc: 0.7200\n",
      "Epoch 46/100\n",
      "37686/37686 [==============================] - 2s 62us/step - loss: 0.6836 - acc: 0.7378 - val_loss: 0.8167 - val_acc: 0.7122\n",
      "Epoch 47/100\n",
      "37686/37686 [==============================] - 2s 62us/step - loss: 0.6843 - acc: 0.7375 - val_loss: 0.8447 - val_acc: 0.7049\n",
      "Epoch 48/100\n",
      "37686/37686 [==============================] - 2s 65us/step - loss: 0.6789 - acc: 0.7380 - val_loss: 0.7803 - val_acc: 0.7170\n",
      "Epoch 49/100\n",
      "37686/37686 [==============================] - 2s 65us/step - loss: 0.6767 - acc: 0.7416 - val_loss: 0.8038 - val_acc: 0.7039\n",
      "Epoch 50/100\n",
      "37686/37686 [==============================] - 2s 60us/step - loss: 0.6780 - acc: 0.7401 - val_loss: 0.8001 - val_acc: 0.7099\n",
      "Epoch 51/100\n",
      "37686/37686 [==============================] - 2s 62us/step - loss: 0.6720 - acc: 0.7397 - val_loss: 0.8029 - val_acc: 0.7067\n",
      "Epoch 52/100\n",
      "37686/37686 [==============================] - 2s 60us/step - loss: 0.6698 - acc: 0.7435 - val_loss: 0.7716 - val_acc: 0.7176\n",
      "Epoch 53/100\n",
      "37686/37686 [==============================] - 2s 63us/step - loss: 0.6699 - acc: 0.7421 - val_loss: 0.8022 - val_acc: 0.7149\n",
      "Epoch 54/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.6700 - acc: 0.7434 - val_loss: 0.7729 - val_acc: 0.7196\n",
      "Epoch 55/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.6674 - acc: 0.7420 - val_loss: 0.7763 - val_acc: 0.7292\n",
      "Epoch 56/100\n",
      "37686/37686 [==============================] - 2s 60us/step - loss: 0.6638 - acc: 0.7430 - val_loss: 0.7884 - val_acc: 0.7120\n",
      "Epoch 57/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.6612 - acc: 0.7473 - val_loss: 0.7728 - val_acc: 0.7207\n",
      "Epoch 58/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.6578 - acc: 0.7470 - val_loss: 0.7924 - val_acc: 0.7157\n",
      "Epoch 59/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.6647 - acc: 0.7445 - val_loss: 0.7702 - val_acc: 0.7168\n",
      "Epoch 60/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.6600 - acc: 0.7431 - val_loss: 0.7898 - val_acc: 0.7175\n",
      "Epoch 61/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.6566 - acc: 0.7462 - val_loss: 0.7801 - val_acc: 0.7146\n",
      "Epoch 62/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.6554 - acc: 0.7471 - val_loss: 0.7723 - val_acc: 0.7302\n",
      "Epoch 63/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.6544 - acc: 0.7493 - val_loss: 0.7726 - val_acc: 0.7186\n",
      "Epoch 64/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.6518 - acc: 0.7495 - val_loss: 0.7588 - val_acc: 0.7261\n",
      "Epoch 65/100\n",
      "37686/37686 [==============================] - 2s 60us/step - loss: 0.6495 - acc: 0.7506 - val_loss: 0.7680 - val_acc: 0.7265\n",
      "Epoch 66/100\n",
      "37686/37686 [==============================] - 2s 60us/step - loss: 0.6481 - acc: 0.7534 - val_loss: 0.7726 - val_acc: 0.7262\n",
      "Epoch 67/100\n",
      "37686/37686 [==============================] - 2s 60us/step - loss: 0.6466 - acc: 0.7524 - val_loss: 0.7840 - val_acc: 0.7205\n",
      "Epoch 68/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.6436 - acc: 0.7550 - val_loss: 0.7561 - val_acc: 0.7329\n",
      "Epoch 69/100\n",
      "37686/37686 [==============================] - 2s 62us/step - loss: 0.6419 - acc: 0.7558 - val_loss: 0.7550 - val_acc: 0.7360\n",
      "Epoch 70/100\n",
      "37686/37686 [==============================] - 2s 65us/step - loss: 0.6418 - acc: 0.7590 - val_loss: 0.7762 - val_acc: 0.7266\n",
      "Epoch 71/100\n",
      "37686/37686 [==============================] - 2s 61us/step - loss: 0.6416 - acc: 0.7574 - val_loss: 0.7865 - val_acc: 0.7319\n",
      "Epoch 72/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.6415 - acc: 0.7572 - val_loss: 0.7589 - val_acc: 0.7212\n",
      "Epoch 73/100\n",
      "37686/37686 [==============================] - 2s 64us/step - loss: 0.6354 - acc: 0.7605 - val_loss: 0.7908 - val_acc: 0.7272\n",
      "Epoch 74/100\n",
      "37686/37686 [==============================] - 2s 65us/step - loss: 0.6333 - acc: 0.7622 - val_loss: 0.7329 - val_acc: 0.7439\n",
      "Epoch 75/100\n",
      "37686/37686 [==============================] - 2s 60us/step - loss: 0.6308 - acc: 0.7651 - val_loss: 0.7494 - val_acc: 0.7402\n",
      "Epoch 76/100\n",
      "37686/37686 [==============================] - 2s 62us/step - loss: 0.6322 - acc: 0.7653 - val_loss: 0.7449 - val_acc: 0.7369\n",
      "Epoch 77/100\n",
      "37686/37686 [==============================] - 2s 61us/step - loss: 0.6282 - acc: 0.7654 - val_loss: 0.7377 - val_acc: 0.7483\n",
      "Epoch 78/100\n",
      "37686/37686 [==============================] - 2s 64us/step - loss: 0.6258 - acc: 0.7681 - val_loss: 0.7524 - val_acc: 0.7448\n",
      "Epoch 79/100\n",
      "37686/37686 [==============================] - 2s 60us/step - loss: 0.6234 - acc: 0.7696 - val_loss: 0.7628 - val_acc: 0.7413\n",
      "Epoch 80/100\n",
      "37686/37686 [==============================] - 2s 61us/step - loss: 0.6244 - acc: 0.7678 - val_loss: 0.7402 - val_acc: 0.7517\n",
      "Epoch 81/100\n",
      "37686/37686 [==============================] - 2s 60us/step - loss: 0.6206 - acc: 0.7716 - val_loss: 0.7214 - val_acc: 0.7506\n",
      "Epoch 82/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.6188 - acc: 0.7713 - val_loss: 0.7542 - val_acc: 0.7413\n",
      "Epoch 83/100\n",
      "37686/37686 [==============================] - 2s 62us/step - loss: 0.6160 - acc: 0.7752 - val_loss: 0.7249 - val_acc: 0.7529\n",
      "Epoch 84/100\n",
      "37686/37686 [==============================] - 2s 61us/step - loss: 0.6198 - acc: 0.7735 - val_loss: 0.7375 - val_acc: 0.7466\n",
      "Epoch 85/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.6153 - acc: 0.7767 - val_loss: 0.7386 - val_acc: 0.7568\n",
      "Epoch 86/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.6139 - acc: 0.7778 - val_loss: 0.7205 - val_acc: 0.7547\n",
      "Epoch 87/100\n",
      "37686/37686 [==============================] - 2s 64us/step - loss: 0.6115 - acc: 0.7801 - val_loss: 0.7150 - val_acc: 0.7606\n",
      "Epoch 88/100\n",
      "37686/37686 [==============================] - 2s 62us/step - loss: 0.6105 - acc: 0.7799 - val_loss: 0.7523 - val_acc: 0.7445\n",
      "Epoch 89/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.6059 - acc: 0.7808 - val_loss: 0.7342 - val_acc: 0.7546\n",
      "Epoch 90/100\n",
      "37686/37686 [==============================] - 2s 66us/step - loss: 0.6023 - acc: 0.7839 - val_loss: 0.7235 - val_acc: 0.7557\n",
      "Epoch 91/100\n",
      "37686/37686 [==============================] - 2s 60us/step - loss: 0.6015 - acc: 0.7834 - val_loss: 0.7395 - val_acc: 0.7540\n",
      "Epoch 92/100\n",
      "37686/37686 [==============================] - 2s 60us/step - loss: 0.5970 - acc: 0.7855 - val_loss: 0.7237 - val_acc: 0.7543\n",
      "Epoch 93/100\n",
      "37686/37686 [==============================] - 2s 62us/step - loss: 0.6004 - acc: 0.7845 - val_loss: 0.7209 - val_acc: 0.7608\n",
      "Epoch 94/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.5948 - acc: 0.7910 - val_loss: 0.7249 - val_acc: 0.7588\n",
      "Epoch 95/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.5961 - acc: 0.7871 - val_loss: 0.7142 - val_acc: 0.7588\n",
      "Epoch 96/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.5941 - acc: 0.7882 - val_loss: 0.7246 - val_acc: 0.7568\n",
      "Epoch 97/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.5903 - acc: 0.7896 - val_loss: 0.7244 - val_acc: 0.7576\n",
      "Epoch 98/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.5892 - acc: 0.7894 - val_loss: 0.7401 - val_acc: 0.7505\n",
      "Epoch 99/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.5919 - acc: 0.7907 - val_loss: 0.7262 - val_acc: 0.7580\n",
      "Epoch 100/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.5853 - acc: 0.7910 - val_loss: 0.7852 - val_acc: 0.7402\n",
      "Inner step: 16 / 40\n",
      "Train on 37686 samples, validate on 13278 samples\n",
      "Epoch 1/100\n",
      "37686/37686 [==============================] - 3s 74us/step - loss: 1.4131 - acc: 0.4009 - val_loss: 1.3724 - val_acc: 0.4214\n",
      "Epoch 2/100\n",
      "37686/37686 [==============================] - 2s 60us/step - loss: 1.3715 - acc: 0.4089 - val_loss: 1.3577 - val_acc: 0.4437\n",
      "Epoch 3/100\n",
      "37686/37686 [==============================] - 2s 63us/step - loss: 1.3052 - acc: 0.4514 - val_loss: 1.3136 - val_acc: 0.4744\n",
      "Epoch 4/100\n",
      "37686/37686 [==============================] - 2s 60us/step - loss: 1.2836 - acc: 0.4644 - val_loss: 1.3076 - val_acc: 0.4699\n",
      "Epoch 5/100\n",
      "37686/37686 [==============================] - 2s 61us/step - loss: 1.2754 - acc: 0.4655 - val_loss: 1.2864 - val_acc: 0.4770\n",
      "Epoch 6/100\n",
      "37686/37686 [==============================] - 2s 60us/step - loss: 1.2678 - acc: 0.4674 - val_loss: 1.2912 - val_acc: 0.4756\n",
      "Epoch 7/100\n",
      "37686/37686 [==============================] - 2s 63us/step - loss: 1.1608 - acc: 0.5211 - val_loss: 1.1157 - val_acc: 0.5488\n",
      "Epoch 8/100\n",
      "37686/37686 [==============================] - 2s 63us/step - loss: 1.0737 - acc: 0.5499 - val_loss: 1.0764 - val_acc: 0.5562\n",
      "Epoch 9/100\n",
      "37686/37686 [==============================] - 2s 60us/step - loss: 1.0638 - acc: 0.5515 - val_loss: 1.0818 - val_acc: 0.5536\n",
      "Epoch 10/100\n",
      "37686/37686 [==============================] - 2s 60us/step - loss: 1.0565 - acc: 0.5548 - val_loss: 1.0538 - val_acc: 0.5673\n",
      "Epoch 11/100\n",
      "37686/37686 [==============================] - 2s 66us/step - loss: 1.0281 - acc: 0.5727 - val_loss: 1.0257 - val_acc: 0.5811\n",
      "Epoch 12/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.9993 - acc: 0.5854 - val_loss: 1.0348 - val_acc: 0.5868\n",
      "Epoch 13/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.9812 - acc: 0.5912 - val_loss: 1.0322 - val_acc: 0.5752\n",
      "Epoch 14/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.9754 - acc: 0.5916 - val_loss: 1.0106 - val_acc: 0.5859\n",
      "Epoch 15/100\n",
      "37686/37686 [==============================] - 2s 61us/step - loss: 0.9738 - acc: 0.5930 - val_loss: 1.0284 - val_acc: 0.5752\n",
      "Epoch 16/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.9662 - acc: 0.5955 - val_loss: 1.0029 - val_acc: 0.5934\n",
      "Epoch 17/100\n",
      "37686/37686 [==============================] - 2s 60us/step - loss: 0.9633 - acc: 0.5973 - val_loss: 1.0066 - val_acc: 0.5865\n",
      "Epoch 18/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.9615 - acc: 0.5993 - val_loss: 0.9932 - val_acc: 0.5907\n",
      "Epoch 19/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37686/37686 [==============================] - 2s 60us/step - loss: 0.9558 - acc: 0.5994 - val_loss: 1.0269 - val_acc: 0.5917\n",
      "Epoch 20/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.9545 - acc: 0.6000 - val_loss: 1.0228 - val_acc: 0.5766\n",
      "Epoch 21/100\n",
      "37686/37686 [==============================] - 2s 60us/step - loss: 0.9561 - acc: 0.5992 - val_loss: 1.0113 - val_acc: 0.6034\n",
      "Epoch 22/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.9517 - acc: 0.6008 - val_loss: 1.0672 - val_acc: 0.5639\n",
      "Epoch 23/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.9517 - acc: 0.6010 - val_loss: 1.0016 - val_acc: 0.5944\n",
      "Epoch 24/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.9496 - acc: 0.6011 - val_loss: 1.0200 - val_acc: 0.5837\n",
      "Epoch 25/100\n",
      "37686/37686 [==============================] - 2s 64us/step - loss: 0.9515 - acc: 0.6014 - val_loss: 0.9983 - val_acc: 0.5919\n",
      "Epoch 26/100\n",
      "37686/37686 [==============================] - 2s 65us/step - loss: 0.9492 - acc: 0.6016 - val_loss: 1.0150 - val_acc: 0.5834\n",
      "Epoch 27/100\n",
      "37686/37686 [==============================] - 3s 67us/step - loss: 0.9481 - acc: 0.6013 - val_loss: 1.0039 - val_acc: 0.5983\n",
      "Epoch 28/100\n",
      "37686/37686 [==============================] - 2s 61us/step - loss: 0.9436 - acc: 0.6042 - val_loss: 1.0115 - val_acc: 0.5977\n",
      "Epoch 29/100\n",
      "37686/37686 [==============================] - 2s 65us/step - loss: 0.9460 - acc: 0.6035 - val_loss: 0.9965 - val_acc: 0.5894\n",
      "Epoch 30/100\n",
      "37686/37686 [==============================] - 3s 70us/step - loss: 0.9112 - acc: 0.6276 - val_loss: 0.9192 - val_acc: 0.6444\n",
      "Epoch 31/100\n",
      "37686/37686 [==============================] - 2s 60us/step - loss: 0.8543 - acc: 0.6591 - val_loss: 0.9171 - val_acc: 0.6465\n",
      "Epoch 32/100\n",
      "37686/37686 [==============================] - 2s 64us/step - loss: 0.8451 - acc: 0.6601 - val_loss: 0.9221 - val_acc: 0.6341\n",
      "Epoch 33/100\n",
      "37686/37686 [==============================] - 2s 61us/step - loss: 0.8349 - acc: 0.6651 - val_loss: 0.9221 - val_acc: 0.6455\n",
      "Epoch 34/100\n",
      "37686/37686 [==============================] - 2s 61us/step - loss: 0.8335 - acc: 0.6648 - val_loss: 0.8999 - val_acc: 0.6481\n",
      "Epoch 35/100\n",
      "37686/37686 [==============================] - 2s 61us/step - loss: 0.8306 - acc: 0.6645 - val_loss: 0.9013 - val_acc: 0.6456\n",
      "Epoch 36/100\n",
      "37686/37686 [==============================] - 2s 66us/step - loss: 0.8312 - acc: 0.6650 - val_loss: 0.9179 - val_acc: 0.6450\n",
      "Epoch 37/100\n",
      "37686/37686 [==============================] - 3s 67us/step - loss: 0.8237 - acc: 0.6688 - val_loss: 0.9150 - val_acc: 0.6497\n",
      "Epoch 38/100\n",
      "37686/37686 [==============================] - 2s 61us/step - loss: 0.8226 - acc: 0.6648 - val_loss: 0.9180 - val_acc: 0.6453\n",
      "Epoch 39/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.8231 - acc: 0.6669 - val_loss: 0.9027 - val_acc: 0.6427\n",
      "Epoch 40/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.8197 - acc: 0.6684 - val_loss: 0.9139 - val_acc: 0.6512\n",
      "Epoch 41/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.8189 - acc: 0.6684 - val_loss: 0.9241 - val_acc: 0.6432\n",
      "Epoch 42/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.8177 - acc: 0.6688 - val_loss: 0.8920 - val_acc: 0.6478\n",
      "Epoch 43/100\n",
      "37686/37686 [==============================] - 3s 67us/step - loss: 0.8188 - acc: 0.6677 - val_loss: 0.8960 - val_acc: 0.6488\n",
      "Epoch 44/100\n",
      "37686/37686 [==============================] - 2s 66us/step - loss: 0.8118 - acc: 0.6689 - val_loss: 0.9075 - val_acc: 0.6519\n",
      "Epoch 45/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.8116 - acc: 0.6703 - val_loss: 0.8918 - val_acc: 0.6524\n",
      "Epoch 46/100\n",
      "37686/37686 [==============================] - 2s 65us/step - loss: 0.8109 - acc: 0.6706 - val_loss: 0.9008 - val_acc: 0.6400\n",
      "Epoch 47/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.8135 - acc: 0.6683 - val_loss: 0.9450 - val_acc: 0.6249\n",
      "Epoch 48/100\n",
      "37686/37686 [==============================] - 2s 65us/step - loss: 0.8101 - acc: 0.6709 - val_loss: 0.9041 - val_acc: 0.6497\n",
      "Epoch 49/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.8051 - acc: 0.6715 - val_loss: 0.8836 - val_acc: 0.6556\n",
      "Epoch 50/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.8097 - acc: 0.6703 - val_loss: 0.8930 - val_acc: 0.6577\n",
      "Epoch 51/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.8069 - acc: 0.6695 - val_loss: 1.0312 - val_acc: 0.5951\n",
      "Epoch 52/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.8050 - acc: 0.6729 - val_loss: 0.9146 - val_acc: 0.6481\n",
      "Epoch 53/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.8057 - acc: 0.6732 - val_loss: 0.8910 - val_acc: 0.6535\n",
      "Epoch 54/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.8019 - acc: 0.6723 - val_loss: 0.9117 - val_acc: 0.6560\n",
      "Epoch 55/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.7996 - acc: 0.6737 - val_loss: 0.9021 - val_acc: 0.6551\n",
      "Epoch 56/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.8029 - acc: 0.6721 - val_loss: 0.8994 - val_acc: 0.6521\n",
      "Epoch 57/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.7999 - acc: 0.6735 - val_loss: 0.9037 - val_acc: 0.6528\n",
      "Epoch 58/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.7993 - acc: 0.6728 - val_loss: 0.8831 - val_acc: 0.6545\n",
      "Epoch 59/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.7998 - acc: 0.6731 - val_loss: 0.8899 - val_acc: 0.6542\n",
      "Epoch 60/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.8013 - acc: 0.6718 - val_loss: 0.9040 - val_acc: 0.6474\n",
      "Epoch 61/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.7989 - acc: 0.6710 - val_loss: 0.8877 - val_acc: 0.6578\n",
      "Epoch 62/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.7957 - acc: 0.6733 - val_loss: 0.9128 - val_acc: 0.6394\n",
      "Epoch 63/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.7912 - acc: 0.6731 - val_loss: 0.8931 - val_acc: 0.6536\n",
      "Epoch 64/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.7880 - acc: 0.6729 - val_loss: 0.8769 - val_acc: 0.6545\n",
      "Epoch 65/100\n",
      "37686/37686 [==============================] - 2s 62us/step - loss: 0.7811 - acc: 0.6740 - val_loss: 0.8832 - val_acc: 0.6457\n",
      "Epoch 66/100\n",
      "37686/37686 [==============================] - 2s 60us/step - loss: 0.7703 - acc: 0.6794 - val_loss: 0.8716 - val_acc: 0.6631\n",
      "Epoch 67/100\n",
      "37686/37686 [==============================] - 2s 60us/step - loss: 0.7533 - acc: 0.6951 - val_loss: 0.8647 - val_acc: 0.6691\n",
      "Epoch 68/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.7412 - acc: 0.7098 - val_loss: 0.8510 - val_acc: 0.6965\n",
      "Epoch 69/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.7234 - acc: 0.7194 - val_loss: 0.8569 - val_acc: 0.6838\n",
      "Epoch 70/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.7147 - acc: 0.7271 - val_loss: 0.8162 - val_acc: 0.6990\n",
      "Epoch 71/100\n",
      "37686/37686 [==============================] - 2s 60us/step - loss: 0.7046 - acc: 0.7282 - val_loss: 0.8087 - val_acc: 0.7059\n",
      "Epoch 72/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.7027 - acc: 0.7308 - val_loss: 0.7971 - val_acc: 0.7143\n",
      "Epoch 73/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.6964 - acc: 0.7341 - val_loss: 0.8179 - val_acc: 0.7090\n",
      "Epoch 74/100\n",
      "37686/37686 [==============================] - 2s 61us/step - loss: 0.6878 - acc: 0.7378 - val_loss: 0.8147 - val_acc: 0.7152\n",
      "Epoch 75/100\n",
      "37686/37686 [==============================] - 2s 60us/step - loss: 0.6859 - acc: 0.7349 - val_loss: 0.8193 - val_acc: 0.7085\n",
      "Epoch 76/100\n",
      "37686/37686 [==============================] - 2s 63us/step - loss: 0.6772 - acc: 0.7385 - val_loss: 0.8303 - val_acc: 0.6997\n",
      "Epoch 77/100\n",
      "37686/37686 [==============================] - 2s 66us/step - loss: 0.6785 - acc: 0.7406 - val_loss: 0.8051 - val_acc: 0.7155\n",
      "Epoch 78/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37686/37686 [==============================] - 2s 64us/step - loss: 0.6754 - acc: 0.7419 - val_loss: 0.8129 - val_acc: 0.7104\n",
      "Epoch 79/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.6740 - acc: 0.7415 - val_loss: 0.7894 - val_acc: 0.7232\n",
      "Epoch 80/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.6695 - acc: 0.7428 - val_loss: 0.7879 - val_acc: 0.7170\n",
      "Epoch 81/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.6638 - acc: 0.7467 - val_loss: 0.8148 - val_acc: 0.7100\n",
      "Epoch 82/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.6666 - acc: 0.7469 - val_loss: 0.7779 - val_acc: 0.7171\n",
      "Epoch 83/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.6600 - acc: 0.7488 - val_loss: 0.8074 - val_acc: 0.7190\n",
      "Epoch 84/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.6644 - acc: 0.7466 - val_loss: 0.7836 - val_acc: 0.7254\n",
      "Epoch 85/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.6575 - acc: 0.7483 - val_loss: 0.7780 - val_acc: 0.7170\n",
      "Epoch 86/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.6582 - acc: 0.7512 - val_loss: 0.7695 - val_acc: 0.7219\n",
      "Epoch 87/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.6603 - acc: 0.7489 - val_loss: 0.8029 - val_acc: 0.7295\n",
      "Epoch 88/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.6519 - acc: 0.7525 - val_loss: 0.7715 - val_acc: 0.7299\n",
      "Epoch 89/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.6518 - acc: 0.7543 - val_loss: 0.7617 - val_acc: 0.7340\n",
      "Epoch 90/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.6443 - acc: 0.7580 - val_loss: 0.7917 - val_acc: 0.7206\n",
      "Epoch 91/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.6460 - acc: 0.7592 - val_loss: 0.7617 - val_acc: 0.7383\n",
      "Epoch 92/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.6473 - acc: 0.7613 - val_loss: 0.7675 - val_acc: 0.7327\n",
      "Epoch 93/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.6401 - acc: 0.7618 - val_loss: 0.7455 - val_acc: 0.7456\n",
      "Epoch 94/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.6375 - acc: 0.7648 - val_loss: 0.7599 - val_acc: 0.7380\n",
      "Epoch 95/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.6366 - acc: 0.7674 - val_loss: 0.7664 - val_acc: 0.7445\n",
      "Epoch 96/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.6308 - acc: 0.7709 - val_loss: 0.7701 - val_acc: 0.7387\n",
      "Epoch 97/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.6289 - acc: 0.7717 - val_loss: 0.7469 - val_acc: 0.7454\n",
      "Epoch 98/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.6262 - acc: 0.7750 - val_loss: 0.7564 - val_acc: 0.7476\n",
      "Epoch 99/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.6255 - acc: 0.7764 - val_loss: 0.7796 - val_acc: 0.7333\n",
      "Epoch 100/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.6208 - acc: 0.7762 - val_loss: 0.7339 - val_acc: 0.7509\n",
      "Inner step: 17 / 40\n",
      "Train on 37686 samples, validate on 13278 samples\n",
      "Epoch 1/100\n",
      "37686/37686 [==============================] - 3s 72us/step - loss: 1.3596 - acc: 0.4361 - val_loss: 1.2959 - val_acc: 0.4739\n",
      "Epoch 2/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 1.2606 - acc: 0.4824 - val_loss: 1.2876 - val_acc: 0.4750\n",
      "Epoch 3/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 1.2482 - acc: 0.4851 - val_loss: 1.2805 - val_acc: 0.4794\n",
      "Epoch 4/100\n",
      "37686/37686 [==============================] - 2s 64us/step - loss: 1.2433 - acc: 0.4891 - val_loss: 1.3011 - val_acc: 0.4766\n",
      "Epoch 5/100\n",
      "37686/37686 [==============================] - 3s 68us/step - loss: 1.2301 - acc: 0.4935 - val_loss: 1.2609 - val_acc: 0.5011\n",
      "Epoch 6/100\n",
      "37686/37686 [==============================] - 3s 82us/step - loss: 1.2053 - acc: 0.5067 - val_loss: 1.2564 - val_acc: 0.5037\n",
      "Epoch 7/100\n",
      "37686/37686 [==============================] - 3s 82us/step - loss: 1.1927 - acc: 0.5129 - val_loss: 1.2886 - val_acc: 0.4840\n",
      "Epoch 8/100\n",
      "37686/37686 [==============================] - 2s 63us/step - loss: 1.1841 - acc: 0.5168 - val_loss: 1.2464 - val_acc: 0.5003\n",
      "Epoch 9/100\n",
      "37686/37686 [==============================] - 2s 66us/step - loss: 1.1829 - acc: 0.5175 - val_loss: 1.2500 - val_acc: 0.5075\n",
      "Epoch 10/100\n",
      "37686/37686 [==============================] - 4s 99us/step - loss: 1.1794 - acc: 0.5181 - val_loss: 1.2243 - val_acc: 0.5104\n",
      "Epoch 11/100\n",
      "37686/37686 [==============================] - 3s 68us/step - loss: 1.1714 - acc: 0.5200 - val_loss: 1.1799 - val_acc: 0.5170\n",
      "Epoch 12/100\n",
      "37686/37686 [==============================] - 3s 73us/step - loss: 1.0613 - acc: 0.5820 - val_loss: 1.0996 - val_acc: 0.5725\n",
      "Epoch 13/100\n",
      "37686/37686 [==============================] - 3s 75us/step - loss: 1.0078 - acc: 0.5966 - val_loss: 1.0631 - val_acc: 0.5694\n",
      "Epoch 14/100\n",
      "37686/37686 [==============================] - 3s 71us/step - loss: 0.9755 - acc: 0.6043 - val_loss: 1.0204 - val_acc: 0.5949\n",
      "Epoch 15/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.9565 - acc: 0.6096 - val_loss: 0.9969 - val_acc: 0.6094\n",
      "Epoch 16/100\n",
      "37686/37686 [==============================] - 2s 61us/step - loss: 0.9411 - acc: 0.6163 - val_loss: 1.0132 - val_acc: 0.6054\n",
      "Epoch 17/100\n",
      "37686/37686 [==============================] - 2s 61us/step - loss: 0.9253 - acc: 0.6251 - val_loss: 0.9921 - val_acc: 0.6105\n",
      "Epoch 18/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.9085 - acc: 0.6390 - val_loss: 0.9670 - val_acc: 0.6324\n",
      "Epoch 19/100\n",
      "37686/37686 [==============================] - 2s 65us/step - loss: 0.8777 - acc: 0.6627 - val_loss: 0.9488 - val_acc: 0.6452\n",
      "Epoch 20/100\n",
      "37686/37686 [==============================] - 2s 63us/step - loss: 0.8438 - acc: 0.6811 - val_loss: 0.9511 - val_acc: 0.6388\n",
      "Epoch 21/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.8221 - acc: 0.6886 - val_loss: 0.9052 - val_acc: 0.6672\n",
      "Epoch 22/100\n",
      "37686/37686 [==============================] - 2s 60us/step - loss: 0.7948 - acc: 0.6986 - val_loss: 0.8890 - val_acc: 0.6975\n",
      "Epoch 23/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.7535 - acc: 0.7336 - val_loss: 0.8638 - val_acc: 0.7122\n",
      "Epoch 24/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.7230 - acc: 0.7449 - val_loss: 0.8758 - val_acc: 0.6995\n",
      "Epoch 25/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.7133 - acc: 0.7505 - val_loss: 0.8452 - val_acc: 0.7075\n",
      "Epoch 26/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.7098 - acc: 0.7509 - val_loss: 0.8204 - val_acc: 0.7232\n",
      "Epoch 27/100\n",
      "37686/37686 [==============================] - 2s 61us/step - loss: 0.7057 - acc: 0.7542 - val_loss: 0.8202 - val_acc: 0.7271\n",
      "Epoch 28/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.7002 - acc: 0.7548 - val_loss: 0.8084 - val_acc: 0.7270\n",
      "Epoch 29/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.6958 - acc: 0.7560 - val_loss: 0.8329 - val_acc: 0.7198\n",
      "Epoch 30/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.6981 - acc: 0.7541 - val_loss: 0.8282 - val_acc: 0.7202\n",
      "Epoch 31/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.6926 - acc: 0.7565 - val_loss: 0.8225 - val_acc: 0.7149\n",
      "Epoch 32/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.6943 - acc: 0.7578 - val_loss: 0.8484 - val_acc: 0.7073\n",
      "Epoch 33/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.6867 - acc: 0.7591 - val_loss: 0.7941 - val_acc: 0.7326\n",
      "Epoch 34/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.6816 - acc: 0.7608 - val_loss: 0.8339 - val_acc: 0.7152\n",
      "Epoch 35/100\n",
      "37686/37686 [==============================] - 2s 61us/step - loss: 0.6801 - acc: 0.7615 - val_loss: 0.8110 - val_acc: 0.7308\n",
      "Epoch 36/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37686/37686 [==============================] - 2s 60us/step - loss: 0.6795 - acc: 0.7603 - val_loss: 0.7886 - val_acc: 0.7360\n",
      "Epoch 37/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.6745 - acc: 0.7630 - val_loss: 0.8045 - val_acc: 0.7252\n",
      "Epoch 38/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.6685 - acc: 0.7669 - val_loss: 0.8140 - val_acc: 0.7196\n",
      "Epoch 39/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.6675 - acc: 0.7663 - val_loss: 0.8212 - val_acc: 0.7205\n",
      "Epoch 40/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.6649 - acc: 0.7648 - val_loss: 0.7813 - val_acc: 0.7364\n",
      "Epoch 41/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.6644 - acc: 0.7646 - val_loss: 0.8302 - val_acc: 0.7174\n",
      "Epoch 42/100\n",
      "37686/37686 [==============================] - 2s 60us/step - loss: 0.6570 - acc: 0.7669 - val_loss: 0.8188 - val_acc: 0.7153\n",
      "Epoch 43/100\n",
      "37686/37686 [==============================] - 2s 61us/step - loss: 0.6611 - acc: 0.7657 - val_loss: 0.7860 - val_acc: 0.7265\n",
      "Epoch 44/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.6561 - acc: 0.7657 - val_loss: 0.7902 - val_acc: 0.7349\n",
      "Epoch 45/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.6539 - acc: 0.7679 - val_loss: 0.7889 - val_acc: 0.7275\n",
      "Epoch 46/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.6511 - acc: 0.7692 - val_loss: 0.7803 - val_acc: 0.7299\n",
      "Epoch 47/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.6459 - acc: 0.7699 - val_loss: 0.7957 - val_acc: 0.7328\n",
      "Epoch 48/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.6463 - acc: 0.7710 - val_loss: 0.7636 - val_acc: 0.7415\n",
      "Epoch 49/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.6413 - acc: 0.7717 - val_loss: 0.7806 - val_acc: 0.7310\n",
      "Epoch 50/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.6380 - acc: 0.7717 - val_loss: 0.7717 - val_acc: 0.7368\n",
      "Epoch 51/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.6387 - acc: 0.7712 - val_loss: 0.7697 - val_acc: 0.7360\n",
      "Epoch 52/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.6358 - acc: 0.7722 - val_loss: 0.7607 - val_acc: 0.7393\n",
      "Epoch 53/100\n",
      "37686/37686 [==============================] - 2s 61us/step - loss: 0.6371 - acc: 0.7730 - val_loss: 0.7797 - val_acc: 0.7285\n",
      "Epoch 54/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.6299 - acc: 0.7734 - val_loss: 0.7627 - val_acc: 0.7340\n",
      "Epoch 55/100\n",
      "37686/37686 [==============================] - 2s 60us/step - loss: 0.6281 - acc: 0.7762 - val_loss: 0.7604 - val_acc: 0.7407\n",
      "Epoch 56/100\n",
      "37686/37686 [==============================] - 2s 60us/step - loss: 0.6310 - acc: 0.7756 - val_loss: 0.7549 - val_acc: 0.7366\n",
      "Epoch 57/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.6243 - acc: 0.7760 - val_loss: 0.7976 - val_acc: 0.7238\n",
      "Epoch 58/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.6227 - acc: 0.7759 - val_loss: 0.7951 - val_acc: 0.7187\n",
      "Epoch 59/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.6196 - acc: 0.7749 - val_loss: 0.8174 - val_acc: 0.7248\n",
      "Epoch 60/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.6169 - acc: 0.7776 - val_loss: 0.7693 - val_acc: 0.7291\n",
      "Epoch 61/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.6209 - acc: 0.7753 - val_loss: 0.7590 - val_acc: 0.7446\n",
      "Epoch 62/100\n",
      "37686/37686 [==============================] - 2s 60us/step - loss: 0.6201 - acc: 0.7777 - val_loss: 0.7713 - val_acc: 0.7253\n",
      "Epoch 63/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.6138 - acc: 0.7787 - val_loss: 0.7648 - val_acc: 0.7421\n",
      "Epoch 64/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.6116 - acc: 0.7768 - val_loss: 0.7437 - val_acc: 0.7449\n",
      "Epoch 65/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.6160 - acc: 0.7760 - val_loss: 0.7724 - val_acc: 0.7376\n",
      "Epoch 66/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.6103 - acc: 0.7789 - val_loss: 0.7482 - val_acc: 0.7440\n",
      "Epoch 67/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.6129 - acc: 0.7802 - val_loss: 0.7568 - val_acc: 0.7463\n",
      "Epoch 68/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.6073 - acc: 0.7802 - val_loss: 0.7868 - val_acc: 0.7373\n",
      "Epoch 69/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.6024 - acc: 0.7828 - val_loss: 0.7924 - val_acc: 0.7366\n",
      "Epoch 70/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.6025 - acc: 0.7811 - val_loss: 0.7855 - val_acc: 0.7384\n",
      "Epoch 71/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.6040 - acc: 0.7799 - val_loss: 0.7547 - val_acc: 0.7381\n",
      "Epoch 72/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.5992 - acc: 0.7850 - val_loss: 0.7508 - val_acc: 0.7405\n",
      "Epoch 73/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.5960 - acc: 0.7833 - val_loss: 0.7405 - val_acc: 0.7468\n",
      "Epoch 74/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.5990 - acc: 0.7828 - val_loss: 0.7629 - val_acc: 0.7425\n",
      "Epoch 75/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.5916 - acc: 0.7863 - val_loss: 0.7523 - val_acc: 0.7387\n",
      "Epoch 76/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.5968 - acc: 0.7851 - val_loss: 0.7842 - val_acc: 0.7370\n",
      "Epoch 77/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.5903 - acc: 0.7882 - val_loss: 0.7599 - val_acc: 0.7389\n",
      "Epoch 78/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.5900 - acc: 0.7905 - val_loss: 0.7780 - val_acc: 0.7545\n",
      "Epoch 79/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.5874 - acc: 0.7951 - val_loss: 0.7189 - val_acc: 0.7662\n",
      "Epoch 80/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.5814 - acc: 0.7959 - val_loss: 0.7335 - val_acc: 0.7738\n",
      "Epoch 81/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.5776 - acc: 0.8006 - val_loss: 0.7718 - val_acc: 0.7430\n",
      "Epoch 82/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.5755 - acc: 0.8040 - val_loss: 0.7366 - val_acc: 0.7684\n",
      "Epoch 83/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.5725 - acc: 0.8107 - val_loss: 0.7256 - val_acc: 0.7749\n",
      "Epoch 84/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.5634 - acc: 0.8151 - val_loss: 0.7130 - val_acc: 0.7797\n",
      "Epoch 85/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.5617 - acc: 0.8160 - val_loss: 0.7126 - val_acc: 0.7792\n",
      "Epoch 86/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.5593 - acc: 0.8186 - val_loss: 0.7188 - val_acc: 0.7851\n",
      "Epoch 87/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.5508 - acc: 0.8221 - val_loss: 0.7089 - val_acc: 0.7858\n",
      "Epoch 88/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.5512 - acc: 0.8222 - val_loss: 0.7365 - val_acc: 0.7701\n",
      "Epoch 89/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.5435 - acc: 0.8246 - val_loss: 0.7279 - val_acc: 0.7818\n",
      "Epoch 90/100\n",
      "37686/37686 [==============================] - 2s 60us/step - loss: 0.5456 - acc: 0.8235 - val_loss: 0.7092 - val_acc: 0.7836\n",
      "Epoch 91/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.5443 - acc: 0.8257 - val_loss: 0.7177 - val_acc: 0.7783\n",
      "Epoch 92/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.5393 - acc: 0.8254 - val_loss: 0.6905 - val_acc: 0.7897\n",
      "Epoch 93/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.5365 - acc: 0.8290 - val_loss: 0.7131 - val_acc: 0.7819\n",
      "Epoch 94/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.5370 - acc: 0.8278 - val_loss: 0.6999 - val_acc: 0.7820\n",
      "Epoch 95/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37686/37686 [==============================] - 2s 60us/step - loss: 0.5382 - acc: 0.8270 - val_loss: 0.7312 - val_acc: 0.7743\n",
      "Epoch 96/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.5308 - acc: 0.8302 - val_loss: 0.6906 - val_acc: 0.7913\n",
      "Epoch 97/100\n",
      "37686/37686 [==============================] - 2s 61us/step - loss: 0.5311 - acc: 0.8301 - val_loss: 0.7307 - val_acc: 0.7766\n",
      "Epoch 98/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.5302 - acc: 0.8319 - val_loss: 0.7205 - val_acc: 0.7878\n",
      "Epoch 99/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.5299 - acc: 0.8315 - val_loss: 0.7138 - val_acc: 0.7906\n",
      "Epoch 100/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.5283 - acc: 0.8314 - val_loss: 0.7135 - val_acc: 0.7833\n",
      "Inner step: 18 / 40\n",
      "Train on 37686 samples, validate on 13278 samples\n",
      "Epoch 1/100\n",
      "37686/37686 [==============================] - 3s 72us/step - loss: 1.1711 - acc: 0.5219 - val_loss: 1.0579 - val_acc: 0.5664\n",
      "Epoch 2/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 1.0069 - acc: 0.5796 - val_loss: 1.0507 - val_acc: 0.5657\n",
      "Epoch 3/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.9876 - acc: 0.5853 - val_loss: 1.0141 - val_acc: 0.5913\n",
      "Epoch 4/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.9847 - acc: 0.5883 - val_loss: 1.0039 - val_acc: 0.5872\n",
      "Epoch 5/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.9742 - acc: 0.5927 - val_loss: 0.9940 - val_acc: 0.5914\n",
      "Epoch 6/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.9727 - acc: 0.5942 - val_loss: 1.0707 - val_acc: 0.5634\n",
      "Epoch 7/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.9714 - acc: 0.5944 - val_loss: 1.0189 - val_acc: 0.5878\n",
      "Epoch 8/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.9650 - acc: 0.5951 - val_loss: 0.9961 - val_acc: 0.5975\n",
      "Epoch 9/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.9627 - acc: 0.5942 - val_loss: 1.0189 - val_acc: 0.5813\n",
      "Epoch 10/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.9633 - acc: 0.5969 - val_loss: 1.0038 - val_acc: 0.5890\n",
      "Epoch 11/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.9608 - acc: 0.5967 - val_loss: 1.0252 - val_acc: 0.5846\n",
      "Epoch 12/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.9610 - acc: 0.5962 - val_loss: 1.0089 - val_acc: 0.5903\n",
      "Epoch 13/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.9565 - acc: 0.5994 - val_loss: 1.0180 - val_acc: 0.5812\n",
      "Epoch 14/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.9565 - acc: 0.5987 - val_loss: 1.0143 - val_acc: 0.5850\n",
      "Epoch 15/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.9525 - acc: 0.6001 - val_loss: 1.0323 - val_acc: 0.5780\n",
      "Epoch 16/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.9549 - acc: 0.5992 - val_loss: 1.0560 - val_acc: 0.5659\n",
      "Epoch 17/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.9542 - acc: 0.6016 - val_loss: 1.0061 - val_acc: 0.5861\n",
      "Epoch 18/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.9535 - acc: 0.6010 - val_loss: 1.0647 - val_acc: 0.5609\n",
      "Epoch 19/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.9524 - acc: 0.6014 - val_loss: 1.0156 - val_acc: 0.5868\n",
      "Epoch 20/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.9497 - acc: 0.6035 - val_loss: 1.0266 - val_acc: 0.5836\n",
      "Epoch 21/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.9507 - acc: 0.6006 - val_loss: 0.9968 - val_acc: 0.5993\n",
      "Epoch 22/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.9500 - acc: 0.6013 - val_loss: 1.0084 - val_acc: 0.5886\n",
      "Epoch 23/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.9503 - acc: 0.6021 - val_loss: 1.0190 - val_acc: 0.5957\n",
      "Epoch 24/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.9469 - acc: 0.6031 - val_loss: 1.0081 - val_acc: 0.6005\n",
      "Epoch 25/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.9324 - acc: 0.6153 - val_loss: 0.9617 - val_acc: 0.6376\n",
      "Epoch 26/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.8707 - acc: 0.6517 - val_loss: 0.9387 - val_acc: 0.6431\n",
      "Epoch 27/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.8480 - acc: 0.6594 - val_loss: 0.9234 - val_acc: 0.6426\n",
      "Epoch 28/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.8367 - acc: 0.6624 - val_loss: 0.9053 - val_acc: 0.6527\n",
      "Epoch 29/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.8354 - acc: 0.6611 - val_loss: 0.9075 - val_acc: 0.6441\n",
      "Epoch 30/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.8332 - acc: 0.6604 - val_loss: 0.9299 - val_acc: 0.6377\n",
      "Epoch 31/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.8245 - acc: 0.6650 - val_loss: 0.9030 - val_acc: 0.6526\n",
      "Epoch 32/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.8211 - acc: 0.6678 - val_loss: 0.8972 - val_acc: 0.6468\n",
      "Epoch 33/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.8182 - acc: 0.6665 - val_loss: 0.9015 - val_acc: 0.6490\n",
      "Epoch 34/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.7988 - acc: 0.6734 - val_loss: 0.8554 - val_acc: 0.6837\n",
      "Epoch 35/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.7616 - acc: 0.7093 - val_loss: 0.8353 - val_acc: 0.6981\n",
      "Epoch 36/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.7349 - acc: 0.7177 - val_loss: 0.8632 - val_acc: 0.6949\n",
      "Epoch 37/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.7235 - acc: 0.7222 - val_loss: 0.8333 - val_acc: 0.6972\n",
      "Epoch 38/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.7189 - acc: 0.7266 - val_loss: 0.8087 - val_acc: 0.7092\n",
      "Epoch 39/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.7136 - acc: 0.7282 - val_loss: 0.8238 - val_acc: 0.7055\n",
      "Epoch 40/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.7165 - acc: 0.7266 - val_loss: 0.8245 - val_acc: 0.7078\n",
      "Epoch 41/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.7078 - acc: 0.7317 - val_loss: 0.8107 - val_acc: 0.7053\n",
      "Epoch 42/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.7046 - acc: 0.7339 - val_loss: 0.8118 - val_acc: 0.7165\n",
      "Epoch 43/100\n",
      "37686/37686 [==============================] - 2s 65us/step - loss: 0.7016 - acc: 0.7346 - val_loss: 0.8102 - val_acc: 0.7134\n",
      "Epoch 44/100\n",
      "37686/37686 [==============================] - 3s 68us/step - loss: 0.7025 - acc: 0.7360 - val_loss: 0.7967 - val_acc: 0.7147\n",
      "Epoch 45/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.7009 - acc: 0.7350 - val_loss: 0.8103 - val_acc: 0.7064\n",
      "Epoch 46/100\n",
      "37686/37686 [==============================] - 2s 62us/step - loss: 0.6950 - acc: 0.7378 - val_loss: 0.8106 - val_acc: 0.7156\n",
      "Epoch 47/100\n",
      "37686/37686 [==============================] - 2s 60us/step - loss: 0.6960 - acc: 0.7399 - val_loss: 0.8186 - val_acc: 0.7106\n",
      "Epoch 48/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.6945 - acc: 0.7433 - val_loss: 0.7922 - val_acc: 0.7221\n",
      "Epoch 49/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.6918 - acc: 0.7443 - val_loss: 0.8161 - val_acc: 0.7129\n",
      "Epoch 50/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.6900 - acc: 0.7442 - val_loss: 0.7953 - val_acc: 0.7273\n",
      "Epoch 51/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.6826 - acc: 0.7478 - val_loss: 0.7916 - val_acc: 0.7233\n",
      "Epoch 52/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.6808 - acc: 0.7523 - val_loss: 0.8042 - val_acc: 0.7211\n",
      "Epoch 53/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.6806 - acc: 0.7501 - val_loss: 0.7886 - val_acc: 0.7291\n",
      "Epoch 54/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.6780 - acc: 0.7531 - val_loss: 0.8018 - val_acc: 0.7250\n",
      "Epoch 55/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.6735 - acc: 0.7557 - val_loss: 0.8197 - val_acc: 0.7169\n",
      "Epoch 56/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.6735 - acc: 0.7557 - val_loss: 0.7650 - val_acc: 0.7353\n",
      "Epoch 57/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.6762 - acc: 0.7543 - val_loss: 0.8028 - val_acc: 0.7225\n",
      "Epoch 58/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.6649 - acc: 0.7591 - val_loss: 0.7921 - val_acc: 0.7299\n",
      "Epoch 59/100\n",
      "37686/37686 [==============================] - 2s 60us/step - loss: 0.6743 - acc: 0.7567 - val_loss: 0.7849 - val_acc: 0.7340\n",
      "Epoch 60/100\n",
      "37686/37686 [==============================] - 2s 62us/step - loss: 0.6644 - acc: 0.7615 - val_loss: 0.7992 - val_acc: 0.7333\n",
      "Epoch 61/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.6656 - acc: 0.7636 - val_loss: 0.7892 - val_acc: 0.7415\n",
      "Epoch 62/100\n",
      "37686/37686 [==============================] - 2s 60us/step - loss: 0.6609 - acc: 0.7630 - val_loss: 0.7971 - val_acc: 0.7298\n",
      "Epoch 63/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.6593 - acc: 0.7642 - val_loss: 0.7794 - val_acc: 0.7356\n",
      "Epoch 64/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.6553 - acc: 0.7674 - val_loss: 0.7864 - val_acc: 0.7323\n",
      "Epoch 65/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.6573 - acc: 0.7646 - val_loss: 0.7772 - val_acc: 0.7366\n",
      "Epoch 66/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.6527 - acc: 0.7673 - val_loss: 0.7737 - val_acc: 0.7369\n",
      "Epoch 67/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.6556 - acc: 0.7664 - val_loss: 0.7795 - val_acc: 0.7330\n",
      "Epoch 68/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.6458 - acc: 0.7689 - val_loss: 0.7706 - val_acc: 0.7364\n",
      "Epoch 69/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.6478 - acc: 0.7691 - val_loss: 0.7789 - val_acc: 0.7380\n",
      "Epoch 70/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.6413 - acc: 0.7725 - val_loss: 0.8093 - val_acc: 0.7207\n",
      "Epoch 71/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.6430 - acc: 0.7724 - val_loss: 0.7729 - val_acc: 0.7351\n",
      "Epoch 72/100\n",
      "37686/37686 [==============================] - 2s 61us/step - loss: 0.6394 - acc: 0.7729 - val_loss: 0.8144 - val_acc: 0.7309\n",
      "Epoch 73/100\n",
      "37686/37686 [==============================] - 2s 60us/step - loss: 0.6377 - acc: 0.7717 - val_loss: 0.7799 - val_acc: 0.7356\n",
      "Epoch 74/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.6343 - acc: 0.7750 - val_loss: 0.7792 - val_acc: 0.7356\n",
      "Epoch 75/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.6315 - acc: 0.7769 - val_loss: 0.8015 - val_acc: 0.7378\n",
      "Epoch 76/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.6305 - acc: 0.7755 - val_loss: 0.7759 - val_acc: 0.7373\n",
      "Epoch 77/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.6236 - acc: 0.7792 - val_loss: 0.7905 - val_acc: 0.7284\n",
      "Epoch 78/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.6283 - acc: 0.7782 - val_loss: 0.7748 - val_acc: 0.7470\n",
      "Epoch 79/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.6258 - acc: 0.7780 - val_loss: 0.7907 - val_acc: 0.7361\n",
      "Epoch 80/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.6276 - acc: 0.7789 - val_loss: 0.7836 - val_acc: 0.7338\n",
      "Epoch 81/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.6230 - acc: 0.7809 - val_loss: 0.7648 - val_acc: 0.7436\n",
      "Epoch 82/100\n",
      "37686/37686 [==============================] - 2s 60us/step - loss: 0.6176 - acc: 0.7801 - val_loss: 0.7853 - val_acc: 0.7413\n",
      "Epoch 83/100\n",
      "37686/37686 [==============================] - 2s 61us/step - loss: 0.6201 - acc: 0.7793 - val_loss: 0.7669 - val_acc: 0.7412\n",
      "Epoch 84/100\n",
      "37686/37686 [==============================] - 2s 62us/step - loss: 0.6195 - acc: 0.7810 - val_loss: 0.7627 - val_acc: 0.7407\n",
      "Epoch 85/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.6150 - acc: 0.7820 - val_loss: 0.7692 - val_acc: 0.7418\n",
      "Epoch 86/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.6175 - acc: 0.7829 - val_loss: 0.7743 - val_acc: 0.7430\n",
      "Epoch 87/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.6154 - acc: 0.7816 - val_loss: 0.7881 - val_acc: 0.7463\n",
      "Epoch 88/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.6204 - acc: 0.7804 - val_loss: 0.7763 - val_acc: 0.7387\n",
      "Epoch 89/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.6139 - acc: 0.7838 - val_loss: 0.7967 - val_acc: 0.7370\n",
      "Epoch 90/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.6145 - acc: 0.7821 - val_loss: 0.7720 - val_acc: 0.7445\n",
      "Epoch 91/100\n",
      "37686/37686 [==============================] - 2s 60us/step - loss: 0.6106 - acc: 0.7832 - val_loss: 0.7618 - val_acc: 0.7454\n",
      "Epoch 92/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.6121 - acc: 0.7842 - val_loss: 0.7688 - val_acc: 0.7479\n",
      "Epoch 93/100\n",
      "37686/37686 [==============================] - 2s 62us/step - loss: 0.6108 - acc: 0.7844 - val_loss: 0.7872 - val_acc: 0.7399\n",
      "Epoch 94/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.6081 - acc: 0.7847 - val_loss: 0.8050 - val_acc: 0.7383\n",
      "Epoch 95/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.6096 - acc: 0.7845 - val_loss: 0.7633 - val_acc: 0.7383\n",
      "Epoch 96/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.6075 - acc: 0.7829 - val_loss: 0.7707 - val_acc: 0.7426\n",
      "Epoch 97/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.6061 - acc: 0.7846 - val_loss: 0.7504 - val_acc: 0.7439\n",
      "Epoch 98/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.6053 - acc: 0.7838 - val_loss: 0.7568 - val_acc: 0.7470\n",
      "Epoch 99/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.6053 - acc: 0.7851 - val_loss: 0.7712 - val_acc: 0.7414\n",
      "Epoch 100/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.6077 - acc: 0.7846 - val_loss: 0.7650 - val_acc: 0.7413\n",
      "Inner step: 19 / 40\n",
      "Train on 37686 samples, validate on 13278 samples\n",
      "Epoch 1/100\n",
      "37686/37686 [==============================] - 3s 72us/step - loss: 1.2298 - acc: 0.4869 - val_loss: 1.1034 - val_acc: 0.5407\n",
      "Epoch 2/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 1.0868 - acc: 0.5427 - val_loss: 1.0748 - val_acc: 0.5517\n",
      "Epoch 3/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 1.0694 - acc: 0.5544 - val_loss: 1.0640 - val_acc: 0.5630\n",
      "Epoch 4/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 1.0644 - acc: 0.5587 - val_loss: 1.0924 - val_acc: 0.5535\n",
      "Epoch 5/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 1.0604 - acc: 0.5628 - val_loss: 1.1143 - val_acc: 0.5667\n",
      "Epoch 6/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 1.0610 - acc: 0.5611 - val_loss: 1.0691 - val_acc: 0.5703\n",
      "Epoch 7/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 1.0564 - acc: 0.5632 - val_loss: 1.0684 - val_acc: 0.5691\n",
      "Epoch 8/100\n",
      "37686/37686 [==============================] - 2s 61us/step - loss: 1.0524 - acc: 0.5660 - val_loss: 1.0814 - val_acc: 0.5673\n",
      "Epoch 9/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 1.0496 - acc: 0.5679 - val_loss: 1.1074 - val_acc: 0.5487\n",
      "Epoch 10/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 1.0519 - acc: 0.5688 - val_loss: 1.0728 - val_acc: 0.5709\n",
      "Epoch 11/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37686/37686 [==============================] - 2s 59us/step - loss: 1.0540 - acc: 0.5696 - val_loss: 1.0868 - val_acc: 0.5699\n",
      "Epoch 12/100\n",
      "37686/37686 [==============================] - 2s 60us/step - loss: 1.0531 - acc: 0.5662 - val_loss: 1.0800 - val_acc: 0.5585\n",
      "Epoch 13/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 1.0498 - acc: 0.5695 - val_loss: 1.0665 - val_acc: 0.5712\n",
      "Epoch 14/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 1.0144 - acc: 0.5889 - val_loss: 1.0201 - val_acc: 0.6093\n",
      "Epoch 15/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.9577 - acc: 0.6202 - val_loss: 1.0265 - val_acc: 0.6007\n",
      "Epoch 16/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.9352 - acc: 0.6268 - val_loss: 0.9586 - val_acc: 0.6318\n",
      "Epoch 17/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.8731 - acc: 0.6515 - val_loss: 0.9365 - val_acc: 0.6392\n",
      "Epoch 18/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.8590 - acc: 0.6559 - val_loss: 0.9389 - val_acc: 0.6313\n",
      "Epoch 19/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.8456 - acc: 0.6598 - val_loss: 0.9353 - val_acc: 0.6326\n",
      "Epoch 20/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.8236 - acc: 0.6733 - val_loss: 0.8911 - val_acc: 0.6561\n",
      "Epoch 21/100\n",
      "37686/37686 [==============================] - 2s 60us/step - loss: 0.7670 - acc: 0.7088 - val_loss: 0.8088 - val_acc: 0.7093\n",
      "Epoch 22/100\n",
      "37686/37686 [==============================] - 2s 61us/step - loss: 0.7335 - acc: 0.7225 - val_loss: 0.8207 - val_acc: 0.7012\n",
      "Epoch 23/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.7275 - acc: 0.7256 - val_loss: 0.8149 - val_acc: 0.7011\n",
      "Epoch 24/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.7213 - acc: 0.7284 - val_loss: 0.8028 - val_acc: 0.7073\n",
      "Epoch 25/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.7124 - acc: 0.7308 - val_loss: 0.8020 - val_acc: 0.7124\n",
      "Epoch 26/100\n",
      "37686/37686 [==============================] - 2s 65us/step - loss: 0.7058 - acc: 0.7310 - val_loss: 0.8686 - val_acc: 0.6854\n",
      "Epoch 27/100\n",
      "37686/37686 [==============================] - 2s 61us/step - loss: 0.7035 - acc: 0.7322 - val_loss: 0.8145 - val_acc: 0.7081\n",
      "Epoch 28/100\n",
      "37686/37686 [==============================] - 2s 60us/step - loss: 0.6982 - acc: 0.7347 - val_loss: 0.7899 - val_acc: 0.7158\n",
      "Epoch 29/100\n",
      "37686/37686 [==============================] - 2s 62us/step - loss: 0.6953 - acc: 0.7341 - val_loss: 0.7795 - val_acc: 0.7171\n",
      "Epoch 30/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.6901 - acc: 0.7353 - val_loss: 0.8039 - val_acc: 0.7084\n",
      "Epoch 31/100\n",
      "37686/37686 [==============================] - 2s 62us/step - loss: 0.6866 - acc: 0.7372 - val_loss: 0.7990 - val_acc: 0.7139\n",
      "Epoch 32/100\n",
      "37686/37686 [==============================] - 2s 65us/step - loss: 0.6872 - acc: 0.7387 - val_loss: 0.7950 - val_acc: 0.7275\n",
      "Epoch 33/100\n",
      "37686/37686 [==============================] - 2s 62us/step - loss: 0.6791 - acc: 0.7438 - val_loss: 0.8072 - val_acc: 0.7210\n",
      "Epoch 34/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.6695 - acc: 0.7478 - val_loss: 0.7721 - val_acc: 0.7339\n",
      "Epoch 35/100\n",
      "37686/37686 [==============================] - 2s 60us/step - loss: 0.6671 - acc: 0.7512 - val_loss: 0.7631 - val_acc: 0.7346\n",
      "Epoch 36/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.6614 - acc: 0.7545 - val_loss: 0.7865 - val_acc: 0.7343\n",
      "Epoch 37/100\n",
      "37686/37686 [==============================] - 2s 62us/step - loss: 0.6580 - acc: 0.7572 - val_loss: 0.7590 - val_acc: 0.7413\n",
      "Epoch 38/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.6535 - acc: 0.7602 - val_loss: 0.7647 - val_acc: 0.7434\n",
      "Epoch 39/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.6489 - acc: 0.7655 - val_loss: 0.7718 - val_acc: 0.7286\n",
      "Epoch 40/100\n",
      "37686/37686 [==============================] - 2s 62us/step - loss: 0.6435 - acc: 0.7671 - val_loss: 0.7777 - val_acc: 0.7407\n",
      "Epoch 41/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.6466 - acc: 0.7665 - val_loss: 0.7511 - val_acc: 0.7436\n",
      "Epoch 42/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.6344 - acc: 0.7687 - val_loss: 0.7926 - val_acc: 0.7256\n",
      "Epoch 43/100\n",
      "37686/37686 [==============================] - 2s 60us/step - loss: 0.6318 - acc: 0.7719 - val_loss: 0.7586 - val_acc: 0.7423\n",
      "Epoch 44/100\n",
      "37686/37686 [==============================] - 2s 61us/step - loss: 0.6292 - acc: 0.7710 - val_loss: 0.7519 - val_acc: 0.7464\n",
      "Epoch 45/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.6269 - acc: 0.7750 - val_loss: 0.7832 - val_acc: 0.7256\n",
      "Epoch 46/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.6227 - acc: 0.7737 - val_loss: 0.7933 - val_acc: 0.7272\n",
      "Epoch 47/100\n",
      "37686/37686 [==============================] - 2s 65us/step - loss: 0.6228 - acc: 0.7758 - val_loss: 0.7455 - val_acc: 0.7460\n",
      "Epoch 48/100\n",
      "37686/37686 [==============================] - 2s 62us/step - loss: 0.6200 - acc: 0.7760 - val_loss: 0.7793 - val_acc: 0.7299\n",
      "Epoch 49/100\n",
      "37686/37686 [==============================] - 2s 63us/step - loss: 0.6195 - acc: 0.7752 - val_loss: 0.7460 - val_acc: 0.7450\n",
      "Epoch 50/100\n",
      "37686/37686 [==============================] - 2s 61us/step - loss: 0.6201 - acc: 0.7766 - val_loss: 0.7330 - val_acc: 0.7415\n",
      "Epoch 51/100\n",
      "37686/37686 [==============================] - 2s 60us/step - loss: 0.6169 - acc: 0.7772 - val_loss: 0.7453 - val_acc: 0.7387\n",
      "Epoch 52/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.6111 - acc: 0.7789 - val_loss: 0.7444 - val_acc: 0.7441\n",
      "Epoch 53/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.6128 - acc: 0.7764 - val_loss: 0.7588 - val_acc: 0.7390\n",
      "Epoch 54/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.6163 - acc: 0.7774 - val_loss: 0.7379 - val_acc: 0.7449\n",
      "Epoch 55/100\n",
      "37686/37686 [==============================] - 2s 64us/step - loss: 0.6116 - acc: 0.7781 - val_loss: 0.7711 - val_acc: 0.7403\n",
      "Epoch 56/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.6109 - acc: 0.7810 - val_loss: 0.7577 - val_acc: 0.7402\n",
      "Epoch 57/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.6113 - acc: 0.7780 - val_loss: 0.7614 - val_acc: 0.7415\n",
      "Epoch 58/100\n",
      "37686/37686 [==============================] - 2s 62us/step - loss: 0.6052 - acc: 0.7804 - val_loss: 0.7792 - val_acc: 0.7426\n",
      "Epoch 59/100\n",
      "37686/37686 [==============================] - 2s 61us/step - loss: 0.6080 - acc: 0.7782 - val_loss: 0.7495 - val_acc: 0.7402\n",
      "Epoch 60/100\n",
      "37686/37686 [==============================] - 2s 61us/step - loss: 0.6029 - acc: 0.7803 - val_loss: 0.7552 - val_acc: 0.7430\n",
      "Epoch 61/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.6031 - acc: 0.7800 - val_loss: 0.7656 - val_acc: 0.7376\n",
      "Epoch 62/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.6066 - acc: 0.7805 - val_loss: 0.7639 - val_acc: 0.7442\n",
      "Epoch 63/100\n",
      "37686/37686 [==============================] - 2s 62us/step - loss: 0.5970 - acc: 0.7825 - val_loss: 0.7574 - val_acc: 0.7395\n",
      "Epoch 64/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.5990 - acc: 0.7822 - val_loss: 0.7927 - val_acc: 0.7219\n",
      "Epoch 65/100\n",
      "37686/37686 [==============================] - 2s 62us/step - loss: 0.5984 - acc: 0.7836 - val_loss: 0.7387 - val_acc: 0.7465\n",
      "Epoch 66/100\n",
      "37686/37686 [==============================] - 3s 74us/step - loss: 0.5971 - acc: 0.7826 - val_loss: 0.7460 - val_acc: 0.7443\n",
      "Epoch 67/100\n",
      "37686/37686 [==============================] - 2s 61us/step - loss: 0.5974 - acc: 0.7818 - val_loss: 0.7558 - val_acc: 0.7473\n",
      "Epoch 68/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.5942 - acc: 0.7833 - val_loss: 0.7379 - val_acc: 0.7488\n",
      "Epoch 69/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.5969 - acc: 0.7823 - val_loss: 0.7370 - val_acc: 0.7455\n",
      "Epoch 70/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.5904 - acc: 0.7843 - val_loss: 0.7632 - val_acc: 0.7409\n",
      "Epoch 71/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.5926 - acc: 0.7865 - val_loss: 0.7481 - val_acc: 0.7482\n",
      "Epoch 72/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.5944 - acc: 0.7840 - val_loss: 0.7511 - val_acc: 0.7457\n",
      "Epoch 73/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.5934 - acc: 0.7843 - val_loss: 0.7292 - val_acc: 0.7521\n",
      "Epoch 74/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.5903 - acc: 0.7850 - val_loss: 0.7556 - val_acc: 0.7491\n",
      "Epoch 75/100\n",
      "37686/37686 [==============================] - 2s 53us/step - loss: 0.5917 - acc: 0.7839 - val_loss: 0.7464 - val_acc: 0.7460\n",
      "Epoch 76/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.5899 - acc: 0.7865 - val_loss: 0.7551 - val_acc: 0.7495\n",
      "Epoch 77/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.5872 - acc: 0.7867 - val_loss: 0.7485 - val_acc: 0.7495\n",
      "Epoch 78/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.5843 - acc: 0.7871 - val_loss: 0.7244 - val_acc: 0.7535\n",
      "Epoch 79/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.5845 - acc: 0.7876 - val_loss: 0.7601 - val_acc: 0.7381\n",
      "Epoch 80/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.5898 - acc: 0.7867 - val_loss: 0.7409 - val_acc: 0.7434\n",
      "Epoch 81/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.5873 - acc: 0.7858 - val_loss: 0.7393 - val_acc: 0.7395\n",
      "Epoch 82/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.5872 - acc: 0.7857 - val_loss: 0.7382 - val_acc: 0.7428\n",
      "Epoch 83/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.5827 - acc: 0.7877 - val_loss: 0.7755 - val_acc: 0.7357\n",
      "Epoch 84/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.5823 - acc: 0.7879 - val_loss: 0.7406 - val_acc: 0.7478\n",
      "Epoch 85/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.5817 - acc: 0.7864 - val_loss: 0.7190 - val_acc: 0.7544\n",
      "Epoch 86/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.5782 - acc: 0.7891 - val_loss: 0.7404 - val_acc: 0.7504\n",
      "Epoch 87/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.5757 - acc: 0.7895 - val_loss: 0.7380 - val_acc: 0.7459\n",
      "Epoch 88/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.5769 - acc: 0.7897 - val_loss: 0.7497 - val_acc: 0.7455\n",
      "Epoch 89/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.5800 - acc: 0.7891 - val_loss: 0.7420 - val_acc: 0.7503\n",
      "Epoch 90/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.5751 - acc: 0.7908 - val_loss: 0.7327 - val_acc: 0.7467\n",
      "Epoch 91/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.5798 - acc: 0.7891 - val_loss: 0.7233 - val_acc: 0.7505\n",
      "Epoch 92/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.5721 - acc: 0.7914 - val_loss: 0.7426 - val_acc: 0.7503\n",
      "Epoch 93/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.5754 - acc: 0.7893 - val_loss: 0.7380 - val_acc: 0.7524\n",
      "Epoch 94/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.5725 - acc: 0.7905 - val_loss: 0.7536 - val_acc: 0.7488\n",
      "Epoch 95/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.5736 - acc: 0.7883 - val_loss: 0.7199 - val_acc: 0.7500\n",
      "Epoch 96/100\n",
      "37686/37686 [==============================] - 3s 66us/step - loss: 0.5692 - acc: 0.7920 - val_loss: 0.7410 - val_acc: 0.7460\n",
      "Epoch 97/100\n",
      "37686/37686 [==============================] - 2s 62us/step - loss: 0.5722 - acc: 0.7904 - val_loss: 0.7294 - val_acc: 0.7488\n",
      "Epoch 98/100\n",
      "37686/37686 [==============================] - 3s 77us/step - loss: 0.5662 - acc: 0.7936 - val_loss: 0.7278 - val_acc: 0.7624\n",
      "Epoch 99/100\n",
      "37686/37686 [==============================] - 3s 79us/step - loss: 0.5670 - acc: 0.7931 - val_loss: 0.7230 - val_acc: 0.7552\n",
      "Epoch 100/100\n",
      "37686/37686 [==============================] - 3s 72us/step - loss: 0.5647 - acc: 0.7953 - val_loss: 0.7277 - val_acc: 0.7577\n",
      "Inner step: 20 / 40\n",
      "Train on 37686 samples, validate on 13278 samples\n",
      "Epoch 1/100\n",
      "37686/37686 [==============================] - 4s 94us/step - loss: 1.3583 - acc: 0.4325 - val_loss: 1.3293 - val_acc: 0.4587\n",
      "Epoch 2/100\n",
      "37686/37686 [==============================] - 3s 71us/step - loss: 1.2816 - acc: 0.4640 - val_loss: 1.3293 - val_acc: 0.4623\n",
      "Epoch 3/100\n",
      "37686/37686 [==============================] - 3s 73us/step - loss: 1.2742 - acc: 0.4680 - val_loss: 1.3088 - val_acc: 0.4706\n",
      "Epoch 4/100\n",
      "37686/37686 [==============================] - 3s 74us/step - loss: 1.2681 - acc: 0.4704 - val_loss: 1.2815 - val_acc: 0.4767\n",
      "Epoch 5/100\n",
      "37686/37686 [==============================] - 3s 71us/step - loss: 1.2658 - acc: 0.4713 - val_loss: 1.3013 - val_acc: 0.4767\n",
      "Epoch 6/100\n",
      "37686/37686 [==============================] - 3s 69us/step - loss: 1.2644 - acc: 0.4729 - val_loss: 1.2906 - val_acc: 0.4803\n",
      "Epoch 7/100\n",
      "37686/37686 [==============================] - 3s 67us/step - loss: 1.2625 - acc: 0.4734 - val_loss: 1.2816 - val_acc: 0.4825\n",
      "Epoch 8/100\n",
      "37686/37686 [==============================] - 3s 72us/step - loss: 1.2618 - acc: 0.4736 - val_loss: 1.3092 - val_acc: 0.4764\n",
      "Epoch 9/100\n",
      "37686/37686 [==============================] - 3s 69us/step - loss: 1.2372 - acc: 0.4927 - val_loss: 1.2278 - val_acc: 0.5079\n",
      "Epoch 10/100\n",
      "37686/37686 [==============================] - 3s 70us/step - loss: 1.1935 - acc: 0.5137 - val_loss: 1.2234 - val_acc: 0.5096\n",
      "Epoch 11/100\n",
      "37686/37686 [==============================] - 3s 85us/step - loss: 1.1854 - acc: 0.5152 - val_loss: 1.2339 - val_acc: 0.5084\n",
      "Epoch 12/100\n",
      "37686/37686 [==============================] - 3s 79us/step - loss: 1.1813 - acc: 0.5169 - val_loss: 1.2485 - val_acc: 0.5076\n",
      "Epoch 13/100\n",
      "37686/37686 [==============================] - 3s 76us/step - loss: 1.1770 - acc: 0.5183 - val_loss: 1.2233 - val_acc: 0.5127\n",
      "Epoch 14/100\n",
      "37686/37686 [==============================] - 3s 75us/step - loss: 1.1703 - acc: 0.5220 - val_loss: 1.2437 - val_acc: 0.5193\n",
      "Epoch 15/100\n",
      "37686/37686 [==============================] - 3s 78us/step - loss: 1.1656 - acc: 0.5270 - val_loss: 1.2214 - val_acc: 0.5264\n",
      "Epoch 16/100\n",
      "37686/37686 [==============================] - 4s 118us/step - loss: 1.1582 - acc: 0.5343 - val_loss: 1.2294 - val_acc: 0.5266\n",
      "Epoch 17/100\n",
      "37686/37686 [==============================] - 4s 94us/step - loss: 1.1521 - acc: 0.5415 - val_loss: 1.2090 - val_acc: 0.5380\n",
      "Epoch 18/100\n",
      "37686/37686 [==============================] - 5s 120us/step - loss: 1.1436 - acc: 0.5456 - val_loss: 1.2313 - val_acc: 0.5316\n",
      "Epoch 19/100\n",
      "37686/37686 [==============================] - 3s 73us/step - loss: 1.1397 - acc: 0.5488 - val_loss: 1.2072 - val_acc: 0.5383\n",
      "Epoch 20/100\n",
      "37686/37686 [==============================] - 3s 74us/step - loss: 1.1377 - acc: 0.5504 - val_loss: 1.2079 - val_acc: 0.5401\n",
      "Epoch 21/100\n",
      "37686/37686 [==============================] - 5s 128us/step - loss: 1.1310 - acc: 0.5525 - val_loss: 1.2104 - val_acc: 0.5376\n",
      "Epoch 22/100\n",
      "37686/37686 [==============================] - 4s 103us/step - loss: 1.1286 - acc: 0.5539 - val_loss: 1.2152 - val_acc: 0.5352\n",
      "Epoch 23/100\n",
      "37686/37686 [==============================] - 3s 68us/step - loss: 1.1213 - acc: 0.5590 - val_loss: 1.1646 - val_acc: 0.5728\n",
      "Epoch 24/100\n",
      "37686/37686 [==============================] - 3s 78us/step - loss: 1.0392 - acc: 0.5909 - val_loss: 1.0718 - val_acc: 0.5910\n",
      "Epoch 25/100\n",
      "37686/37686 [==============================] - 3s 72us/step - loss: 0.9989 - acc: 0.6069 - val_loss: 1.0825 - val_acc: 0.5916\n",
      "Epoch 26/100\n",
      "37686/37686 [==============================] - 3s 72us/step - loss: 0.9707 - acc: 0.6192 - val_loss: 1.0404 - val_acc: 0.5983\n",
      "Epoch 27/100\n",
      "37686/37686 [==============================] - 3s 74us/step - loss: 0.9459 - acc: 0.6216 - val_loss: 1.0144 - val_acc: 0.6121\n",
      "Epoch 28/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37686/37686 [==============================] - 3s 71us/step - loss: 0.9279 - acc: 0.6231 - val_loss: 0.9934 - val_acc: 0.6078\n",
      "Epoch 29/100\n",
      "37686/37686 [==============================] - 3s 72us/step - loss: 0.9226 - acc: 0.6274 - val_loss: 0.9980 - val_acc: 0.6074\n",
      "Epoch 30/100\n",
      "37686/37686 [==============================] - 3s 81us/step - loss: 0.9125 - acc: 0.6314 - val_loss: 0.9896 - val_acc: 0.6155\n",
      "Epoch 31/100\n",
      "37686/37686 [==============================] - 3s 71us/step - loss: 0.8812 - acc: 0.6539 - val_loss: 0.9360 - val_acc: 0.6496\n",
      "Epoch 32/100\n",
      "37686/37686 [==============================] - 3s 70us/step - loss: 0.8122 - acc: 0.6944 - val_loss: 0.8984 - val_acc: 0.6806\n",
      "Epoch 33/100\n",
      "37686/37686 [==============================] - 3s 73us/step - loss: 0.8043 - acc: 0.6983 - val_loss: 0.8926 - val_acc: 0.6713\n",
      "Epoch 34/100\n",
      "37686/37686 [==============================] - 3s 73us/step - loss: 0.7922 - acc: 0.7008 - val_loss: 0.8856 - val_acc: 0.6731\n",
      "Epoch 35/100\n",
      "37686/37686 [==============================] - 3s 78us/step - loss: 0.7894 - acc: 0.7024 - val_loss: 0.9105 - val_acc: 0.6689\n",
      "Epoch 36/100\n",
      "37686/37686 [==============================] - 3s 76us/step - loss: 0.7805 - acc: 0.7067 - val_loss: 0.8786 - val_acc: 0.6801\n",
      "Epoch 37/100\n",
      "37686/37686 [==============================] - 3s 74us/step - loss: 0.7781 - acc: 0.7060 - val_loss: 0.8873 - val_acc: 0.6777\n",
      "Epoch 38/100\n",
      "37686/37686 [==============================] - 3s 70us/step - loss: 0.7746 - acc: 0.7051 - val_loss: 0.8911 - val_acc: 0.6765\n",
      "Epoch 39/100\n",
      "37686/37686 [==============================] - 3s 72us/step - loss: 0.7725 - acc: 0.7072 - val_loss: 0.8626 - val_acc: 0.6864\n",
      "Epoch 40/100\n",
      "37686/37686 [==============================] - 3s 72us/step - loss: 0.7717 - acc: 0.7080 - val_loss: 0.8663 - val_acc: 0.6881\n",
      "Epoch 41/100\n",
      "37686/37686 [==============================] - 3s 69us/step - loss: 0.7623 - acc: 0.7096 - val_loss: 0.8945 - val_acc: 0.6823\n",
      "Epoch 42/100\n",
      "37686/37686 [==============================] - 3s 68us/step - loss: 0.7627 - acc: 0.7096 - val_loss: 0.9039 - val_acc: 0.6785\n",
      "Epoch 43/100\n",
      "37686/37686 [==============================] - 3s 70us/step - loss: 0.7542 - acc: 0.7114 - val_loss: 0.8868 - val_acc: 0.6800\n",
      "Epoch 44/100\n",
      "37686/37686 [==============================] - 3s 68us/step - loss: 0.7547 - acc: 0.7125 - val_loss: 0.8586 - val_acc: 0.6855\n",
      "Epoch 45/100\n",
      "37686/37686 [==============================] - 3s 66us/step - loss: 0.7501 - acc: 0.7107 - val_loss: 0.8591 - val_acc: 0.6878\n",
      "Epoch 46/100\n",
      "37686/37686 [==============================] - 3s 71us/step - loss: 0.7427 - acc: 0.7125 - val_loss: 0.8749 - val_acc: 0.6856\n",
      "Epoch 47/100\n",
      "37686/37686 [==============================] - 3s 70us/step - loss: 0.7394 - acc: 0.7129 - val_loss: 0.8728 - val_acc: 0.6823\n",
      "Epoch 48/100\n",
      "37686/37686 [==============================] - 3s 76us/step - loss: 0.7334 - acc: 0.7141 - val_loss: 0.8396 - val_acc: 0.6865\n",
      "Epoch 49/100\n",
      "37686/37686 [==============================] - 3s 68us/step - loss: 0.7171 - acc: 0.7280 - val_loss: 0.8394 - val_acc: 0.6933\n",
      "Epoch 50/100\n",
      "37686/37686 [==============================] - 3s 81us/step - loss: 0.7033 - acc: 0.7449 - val_loss: 0.8028 - val_acc: 0.7229\n",
      "Epoch 51/100\n",
      "37686/37686 [==============================] - 3s 71us/step - loss: 0.6797 - acc: 0.7588 - val_loss: 0.8350 - val_acc: 0.7164\n",
      "Epoch 52/100\n",
      "37686/37686 [==============================] - 3s 70us/step - loss: 0.6799 - acc: 0.7621 - val_loss: 0.8107 - val_acc: 0.7260\n",
      "Epoch 53/100\n",
      "37686/37686 [==============================] - 3s 68us/step - loss: 0.6658 - acc: 0.7674 - val_loss: 0.8015 - val_acc: 0.7333\n",
      "Epoch 54/100\n",
      "37686/37686 [==============================] - 3s 73us/step - loss: 0.6656 - acc: 0.7696 - val_loss: 0.7780 - val_acc: 0.7341\n",
      "Epoch 55/100\n",
      "37686/37686 [==============================] - 3s 72us/step - loss: 0.6569 - acc: 0.7707 - val_loss: 0.7995 - val_acc: 0.7339\n",
      "Epoch 56/100\n",
      "37686/37686 [==============================] - 3s 71us/step - loss: 0.6561 - acc: 0.7709 - val_loss: 0.7742 - val_acc: 0.7415\n",
      "Epoch 57/100\n",
      "37686/37686 [==============================] - 3s 73us/step - loss: 0.6545 - acc: 0.7731 - val_loss: 0.8105 - val_acc: 0.7257\n",
      "Epoch 58/100\n",
      "37686/37686 [==============================] - 3s 69us/step - loss: 0.6451 - acc: 0.7752 - val_loss: 0.7670 - val_acc: 0.7383\n",
      "Epoch 59/100\n",
      "37686/37686 [==============================] - 3s 69us/step - loss: 0.6458 - acc: 0.7754 - val_loss: 0.7728 - val_acc: 0.7396\n",
      "Epoch 60/100\n",
      "37686/37686 [==============================] - 3s 74us/step - loss: 0.6466 - acc: 0.7734 - val_loss: 0.7645 - val_acc: 0.7465\n",
      "Epoch 61/100\n",
      "37686/37686 [==============================] - 3s 69us/step - loss: 0.6364 - acc: 0.7770 - val_loss: 0.7877 - val_acc: 0.7449\n",
      "Epoch 62/100\n",
      "37686/37686 [==============================] - 3s 73us/step - loss: 0.6343 - acc: 0.7782 - val_loss: 0.7803 - val_acc: 0.7430\n",
      "Epoch 63/100\n",
      "37686/37686 [==============================] - 3s 67us/step - loss: 0.6359 - acc: 0.7754 - val_loss: 0.7540 - val_acc: 0.7492\n",
      "Epoch 64/100\n",
      "37686/37686 [==============================] - 3s 75us/step - loss: 0.6317 - acc: 0.7790 - val_loss: 0.7576 - val_acc: 0.7457\n",
      "Epoch 65/100\n",
      "37686/37686 [==============================] - 3s 69us/step - loss: 0.6318 - acc: 0.7789 - val_loss: 0.7876 - val_acc: 0.7522\n",
      "Epoch 66/100\n",
      "37686/37686 [==============================] - 3s 69us/step - loss: 0.6278 - acc: 0.7788 - val_loss: 0.7654 - val_acc: 0.7396\n",
      "Epoch 67/100\n",
      "37686/37686 [==============================] - 3s 71us/step - loss: 0.6260 - acc: 0.7809 - val_loss: 0.7513 - val_acc: 0.7509\n",
      "Epoch 68/100\n",
      "37686/37686 [==============================] - 3s 72us/step - loss: 0.6249 - acc: 0.7806 - val_loss: 0.7529 - val_acc: 0.7454\n",
      "Epoch 69/100\n",
      "37686/37686 [==============================] - 3s 69us/step - loss: 0.6279 - acc: 0.7794 - val_loss: 0.7559 - val_acc: 0.7490\n",
      "Epoch 70/100\n",
      "37686/37686 [==============================] - 3s 70us/step - loss: 0.6213 - acc: 0.7829 - val_loss: 0.7491 - val_acc: 0.7477\n",
      "Epoch 71/100\n",
      "37686/37686 [==============================] - 3s 67us/step - loss: 0.6203 - acc: 0.7825 - val_loss: 0.7436 - val_acc: 0.7534\n",
      "Epoch 72/100\n",
      "37686/37686 [==============================] - 3s 67us/step - loss: 0.6173 - acc: 0.7848 - val_loss: 0.7559 - val_acc: 0.7483\n",
      "Epoch 73/100\n",
      "37686/37686 [==============================] - 3s 69us/step - loss: 0.6146 - acc: 0.7856 - val_loss: 0.7701 - val_acc: 0.7464\n",
      "Epoch 74/100\n",
      "37686/37686 [==============================] - 3s 91us/step - loss: 0.6168 - acc: 0.7838 - val_loss: 0.7477 - val_acc: 0.7498\n",
      "Epoch 75/100\n",
      "37686/37686 [==============================] - 3s 75us/step - loss: 0.6187 - acc: 0.7836 - val_loss: 0.7653 - val_acc: 0.7457\n",
      "Epoch 76/100\n",
      "37686/37686 [==============================] - 3s 73us/step - loss: 0.6160 - acc: 0.7851 - val_loss: 0.7555 - val_acc: 0.7529\n",
      "Epoch 77/100\n",
      "37686/37686 [==============================] - 3s 68us/step - loss: 0.6084 - acc: 0.7875 - val_loss: 0.7310 - val_acc: 0.7562\n",
      "Epoch 78/100\n",
      "37686/37686 [==============================] - 3s 68us/step - loss: 0.6070 - acc: 0.7855 - val_loss: 0.7337 - val_acc: 0.7561\n",
      "Epoch 79/100\n",
      "37686/37686 [==============================] - 3s 73us/step - loss: 0.6100 - acc: 0.7850 - val_loss: 0.7511 - val_acc: 0.7455\n",
      "Epoch 80/100\n",
      "37686/37686 [==============================] - 3s 73us/step - loss: 0.6053 - acc: 0.7862 - val_loss: 0.7489 - val_acc: 0.7503\n",
      "Epoch 81/100\n",
      "37686/37686 [==============================] - 3s 78us/step - loss: 0.6040 - acc: 0.7880 - val_loss: 0.7468 - val_acc: 0.7531\n",
      "Epoch 82/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.6039 - acc: 0.7862 - val_loss: 0.7455 - val_acc: 0.7529\n",
      "Epoch 83/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.5985 - acc: 0.7901 - val_loss: 0.7682 - val_acc: 0.7474\n",
      "Epoch 84/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.6017 - acc: 0.7886 - val_loss: 0.7752 - val_acc: 0.7399\n",
      "Epoch 85/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.6022 - acc: 0.7885 - val_loss: 0.7900 - val_acc: 0.7427\n",
      "Epoch 86/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.6032 - acc: 0.7873 - val_loss: 0.7416 - val_acc: 0.7556\n",
      "Epoch 87/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.6010 - acc: 0.7862 - val_loss: 0.7659 - val_acc: 0.7461\n",
      "Epoch 88/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.5978 - acc: 0.7891 - val_loss: 0.7411 - val_acc: 0.7549\n",
      "Epoch 89/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.5982 - acc: 0.7901 - val_loss: 0.7728 - val_acc: 0.7408\n",
      "Epoch 90/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.5976 - acc: 0.7899 - val_loss: 0.7384 - val_acc: 0.7582\n",
      "Epoch 91/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.5955 - acc: 0.7906 - val_loss: 0.7368 - val_acc: 0.7566\n",
      "Epoch 92/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.5926 - acc: 0.7914 - val_loss: 0.7694 - val_acc: 0.7537\n",
      "Epoch 93/100\n",
      "37686/37686 [==============================] - 2s 61us/step - loss: 0.5908 - acc: 0.7903 - val_loss: 0.7348 - val_acc: 0.7531\n",
      "Epoch 94/100\n",
      "37686/37686 [==============================] - 3s 70us/step - loss: 0.5899 - acc: 0.7924 - val_loss: 0.7284 - val_acc: 0.7618\n",
      "Epoch 95/100\n",
      "37686/37686 [==============================] - 3s 74us/step - loss: 0.5919 - acc: 0.7914 - val_loss: 0.7888 - val_acc: 0.7493\n",
      "Epoch 96/100\n",
      "37686/37686 [==============================] - 2s 66us/step - loss: 0.5878 - acc: 0.7918 - val_loss: 0.7481 - val_acc: 0.7500\n",
      "Epoch 97/100\n",
      "37686/37686 [==============================] - 2s 65us/step - loss: 0.5835 - acc: 0.7928 - val_loss: 0.7365 - val_acc: 0.7535\n",
      "Epoch 98/100\n",
      "37686/37686 [==============================] - 3s 68us/step - loss: 0.5867 - acc: 0.7940 - val_loss: 0.7412 - val_acc: 0.7560\n",
      "Epoch 99/100\n",
      "37686/37686 [==============================] - 3s 71us/step - loss: 0.5908 - acc: 0.7911 - val_loss: 0.7467 - val_acc: 0.7570\n",
      "Epoch 100/100\n",
      "37686/37686 [==============================] - 3s 75us/step - loss: 0.5833 - acc: 0.7947 - val_loss: 0.7366 - val_acc: 0.7564\n",
      "Inner step: 21 / 40\n",
      "Train on 37686 samples, validate on 13278 samples\n",
      "Epoch 1/100\n",
      "37686/37686 [==============================] - 3s 92us/step - loss: 1.3584 - acc: 0.4358 - val_loss: 1.3056 - val_acc: 0.4738\n",
      "Epoch 2/100\n",
      "37686/37686 [==============================] - 3s 70us/step - loss: 1.2808 - acc: 0.4648 - val_loss: 1.2919 - val_acc: 0.4760\n",
      "Epoch 3/100\n",
      "37686/37686 [==============================] - 2s 66us/step - loss: 1.2718 - acc: 0.4687 - val_loss: 1.3215 - val_acc: 0.4662\n",
      "Epoch 4/100\n",
      "37686/37686 [==============================] - 3s 71us/step - loss: 1.2675 - acc: 0.4712 - val_loss: 1.2960 - val_acc: 0.4764\n",
      "Epoch 5/100\n",
      "37686/37686 [==============================] - 3s 67us/step - loss: 1.2666 - acc: 0.4718 - val_loss: 1.2986 - val_acc: 0.4790\n",
      "Epoch 6/100\n",
      "37686/37686 [==============================] - 3s 68us/step - loss: 1.2641 - acc: 0.4709 - val_loss: 1.2935 - val_acc: 0.4760\n",
      "Epoch 7/100\n",
      "37686/37686 [==============================] - 3s 73us/step - loss: 1.2635 - acc: 0.4721 - val_loss: 1.2894 - val_acc: 0.4806\n",
      "Epoch 8/100\n",
      "37686/37686 [==============================] - 2s 60us/step - loss: 1.2611 - acc: 0.4755 - val_loss: 1.2662 - val_acc: 0.4965\n",
      "Epoch 9/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 1.2199 - acc: 0.5036 - val_loss: 1.2271 - val_acc: 0.5126\n",
      "Epoch 10/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 1.1956 - acc: 0.5131 - val_loss: 1.2275 - val_acc: 0.5112\n",
      "Epoch 11/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 1.1849 - acc: 0.5158 - val_loss: 1.2217 - val_acc: 0.5188\n",
      "Epoch 12/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 1.1792 - acc: 0.5190 - val_loss: 1.2398 - val_acc: 0.5079\n",
      "Epoch 13/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 1.1769 - acc: 0.5246 - val_loss: 1.2199 - val_acc: 0.5264\n",
      "Epoch 14/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 1.1614 - acc: 0.5341 - val_loss: 1.2259 - val_acc: 0.5311\n",
      "Epoch 15/100\n",
      "37686/37686 [==============================] - 2s 60us/step - loss: 1.1496 - acc: 0.5434 - val_loss: 1.2005 - val_acc: 0.5409\n",
      "Epoch 16/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 1.0730 - acc: 0.5793 - val_loss: 1.0864 - val_acc: 0.5756\n",
      "Epoch 17/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.9629 - acc: 0.6187 - val_loss: 1.0022 - val_acc: 0.6101\n",
      "Epoch 18/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.9407 - acc: 0.6246 - val_loss: 1.0040 - val_acc: 0.6161\n",
      "Epoch 19/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.9070 - acc: 0.6470 - val_loss: 0.9515 - val_acc: 0.6476\n",
      "Epoch 20/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.8400 - acc: 0.6868 - val_loss: 0.8906 - val_acc: 0.6838\n",
      "Epoch 21/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.8223 - acc: 0.6909 - val_loss: 0.9158 - val_acc: 0.6640\n",
      "Epoch 22/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.8144 - acc: 0.6959 - val_loss: 0.8893 - val_acc: 0.6815\n",
      "Epoch 23/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.8059 - acc: 0.6985 - val_loss: 0.8844 - val_acc: 0.6803\n",
      "Epoch 24/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.7978 - acc: 0.7012 - val_loss: 0.8757 - val_acc: 0.6838\n",
      "Epoch 25/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.7933 - acc: 0.7041 - val_loss: 0.8965 - val_acc: 0.6742\n",
      "Epoch 26/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.7886 - acc: 0.7035 - val_loss: 0.8820 - val_acc: 0.6811\n",
      "Epoch 27/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.7792 - acc: 0.7064 - val_loss: 0.8808 - val_acc: 0.6838\n",
      "Epoch 28/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.7801 - acc: 0.7052 - val_loss: 0.8711 - val_acc: 0.6801\n",
      "Epoch 29/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.7717 - acc: 0.7096 - val_loss: 0.8953 - val_acc: 0.6757\n",
      "Epoch 30/100\n",
      "37686/37686 [==============================] - 2s 60us/step - loss: 0.7662 - acc: 0.7095 - val_loss: 0.9619 - val_acc: 0.6499\n",
      "Epoch 31/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.7613 - acc: 0.7113 - val_loss: 0.9085 - val_acc: 0.6563\n",
      "Epoch 32/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.7561 - acc: 0.7112 - val_loss: 0.8444 - val_acc: 0.6837\n",
      "Epoch 33/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.7512 - acc: 0.7121 - val_loss: 0.8699 - val_acc: 0.6803\n",
      "Epoch 34/100\n",
      "37686/37686 [==============================] - 2s 60us/step - loss: 0.7366 - acc: 0.7230 - val_loss: 0.8572 - val_acc: 0.6990\n",
      "Epoch 35/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.7042 - acc: 0.7477 - val_loss: 0.7901 - val_acc: 0.7261\n",
      "Epoch 36/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.6765 - acc: 0.7641 - val_loss: 0.7988 - val_acc: 0.7340\n",
      "Epoch 37/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.6670 - acc: 0.7671 - val_loss: 0.7683 - val_acc: 0.7394\n",
      "Epoch 38/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.6581 - acc: 0.7693 - val_loss: 0.7976 - val_acc: 0.7272\n",
      "Epoch 39/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.6542 - acc: 0.7723 - val_loss: 0.7457 - val_acc: 0.7487\n",
      "Epoch 40/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.6483 - acc: 0.7754 - val_loss: 0.7735 - val_acc: 0.7386\n",
      "Epoch 41/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.6494 - acc: 0.7742 - val_loss: 0.7963 - val_acc: 0.7344\n",
      "Epoch 42/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.6464 - acc: 0.7743 - val_loss: 0.7584 - val_acc: 0.7397\n",
      "Epoch 43/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.6422 - acc: 0.7753 - val_loss: 0.7502 - val_acc: 0.7479\n",
      "Epoch 44/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.6420 - acc: 0.7755 - val_loss: 0.7958 - val_acc: 0.7347\n",
      "Epoch 45/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.6373 - acc: 0.7781 - val_loss: 0.7498 - val_acc: 0.7510\n",
      "Epoch 46/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.6389 - acc: 0.7773 - val_loss: 0.7585 - val_acc: 0.7430\n",
      "Epoch 47/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.6368 - acc: 0.7778 - val_loss: 0.7485 - val_acc: 0.7488\n",
      "Epoch 48/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.6359 - acc: 0.7770 - val_loss: 0.8003 - val_acc: 0.7335\n",
      "Epoch 49/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.6336 - acc: 0.7799 - val_loss: 0.7652 - val_acc: 0.7400\n",
      "Epoch 50/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.6302 - acc: 0.7790 - val_loss: 0.7802 - val_acc: 0.7346\n",
      "Epoch 51/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.6337 - acc: 0.7778 - val_loss: 0.7575 - val_acc: 0.7427\n",
      "Epoch 52/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.6305 - acc: 0.7790 - val_loss: 0.7826 - val_acc: 0.7362\n",
      "Epoch 53/100\n",
      "37686/37686 [==============================] - 2s 60us/step - loss: 0.6274 - acc: 0.7809 - val_loss: 0.7876 - val_acc: 0.7390\n",
      "Epoch 54/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.6334 - acc: 0.7789 - val_loss: 0.7500 - val_acc: 0.7479\n",
      "Epoch 55/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.6273 - acc: 0.7801 - val_loss: 0.7619 - val_acc: 0.7389\n",
      "Epoch 56/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.6311 - acc: 0.7802 - val_loss: 0.7432 - val_acc: 0.7483\n",
      "Epoch 57/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.6260 - acc: 0.7810 - val_loss: 0.7514 - val_acc: 0.7448\n",
      "Epoch 58/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.6241 - acc: 0.7832 - val_loss: 0.7762 - val_acc: 0.7363\n",
      "Epoch 59/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.6259 - acc: 0.7818 - val_loss: 0.7357 - val_acc: 0.7531\n",
      "Epoch 60/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.6231 - acc: 0.7822 - val_loss: 0.7667 - val_acc: 0.7418\n",
      "Epoch 61/100\n",
      "37686/37686 [==============================] - 2s 66us/step - loss: 0.6244 - acc: 0.7825 - val_loss: 0.7805 - val_acc: 0.7335\n",
      "Epoch 62/100\n",
      "37686/37686 [==============================] - 2s 63us/step - loss: 0.6226 - acc: 0.7813 - val_loss: 0.7701 - val_acc: 0.7413\n",
      "Epoch 63/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.6183 - acc: 0.7846 - val_loss: 0.7833 - val_acc: 0.7411\n",
      "Epoch 64/100\n",
      "37686/37686 [==============================] - 2s 61us/step - loss: 0.6206 - acc: 0.7811 - val_loss: 0.7585 - val_acc: 0.7459\n",
      "Epoch 65/100\n",
      "37686/37686 [==============================] - 2s 60us/step - loss: 0.6180 - acc: 0.7856 - val_loss: 0.7419 - val_acc: 0.7515\n",
      "Epoch 66/100\n",
      "37686/37686 [==============================] - 2s 57us/step - loss: 0.6156 - acc: 0.7854 - val_loss: 0.7610 - val_acc: 0.7448\n",
      "Epoch 67/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.6171 - acc: 0.7840 - val_loss: 0.7703 - val_acc: 0.7368\n",
      "Epoch 68/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.6169 - acc: 0.7829 - val_loss: 0.7801 - val_acc: 0.7433\n",
      "Epoch 69/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.6176 - acc: 0.7850 - val_loss: 0.7504 - val_acc: 0.7468\n",
      "Epoch 70/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.6179 - acc: 0.7826 - val_loss: 0.7580 - val_acc: 0.7455\n",
      "Epoch 71/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.6141 - acc: 0.7863 - val_loss: 0.7401 - val_acc: 0.7539\n",
      "Epoch 72/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.6131 - acc: 0.7853 - val_loss: 0.7642 - val_acc: 0.7492\n",
      "Epoch 73/100\n",
      "37686/37686 [==============================] - 2s 54us/step - loss: 0.6107 - acc: 0.7868 - val_loss: 0.7792 - val_acc: 0.7353\n",
      "Epoch 74/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.6162 - acc: 0.7851 - val_loss: 0.7516 - val_acc: 0.7502\n",
      "Epoch 75/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.6140 - acc: 0.7848 - val_loss: 0.7886 - val_acc: 0.7311\n",
      "Epoch 76/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.6072 - acc: 0.7881 - val_loss: 0.7605 - val_acc: 0.7494\n",
      "Epoch 77/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.6122 - acc: 0.7875 - val_loss: 0.7691 - val_acc: 0.7476\n",
      "Epoch 78/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.6132 - acc: 0.7871 - val_loss: 0.7456 - val_acc: 0.7518\n",
      "Epoch 79/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.6071 - acc: 0.7868 - val_loss: 0.7847 - val_acc: 0.7358\n",
      "Epoch 80/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.6092 - acc: 0.7880 - val_loss: 0.7511 - val_acc: 0.7505\n",
      "Epoch 81/100\n",
      "37686/37686 [==============================] - 2s 60us/step - loss: 0.6084 - acc: 0.7868 - val_loss: 0.7455 - val_acc: 0.7508\n",
      "Epoch 82/100\n",
      "37686/37686 [==============================] - 2s 56us/step - loss: 0.6049 - acc: 0.7876 - val_loss: 0.7541 - val_acc: 0.7448\n",
      "Epoch 83/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.6042 - acc: 0.7884 - val_loss: 0.7544 - val_acc: 0.7484\n",
      "Epoch 84/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.6034 - acc: 0.7899 - val_loss: 0.7703 - val_acc: 0.7375\n",
      "Epoch 85/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.6039 - acc: 0.7897 - val_loss: 0.7701 - val_acc: 0.7452\n",
      "Epoch 86/100\n",
      "37686/37686 [==============================] - 2s 55us/step - loss: 0.6012 - acc: 0.7901 - val_loss: 0.7382 - val_acc: 0.7496\n",
      "Epoch 87/100\n",
      "37686/37686 [==============================] - 2s 58us/step - loss: 0.5980 - acc: 0.7904 - val_loss: 0.7572 - val_acc: 0.7520\n",
      "Epoch 88/100\n",
      "37686/37686 [==============================] - 2s 59us/step - loss: 0.5984 - acc: 0.7913 - val_loss: 0.7367 - val_acc: 0.7568\n",
      "Epoch 89/100\n",
      "37686/37686 [==============================] - 2s 66us/step - loss: 0.5950 - acc: 0.7959 - val_loss: 0.7279 - val_acc: 0.7683\n",
      "Epoch 90/100\n",
      "37686/37686 [==============================] - 3s 71us/step - loss: 0.5969 - acc: 0.7970 - val_loss: 0.7569 - val_acc: 0.7541\n",
      "Epoch 91/100\n",
      "37686/37686 [==============================] - 3s 69us/step - loss: 0.5907 - acc: 0.8039 - val_loss: 0.7233 - val_acc: 0.7767\n",
      "Epoch 92/100\n",
      "37686/37686 [==============================] - 2s 64us/step - loss: 0.5875 - acc: 0.8097 - val_loss: 0.7544 - val_acc: 0.7710\n",
      "Epoch 93/100\n",
      "37686/37686 [==============================] - 3s 82us/step - loss: 0.5845 - acc: 0.8121 - val_loss: 0.7241 - val_acc: 0.7775\n",
      "Epoch 94/100\n",
      "37686/37686 [==============================] - 3s 72us/step - loss: 0.5770 - acc: 0.8176 - val_loss: 0.7189 - val_acc: 0.7770\n",
      "Epoch 95/100\n",
      "37686/37686 [==============================] - 3s 68us/step - loss: 0.5812 - acc: 0.8165 - val_loss: 0.7154 - val_acc: 0.7863\n",
      "Epoch 96/100\n",
      "37686/37686 [==============================] - 2s 66us/step - loss: 0.5749 - acc: 0.8206 - val_loss: 0.7356 - val_acc: 0.7800\n",
      "Epoch 97/100\n",
      "37686/37686 [==============================] - 2s 66us/step - loss: 0.5710 - acc: 0.8235 - val_loss: 0.7174 - val_acc: 0.7842\n",
      "Epoch 98/100\n",
      "37686/37686 [==============================] - 3s 67us/step - loss: 0.5672 - acc: 0.8251 - val_loss: 0.7217 - val_acc: 0.7869\n",
      "Epoch 99/100\n",
      "37686/37686 [==============================] - 2s 66us/step - loss: 0.5624 - acc: 0.8260 - val_loss: 0.7113 - val_acc: 0.7874\n",
      "Epoch 100/100\n",
      "37686/37686 [==============================] - 3s 72us/step - loss: 0.5631 - acc: 0.8275 - val_loss: 0.7363 - val_acc: 0.7856\n",
      "Inner step: 22 / 40\n",
      "Train on 37686 samples, validate on 13278 samples\n",
      "Epoch 1/100\n",
      "37686/37686 [==============================] - 3s 84us/step - loss: 1.1826 - acc: 0.5189 - val_loss: 1.0738 - val_acc: 0.5549\n",
      "Epoch 2/100\n",
      "37686/37686 [==============================] - 2s 65us/step - loss: 1.0083 - acc: 0.5795 - val_loss: 1.0438 - val_acc: 0.5768\n",
      "Epoch 3/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37686/37686 [==============================] - 2s 66us/step - loss: 0.9941 - acc: 0.5855 - val_loss: 1.0142 - val_acc: 0.5831\n",
      "Epoch 4/100\n",
      "37686/37686 [==============================] - 3s 67us/step - loss: 0.9839 - acc: 0.5890 - val_loss: 1.0087 - val_acc: 0.5847\n",
      "Epoch 5/100\n",
      "37686/37686 [==============================] - 3s 69us/step - loss: 0.9741 - acc: 0.5923 - val_loss: 1.0339 - val_acc: 0.5866\n",
      "Epoch 6/100\n",
      "37686/37686 [==============================] - 3s 77us/step - loss: 0.9725 - acc: 0.5920 - val_loss: 1.0028 - val_acc: 0.5897\n",
      "Epoch 7/100\n",
      "37686/37686 [==============================] - 3s 70us/step - loss: 0.9625 - acc: 0.5979 - val_loss: 1.0183 - val_acc: 0.5941\n",
      "Epoch 8/100\n",
      "37686/37686 [==============================] - 3s 72us/step - loss: 0.9704 - acc: 0.5947 - val_loss: 1.0014 - val_acc: 0.5990\n",
      "Epoch 9/100\n",
      "37686/37686 [==============================] - 3s 72us/step - loss: 0.9646 - acc: 0.5973 - val_loss: 1.0151 - val_acc: 0.5882\n",
      "Epoch 10/100\n",
      "37686/37686 [==============================] - 3s 70us/step - loss: 0.9626 - acc: 0.5968 - val_loss: 1.0084 - val_acc: 0.5893\n",
      "Epoch 11/100\n",
      "37686/37686 [==============================] - 4s 101us/step - loss: 0.9632 - acc: 0.5964 - val_loss: 1.0064 - val_acc: 0.5863\n",
      "Epoch 12/100\n",
      "37686/37686 [==============================] - 3s 83us/step - loss: 0.9561 - acc: 0.6002 - val_loss: 1.0268 - val_acc: 0.5789\n",
      "Epoch 13/100\n",
      "37686/37686 [==============================] - 4s 99us/step - loss: 0.9553 - acc: 0.6005 - val_loss: 1.0394 - val_acc: 0.5711\n",
      "Epoch 14/100\n",
      "37686/37686 [==============================] - 4s 95us/step - loss: 0.9553 - acc: 0.5989 - val_loss: 1.0008 - val_acc: 0.5911\n",
      "Epoch 15/100\n",
      "37686/37686 [==============================] - 4s 112us/step - loss: 0.9532 - acc: 0.5997 - val_loss: 1.0107 - val_acc: 0.5932\n",
      "Epoch 16/100\n",
      "37686/37686 [==============================] - 4s 113us/step - loss: 0.9545 - acc: 0.6006 - val_loss: 0.9986 - val_acc: 0.5981\n",
      "Epoch 17/100\n",
      "37686/37686 [==============================] - 4s 107us/step - loss: 0.9575 - acc: 0.5973 - val_loss: 1.0086 - val_acc: 0.5892\n",
      "Epoch 18/100\n",
      "37686/37686 [==============================] - 4s 102us/step - loss: 0.9527 - acc: 0.6003 - val_loss: 1.0046 - val_acc: 0.5953\n",
      "Epoch 19/100\n",
      "37686/37686 [==============================] - 4s 109us/step - loss: 0.9528 - acc: 0.6004 - val_loss: 1.0423 - val_acc: 0.5775\n",
      "Epoch 20/100\n",
      "37686/37686 [==============================] - 3s 83us/step - loss: 0.9500 - acc: 0.6008 - val_loss: 1.0020 - val_acc: 0.5962\n",
      "Epoch 21/100\n",
      "37686/37686 [==============================] - 4s 100us/step - loss: 0.9516 - acc: 0.5999 - val_loss: 1.0470 - val_acc: 0.5688\n",
      "Epoch 22/100\n",
      "37686/37686 [==============================] - 4s 102us/step - loss: 0.9494 - acc: 0.6020 - val_loss: 1.0230 - val_acc: 0.5839\n",
      "Epoch 23/100\n",
      "37686/37686 [==============================] - 3s 90us/step - loss: 0.9516 - acc: 0.6005 - val_loss: 1.0055 - val_acc: 0.5915\n",
      "Epoch 24/100\n",
      "37686/37686 [==============================] - 4s 99us/step - loss: 0.9455 - acc: 0.6026 - val_loss: 1.0185 - val_acc: 0.5996\n",
      "Epoch 25/100\n",
      "37686/37686 [==============================] - 3s 71us/step - loss: 0.9420 - acc: 0.6030 - val_loss: 0.9961 - val_acc: 0.5955\n",
      "Epoch 26/100\n",
      "37686/37686 [==============================] - 3s 70us/step - loss: 0.9418 - acc: 0.6044 - val_loss: 0.9966 - val_acc: 0.5980\n",
      "Epoch 27/100\n",
      "37686/37686 [==============================] - 5s 131us/step - loss: 0.9177 - acc: 0.6245 - val_loss: 0.9491 - val_acc: 0.6436\n",
      "Epoch 28/100\n",
      "37686/37686 [==============================] - 4s 107us/step - loss: 0.8614 - acc: 0.6550 - val_loss: 0.9790 - val_acc: 0.6307\n",
      "Epoch 29/100\n",
      "37686/37686 [==============================] - 4s 107us/step - loss: 0.8372 - acc: 0.6607 - val_loss: 0.8938 - val_acc: 0.6534\n",
      "Epoch 30/100\n",
      "37686/37686 [==============================] - 4s 101us/step - loss: 0.8307 - acc: 0.6648 - val_loss: 0.9117 - val_acc: 0.6429\n",
      "Epoch 31/100\n",
      "37686/37686 [==============================] - 3s 89us/step - loss: 0.8272 - acc: 0.6652 - val_loss: 0.8878 - val_acc: 0.6589\n",
      "Epoch 32/100\n",
      "37686/37686 [==============================] - 3s 92us/step - loss: 0.8223 - acc: 0.6652 - val_loss: 0.9263 - val_acc: 0.6464\n",
      "Epoch 33/100\n",
      "37686/37686 [==============================] - 3s 86us/step - loss: 0.8213 - acc: 0.6667 - val_loss: 0.8942 - val_acc: 0.6481\n",
      "Epoch 34/100\n",
      "37686/37686 [==============================] - 3s 75us/step - loss: 0.8163 - acc: 0.6684 - val_loss: 0.8956 - val_acc: 0.6476\n",
      "Epoch 35/100\n",
      "37686/37686 [==============================] - 3s 91us/step - loss: 0.8124 - acc: 0.6682 - val_loss: 0.8953 - val_acc: 0.6514\n",
      "Epoch 36/100\n",
      "37686/37686 [==============================] - 4s 97us/step - loss: 0.8139 - acc: 0.6672 - val_loss: 0.8913 - val_acc: 0.6521\n",
      "Epoch 37/100\n",
      "37686/37686 [==============================] - 3s 86us/step - loss: 0.8127 - acc: 0.6671 - val_loss: 0.8994 - val_acc: 0.6423\n",
      "Epoch 38/100\n",
      "37686/37686 [==============================] - 4s 106us/step - loss: 0.8066 - acc: 0.6679 - val_loss: 0.8788 - val_acc: 0.6546\n",
      "Epoch 39/100\n",
      "37686/37686 [==============================] - 4s 99us/step - loss: 0.8048 - acc: 0.6678 - val_loss: 0.8858 - val_acc: 0.6537\n",
      "Epoch 40/100\n",
      "37686/37686 [==============================] - 3s 77us/step - loss: 0.7992 - acc: 0.6663 - val_loss: 0.8713 - val_acc: 0.6467\n",
      "Epoch 41/100\n",
      "37686/37686 [==============================] - 3s 69us/step - loss: 0.7860 - acc: 0.6770 - val_loss: 0.8505 - val_acc: 0.6774\n",
      "Epoch 42/100\n",
      "37686/37686 [==============================] - 4s 96us/step - loss: 0.7665 - acc: 0.6941 - val_loss: 0.8473 - val_acc: 0.6891\n",
      "Epoch 43/100\n",
      "37686/37686 [==============================] - 3s 78us/step - loss: 0.7401 - acc: 0.7168 - val_loss: 0.8691 - val_acc: 0.7019\n",
      "Epoch 44/100\n",
      "37686/37686 [==============================] - 4s 107us/step - loss: 0.7244 - acc: 0.7242 - val_loss: 0.8006 - val_acc: 0.7131\n",
      "Epoch 45/100\n",
      " 5472/37686 [===>..........................] - ETA: 3s - loss: 0.7311 - acc: 0.7217"
     ]
    }
   ],
   "source": [
    "model_factories = [model_1_hidden_factory, model_2_hidden_factory, model_3_hidden_factory, model_4_hidden_factory]\n",
    "\n",
    "try_per_model = 40\n",
    "\n",
    "results = []\n",
    "\n",
    "for i, factory in enumerate(model_factories):\n",
    "    print('Training model', i + 1, '/', len(model_factories))\n",
    "    \n",
    "    curr_model_res = []\n",
    "    \n",
    "    for i in range(try_per_model):\n",
    "        print('Inner step:', i+1, '/', try_per_model)\n",
    "        \n",
    "        model = factory()\n",
    "        \n",
    "        compile_model(model)\n",
    "        \n",
    "        h = fit_model(model, x_train, y_train, x_test, y_test)\n",
    "        \n",
    "        curr_model_res.append(h)\n",
    "        \n",
    "    results.append(curr_model_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot summary statistics of the best validation loss and accuracy versus the number of hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Not sure about that, val_loss and val_acc in this case are not necessarily linked (not the same epoch)\n",
    "\n",
    "best_results = [[[min(res.history['val_loss']), max(res.history['val_acc'])] for res in model_res] for model_res in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i, model_results in enumerate(best_results):\n",
    "    val_loss, val_acc = list(zip(*model_results))\n",
    "    plt.hist(val_loss, bins=25)\n",
    "    plt.title(\"Distribution of validation's losses for model with \" + str(i+1) + \" hidden layers\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i, model_results in enumerate(best_results):\n",
    "    val_loss, val_acc = list(zip(*model_results))\n",
    "    plt.hist(val_acc, bins=25)\n",
    "    plt.title(\"Distribution of validation's accuracy for model with \" + str(i+1) + \" hidden layers\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_trainings = []\n",
    "\n",
    "for model_results in results:\n",
    "    best_accs = [max(training_history.history['val_acc']) for training_history in model_results]\n",
    "    best_training = model_results[best_accs.index(max(best_accs))]\n",
    "    best_trainings.append(best_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6: Tricks (regularization, batch normalization, dropout)\n",
    "\n",
    "### Description\n",
    "\n",
    "Overfitting can also be counteracted with regularization and dropout. Batch normalization is supposed to mainly decrease convergence time.\n",
    "\n",
    "1. Try to improve the best validation scores of the model with 1 layer and 100 hidden neurons and the model with 4 hidden layers. Experiment with batch_normalization layers, dropout layers and l1- and l2-regularization on weights (kernels) and biases.\n",
    "2. After you have found good settings, plot for both models the learning curves of the naive model you fitted in the previous exercises together with the learning curves of the current version.\n",
    "3. For proper comparison, plot also the learning curves of the two current models in a third figure.\n",
    "\n",
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_normalization = [0.9, 0.5]\n",
    "weight_l1_regularizations = [10, 1.0, 0.1, 0.01, 0.001]\n",
    "weight_l2_regularizations = [10, 1.0, 0.1, 0.01, 0.001]\n",
    "\n",
    "bias_l1_regularizations = [10, 1.0, 0.1, 0.01, 0.001]\n",
    "bias_l2_regularizations = [10, 1.0, 0.1, 0.01, 0.001]\n",
    "\n",
    "dropouts = [[0.2, 0.5], [0.2, 0.9], [0.1, 0.1]]\n",
    "\n",
    "def grid_search(model_factory, compiler, model_fiter):\n",
    "    histories = []\n",
    "    \n",
    "    def inner_loop(*param_lists):\n",
    "        for w_reg, b_reg, dropout in itertools.product(*param_lists):\n",
    "            print(w_reg, b_reg, dropout)\n",
    "            model = model_factory(keras.regularizers.l1(w_reg), keras.regularizers.l1(b_reg), dropout=dropout)\n",
    "            compiler(model)\n",
    "            h = model_fiter(model)\n",
    "            histories.append(h)\n",
    "            \n",
    "    inner_loop(weight_l1_regularizations, bias_l1_regularizations, dropouts)\n",
    "    inner_loop(weight_l2_regularizations, bias_l1_regularizations, dropouts)\n",
    "    inner_loop(weight_l1_regularizations, bias_l2_regularizations, dropouts)\n",
    "    inner_loop(weight_l2_regularizations, bias_l2_regularizations, dropouts)\n",
    "    \n",
    "    return histories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_family = [model_1_hidden_factory, model_4_hidden_factory]\n",
    "\n",
    "results = []\n",
    "for model in model_family:\n",
    "    res = grid_search(model, compile_model, fit_model)\n",
    "    results.append(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7: Convolutional networks\n",
    "\n",
    "### Description\n",
    "\n",
    "Convolutional neural networks have an inductive bias that is well adapted to image classification.\n",
    "\n",
    "1. Design a convolutional neural network, play with the parameters and fit it. Hint: You may get valuable inspiration from the keras [examples](https://github.com/keras-team/keras/tree/master/examples), e.g. [mnist_cnn](https://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py).\n",
    "2. Plot the learning curves of the convolutional neural network together with the so far best performing model.\n",
    "\n",
    "## Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_tensor = np.reshape(x_train, (x_train.shape[0], 16, 16, 1))\n",
    "test_tensor = np.reshape(x_test, (x_test.shape[0], 16, 16, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cnn = Sequential([\n",
    "    Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=train_tensor.shape[1:]),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Dropout(0.25),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(y_train.shape[1], activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnn.compile(loss=keras.losses.categorical_crossentropy,\n",
    "           optimizer=keras.optimizers.Adadelta(),\n",
    "           metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnn.fit(\n",
    "    train_tensor, y_train,\n",
    "    validation_data=(test_tensor, y_test),\n",
    "    epochs=20    \n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

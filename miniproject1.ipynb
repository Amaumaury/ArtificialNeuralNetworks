{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Miniproject 1: Image Classification\n",
    "\n",
    "## Introduction\n",
    "\n",
    "### Description\n",
    "\n",
    "One of the deepest traditions in learning about deep learning is to first [tackle the exciting problem of MNIST classification](http://deeplearning.net/tutorial/logreg.html). [The MNIST database](https://en.wikipedia.org/wiki/MNIST_database) (Modified National Institute of Standards and Technology database) is a large database of handwritten digits that was [recently extended](https://arxiv.org/abs/1702.05373). We break with this tradition (just a little bit) and tackle first the related problem of classifying cropped, downsampled and grayscaled images of house numbers in the [The Street View House Numbers (SVHN) Dataset](http://ufldl.stanford.edu/housenumbers/).\n",
    "\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "- You should have a running installation of [tensorflow](https://www.tensorflow.org/install/) and [keras](https://keras.io/).\n",
    "- You should know the concepts \"multilayer perceptron\", \"stochastic gradient descent with minibatches\", \"training and validation data\", \"overfitting\" and \"early stopping\".\n",
    "\n",
    "### What you will learn\n",
    "\n",
    "- You will learn how to define feedforward neural networks in keras and fit them to data.\n",
    "- You will be guided through a prototyping procedure for the application of deep learning to a specific domain.\n",
    "- You will get in contact with concepts discussed later in the lecture, like \"regularization\", \"batch normalization\" and \"convolutional networks\".\n",
    "- You will gain some experience on the influence of network architecture, optimizer and regularization choices on the goodness of fit.\n",
    "- You will learn to be more patient :) Some fits may take your computer quite a bit of time; run them over night.\n",
    "\n",
    "### Evaluation criteria\n",
    "\n",
    "The evaluation is (mostly) based on the figures you submit and your answer sentences. \n",
    "We will only do random tests of your code and not re-run the full notebook.\n",
    "\n",
    "### Your names\n",
    "\n",
    "Before you start, please enter your full name(s) in the field below; they are used to load the data. The variable student2 may remain empty, if you work alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-09T09:08:24.514461Z",
     "start_time": "2018-03-09T09:08:24.506410Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "student1 = \"Amaury Combes\"\n",
    "student2 = \"Vincenzo Bazzucchi\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some helper functions\n",
    "\n",
    "For your convenience we provide here some functions to preprocess the data and plot the results later. Simply run the following cells with `Shift-Enter`.\n",
    "\n",
    "### Dependencies and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-09T09:09:16.113721Z",
     "start_time": "2018-03-09T09:09:16.100520Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/combes/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, Flatten\n",
    "from keras.optimizers import SGD, Adam\n",
    "\n",
    "import itertools\n",
    "\n",
    "# you may experiment with different subsets, \n",
    "# but make sure in the submission \n",
    "# it is generated with the correct random seed for all exercises.\n",
    "np.random.seed(hash(student1 + student2) % 2**32)\n",
    "subset_of_classes = np.random.choice(range(10), 5, replace = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 10, 6\n",
    "def plot_some_samples(x, y = [], yhat = [], select_from = [], \n",
    "                      ncols = 6, nrows = 4, xdim = 16, ydim = 16,\n",
    "                      label_mapping = range(10)):\n",
    "    \"\"\"plot some input vectors as grayscale images (optionally together with their assigned or predicted labels).\n",
    "    \n",
    "    x is an NxD - dimensional array, where D is the length of an input vector and N is the number of samples.\n",
    "    Out of the N samples, ncols x nrows indices are randomly selected from the list select_from (if it is empty, select_from becomes range(N)).\n",
    "    \n",
    "    Keyword arguments:\n",
    "    y             -- corresponding labels to plot in green below each image.\n",
    "    yhat          -- corresponding predicted labels to plot in red below each image.\n",
    "    select_from   -- list of indices from which to select the images.\n",
    "    ncols, nrows  -- number of columns and rows to plot.\n",
    "    xdim, ydim    -- number of pixels of the images in x- and y-direction.\n",
    "    label_mapping -- map labels to digits.\n",
    "    \n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(nrows, ncols)\n",
    "    if len(select_from) == 0:\n",
    "        select_from = range(x.shape[0])\n",
    "    indices = np.random.choice(select_from, size = min(ncols * nrows, len(select_from)), replace = False)\n",
    "    for i, ind in enumerate(indices):\n",
    "        thisax = ax[i//ncols,i%ncols]\n",
    "        thisax.matshow(x[ind].reshape(xdim, ydim), cmap='gray')\n",
    "        thisax.set_axis_off()\n",
    "        if len(y) != 0:\n",
    "            j = y[ind] if type(y[ind]) != np.ndarray else y[ind].argmax()\n",
    "            thisax.text(0, 0, (label_mapping[j]+1)%10, color='green', \n",
    "                                                       verticalalignment='top',\n",
    "                                                       transform=thisax.transAxes)\n",
    "        if len(yhat) != 0:\n",
    "            k = yhat[ind] if type(yhat[ind]) != np.ndarray else yhat[ind].argmax()\n",
    "            thisax.text(1, 0, (label_mapping[k]+1)%10, color='red',\n",
    "                                             verticalalignment='top',\n",
    "                                             horizontalalignment='right',\n",
    "                                             transform=thisax.transAxes)\n",
    "    return fig\n",
    "\n",
    "def prepare_standardplot(title, xlabel):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.suptitle(title)\n",
    "    ax1.set_ylabel('categorical cross entropy')\n",
    "    ax1.set_xlabel(xlabel)\n",
    "    ax1.set_yscale('log')\n",
    "    ax2.set_ylabel('accuracy [% correct]')\n",
    "    ax2.set_xlabel(xlabel)\n",
    "    return fig, ax1, ax2\n",
    "\n",
    "def finalize_standardplot(fig, ax1, ax2):\n",
    "    ax1handles, ax1labels = ax1.get_legend_handles_labels()\n",
    "    if len(ax1labels) > 0:\n",
    "        ax1.legend(ax1handles, ax1labels)\n",
    "    ax2handles, ax2labels = ax2.get_legend_handles_labels()\n",
    "    if len(ax2labels) > 0:\n",
    "        ax2.legend(ax2handles, ax2labels)\n",
    "    fig.tight_layout()\n",
    "    plt.subplots_adjust(top=0.9)\n",
    "\n",
    "def plot_history(history, title):\n",
    "    fig, ax1, ax2 = prepare_standardplot(title, 'epoch')\n",
    "    ax1.plot(history.history['loss'], label = \"training\")\n",
    "    ax1.plot(history.history['val_loss'], label = \"validation\")\n",
    "    ax2.plot(history.history['acc'], label = \"training\")\n",
    "    ax2.plot(history.history['val_acc'], label = \"validation\")\n",
    "    finalize_standardplot(fig, ax1, ax2)\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and preprocessing the data\n",
    "\n",
    "The data consists of RGB color images with 32x32 pixels, loaded into an array of dimension 32x32x3x(number of images). We convert them to grayscale (using [this method](https://en.wikipedia.org/wiki/SRGB#The_reverse_transformation)) and we downsample them to images of 16x16 pixels by averaging over patches of 2x2 pixels.\n",
    "\n",
    "With these preprocessing steps we obviously remove some information that could be helpful in classifying the images. But, since the processed data is much lower dimensional, the fitting procedures converge faster. This is an advantage in situations like here (or generally when prototyping), were we want to try many different things without having to wait too long for computations to finish. After having gained some experience, one may want to go back to work on the 32x32 RGB images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert RGB images x to grayscale using the formula for Y_linear in https://en.wikipedia.org/wiki/Grayscale#Colorimetric_(perceptual_luminance-preserving)_conversion_to_grayscale\n",
    "def grayscale(x):\n",
    "    x = x.astype('float32')/255\n",
    "    x = np.piecewise(x, [x <= 0.04045, x > 0.04045], \n",
    "                        [lambda x: x/12.92, lambda x: ((x + .055)/1.055)**2.4])\n",
    "    return .2126 * x[:,:,0,:] + .7152 * x[:,:,1,:]  + .07152 * x[:,:,2,:]\n",
    "\n",
    "def downsample(x):\n",
    "    return sum([x[i::2,j::2,:] for i in range(2) for j in range(2)])/4\n",
    "\n",
    "def preprocess(data):\n",
    "    gray = grayscale(data['X'])\n",
    "    downsampled = downsample(gray)\n",
    "    return (downsampled.reshape(16*16, gray.shape[2]).transpose(),\n",
    "            data['y'].flatten() - 1)\n",
    "\n",
    "\n",
    "data_train = scipy.io.loadmat('housenumbers/train_32x32.mat')\n",
    "data_test = scipy.io.loadmat('housenumbers/test_32x32.mat')\n",
    "\n",
    "x_train_all, y_train_all = preprocess(data_train)\n",
    "x_test_all, y_test_all = preprocess(data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting a subset of classes\n",
    "\n",
    "We furter reduce the size of the dataset (and thus reduce computation time) by selecting only the 5 (out of 10 digits) in subset_of_classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_classes(x, y, classes):\n",
    "    indices = []\n",
    "    labels = []\n",
    "    count = 0\n",
    "    for c in classes:\n",
    "        tmp = np.where(y == c)[0]\n",
    "        indices.extend(tmp)\n",
    "        labels.extend(np.ones(len(tmp), dtype='uint8') * count)\n",
    "        count += 1\n",
    "    return x[indices], labels\n",
    "\n",
    "x_train, y_train = extract_classes(x_train_all, y_train_all, subset_of_classes)\n",
    "x_test, y_test = extract_classes(x_test_all, y_test_all, subset_of_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us plot some examples now. The green digit at the bottom left of each image indicates the corresponding label in y_test.\n",
    "For further usage of the function plot_some_samples, please have a look at its definition in the plotting section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10181, 256)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_some_samples(x_test, y_test, label_mapping = subset_of_classes);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prepare for fitting we transform the labels to one hot coding, i.e. for 5 classes, label 2 becomes the vector [0, 0, 1, 0, 0] (python uses 0-indexing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = keras.utils.to_categorical(y_train)\n",
    "y_test = keras.utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: No hidden layer\n",
    "\n",
    "### Description\n",
    "\n",
    "Define and fit a model without a hidden layer. \n",
    "\n",
    "1. Use the softmax activation for the output layer.\n",
    "2. Use the categorical_crossentropy loss.\n",
    "3. Add the accuracy metric to the metrics.\n",
    "4. Choose stochastic gradient descent for the optimizer.\n",
    "5. Choose a minibatch size of 128.\n",
    "6. Fit for as many epochs as needed to see no further decrease in the validation loss.\n",
    "7. Plot the output of the fitting procedure (a history object) using the function plot_history defined above.\n",
    "8. Determine the indices of all test images that are misclassified by the fitted model and plot some of them using the function \n",
    "   `plot_some_samples(x_test, y_test, yhat_test, error_indices, label_mapping = subset_of_classes)`\n",
    "\n",
    "\n",
    "Hints:\n",
    "* Read the keras docs, in particular [Getting started with the Keras Sequential model](https://keras.io/getting-started/sequential-model-guide/).\n",
    "* Have a look at the keras [examples](https://github.com/keras-team/keras/tree/master/examples), e.g. [mnist_mlp](https://github.com/keras-team/keras/blob/master/examples/mnist_mlp.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use the `EarlyStopping` callback to ensure point 7: it will stop the learning process when the validation loss stops decreasing for 5 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40427 samples, validate on 14839 samples\n",
      "Epoch 1/9999999999\n",
      "40427/40427 [==============================] - 1s 16us/step - loss: 1.5214 - acc: 0.3477 - val_loss: 1.5131 - val_acc: 0.3318\n",
      "Epoch 2/9999999999\n",
      "40427/40427 [==============================] - 0s 11us/step - loss: 1.4991 - acc: 0.3669 - val_loss: 1.4983 - val_acc: 0.3522\n",
      "Epoch 3/9999999999\n",
      "40427/40427 [==============================] - 0s 10us/step - loss: 1.4906 - acc: 0.3755 - val_loss: 1.4949 - val_acc: 0.3640\n",
      "Epoch 4/9999999999\n",
      "40427/40427 [==============================] - 0s 10us/step - loss: 1.4854 - acc: 0.3807 - val_loss: 1.4933 - val_acc: 0.3607\n",
      "Epoch 5/9999999999\n",
      "40427/40427 [==============================] - 0s 11us/step - loss: 1.4811 - acc: 0.3878 - val_loss: 1.4937 - val_acc: 0.3693\n",
      "Epoch 6/9999999999\n",
      "40427/40427 [==============================] - 0s 11us/step - loss: 1.4773 - acc: 0.3898 - val_loss: 1.4953 - val_acc: 0.3732\n",
      "Epoch 7/9999999999\n",
      "40427/40427 [==============================] - 0s 10us/step - loss: 1.4754 - acc: 0.3932 - val_loss: 1.4923 - val_acc: 0.3729\n",
      "Epoch 8/9999999999\n",
      "40427/40427 [==============================] - 0s 10us/step - loss: 1.4732 - acc: 0.3950 - val_loss: 1.4933 - val_acc: 0.3764\n",
      "Epoch 9/9999999999\n",
      "40427/40427 [==============================] - 0s 10us/step - loss: 1.4710 - acc: 0.3982 - val_loss: 1.4934 - val_acc: 0.3801\n"
     ]
    }
   ],
   "source": [
    "# ALL HYPERPARAMETERS NEED TUNING\n",
    "model = Sequential([\n",
    "    Dense(y_train.shape[1], input_shape=(x_train.shape[1],), activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    loss=keras.losses.categorical_crossentropy,\n",
    "    optimizer=SGD(lr=0.1), #find params\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    x_train, y_train,\n",
    "    validation_data=(x_test, y_test),\n",
    "    epochs=9999999999, batch_size=128,\n",
    "    callbacks=[keras.callbacks.EarlyStopping('val_loss', patience=5, min_delta=0.001)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14839/14839 [==============================] - 0s 18us/step\n",
      "loss = 1.49337662445\n",
      "acc = 0.380146910201\n"
     ]
    }
   ],
   "source": [
    "for metric, value in zip(model.metrics_names, model.evaluate(x_test, y_test)):\n",
    "    print(metric, '=', value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsgAAAGqCAYAAAAWf7K6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nWCBLkiTEQ0FEAAAACElEQVRJPb8A1azOyYybjAgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x184d79de80>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_history(history, \"No Hidden Layer\") # if do not store in var is displayed twice..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: One hidden layer, different optizimizers\n",
    "### Description\n",
    "\n",
    "Train a network with one hidden layer and compare different optimizers.\n",
    "\n",
    "1. Use one hidden layer with 64 units and the 'relu' activation. Use the [summary method](https://keras.io/models/about-keras-models/) to inspect your model.\n",
    "2. Fit the model for 50 epochs with different learning rates of stochastic gradient descent and answer the question below.\n",
    "3. Replace the stochastic gradient descent optimizer with the [Adam optimizer](https://keras.io/optimizers/#adam).\n",
    "4. Plot the learning curves of SGD with a reasonable learning rate together with the learning curves of Adam in the same figure. Take care of a reasonable labeling of the curves in the plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(64, input_shape=(x_train.shape[1],), activation=\"relu\"),\n",
    "    Dense(y_train.shape[1], activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    loss=keras.losses.categorical_crossentropy,\n",
    "    optimizer=SGD(lr=0.01),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "historySGD = model.fit(\n",
    "    x_train, y_train,\n",
    "    validation_data=(x_test, y_test),\n",
    "    epochs=50,\n",
    "    verbose=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 16,773\n",
      "Trainable params: 16,773\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsgAAAGqCAYAAAAWf7K6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nS5IkSR1/AP9iaB5rPrmyAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x184da8c240>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_history(historySGD, \"SGD\") # if do not store in var is displayed twice..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LARGE_RATE = 0.9\n",
    "SMALL_RATE = 10**(-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sgd_test_rate = Sequential([\n",
    "    Dense(64, input_shape=(x_train.shape[1],), activation=\"relu\"),\n",
    "    Dense(y_train.shape[1], activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sgd_test_rate.compile(\n",
    "    loss=keras.losses.categorical_crossentropy,\n",
    "    optimizer=SGD(lr=LARGE_RATE),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "_ = plot_history(sgd_test_rate.fit(\n",
    "    x_train, y_train,\n",
    "    validation_data=(x_test, y_test),\n",
    "    epochs=50,\n",
    "    verbose=0\n",
    "), \"SGD with very large learning rate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sgd_test_rate.compile(\n",
    "    loss=keras.losses.categorical_crossentropy,\n",
    "    optimizer=SGD(lr=SMALL_RATE),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "_ = plot_history(sgd_test_rate.fit(\n",
    "    x_train, y_train,\n",
    "    validation_data=(x_test, y_test),\n",
    "    epochs=50,\n",
    "    verbose=0\n",
    "), \"SGD with very small learning rate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: What happens if the learning rate of SGD is A) very large B) very small? Please answer A) and B) with one full sentence (double click this markdown cell to edit).\n",
    "\n",
    "**Answer**:\n",
    "\n",
    "A) The validation error and accuracy are very \"unstable\", the search for the optimal value goes in the wrong direction many times\n",
    "\n",
    "B) The improvement is very slow but constant: we would need three times the number of epochs to reach the best result obtained with the larger learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss=keras.losses.categorical_crossentropy,\n",
    "    optimizer=Adam(lr=0.001),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "historyAdam = model.fit(\n",
    "    x_train, y_train,\n",
    "    validation_data=(x_test, y_test),\n",
    "    epochs=50,\n",
    "    verbose=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_ = plot_history(historyAdam, \"Adam\") # if do not store in var is displayed twice..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the same learning rate with Adam and SGD does not allow Adam to improve its accuracy, therefore we changed it here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, ax1, ax2 = prepare_standardplot(\"Comparing SGD and ADAM\", 'epoch')\n",
    "ax1.plot(historySGD.history['loss'], label = \"training with SGD\", linestyle='--', c='b')\n",
    "ax1.plot(historySGD.history['val_loss'], label = \"validation with SGD\", linestyle='-', c='b')\n",
    "ax2.plot(historySGD.history['acc'], label = \"training with SGD\", linestyle='--', c='b')\n",
    "ax2.plot(historySGD.history['val_acc'], label = \"validation with SGD\", linestyle='-', c='b')\n",
    "\n",
    "ax1.plot(historyAdam.history['loss'], label = \"training with Adam\", linestyle='--', c='orange')\n",
    "ax1.plot(historyAdam.history['val_loss'], label = \"validation with Adam\", linestyle='-', c='orange')\n",
    "ax2.plot(historyAdam.history['acc'], label = \"training with Adam\", linestyle='--', c='orange')\n",
    "ax2.plot(historyAdam.history['val_acc'], label = \"validation with Adam\", linestyle='-', c='orange')\n",
    "finalize_standardplot(fig, ax1, ax2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Overfitting and early stopping with Adam\n",
    "\n",
    "### Description\n",
    "\n",
    "Run the above simulation with Adam for sufficiently many epochs (be patient!) until you see clear overfitting.\n",
    "\n",
    "1. Plot the learning curves of a fit with Adam and sufficiently many epochs and answer the questions below.\n",
    "\n",
    "A simple, but effective mean to avoid overfitting is early stopping, i.e. a fit is not run until convergence but stopped as soon as the validation error starts to increase. We will use early stopping in all subsequent exercises.\n",
    "\n",
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_ = plot_history(historyAdam, \"Adam\") # if do not store in var is displayed twice..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training we ran before was already overfitting: we can clearly see that the training error keeps decreasing while the validation error stays stable. We can see the same pattern observing the accuracy: the training accuracy keeps increasing while the validation accuracy is mostly stable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1**: At which epoch (approximately) does the model start to overfit? Please answer with one full sentence.\n",
    "\n",
    "**Answer**: The model start to overfit right away but after epoch 15 we clearly see that the validation error stays stable or increases while the training error keeps decreasing\n",
    "\n",
    "**Question 2**: Explain the qualitative difference between the loss curves and the accuracy curves with respect to signs of overfitting. Please answer with at most 3 full sentences.\n",
    "\n",
    "**Answer**: # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Model performance as a function of number of hidden neurons\n",
    "\n",
    "### Description\n",
    "\n",
    "Investigate how the best validation loss and accuracy depends on the number of hidden neurons in a single layer.\n",
    "\n",
    "1. Fit a reasonable number of models with different hidden layer size (between 10 and 1000 hidden neurons) for a fixed number of epochs well beyond the point of overfitting.\n",
    "2. Collect some statistics by fitting the same models as in 1. for multiple initial conditions. Hints: 1. If you don't reset the random seed, you get different initial conditions each time you create a new model. 2. Let your computer work while you are asleep.\n",
    "3. Plot summary statistics of the final validation loss and accuracy versus the number of hidden neurons. Hint: [boxplots](https://matplotlib.org/examples/pylab_examples/boxplot_demo.html) (also [here](https://matplotlib.org/api/_as_gen/matplotlib.axes.Axes.boxplot.html?highlight=boxplot#matplotlib.axes.Axes.boxplot)) are useful. You may also want to use the matplotlib method set_xticklabels.\n",
    "4. Plot summary statistics of the loss and accuracy for early stopping versus the number of hidden neurons.\n",
    "\n",
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we observed from our previous plots, the Adam optimizer converges much quicker than the SGD one. Therefore we will use it here. We saw that the model start to overfit at around 15 so we will train it for 30 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_model_result(hidden_neurons):\n",
    "    m = Sequential([\n",
    "        Dense(hidden_neurons, input_shape=(x_train.shape[1],), activation=\"relu\"),\n",
    "        Dense(y_train.shape[1], activation=\"softmax\")\n",
    "    ])\n",
    "    \n",
    "    m.compile(\n",
    "        loss=keras.losses.categorical_crossentropy,\n",
    "        optimizer=Adam(lr=0.001),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    h = model.fit(\n",
    "        x_train, y_train,\n",
    "        validation_data=(x_test, y_test),\n",
    "        epochs=30,\n",
    "        verbose=0,\n",
    "        callbacks=[keras.callbacks.EarlyStopping('val_loss', patience=3, min_delta=0.001)]\n",
    "    )\n",
    "    \n",
    "    return h.history['val_loss'][-1], h.history['val_acc'][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions 1 and 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed train 1/20\n",
      "Completed train 2/20\n",
      "Completed train 3/20\n",
      "Completed train 4/20\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-78b625833ddd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# https://keras.io/initializers/\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhash\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudent1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstudent2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_model_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Completed train {}/{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-38-e403f26cdfe2>\u001b[0m in \u001b[0;36mget_model_result\u001b[0;34m(hidden_neurons)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_delta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     )\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    958\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 960\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1655\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1656\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1657\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1659\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1217\u001b[0m                         \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mcallback_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_ts_batch_end\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt_before_callbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mdelta_t_median\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_ts_batch_end\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         if (self._delta_t_batch > 0. and\n\u001b[1;32m    113\u001b[0m            (delta_t_median > 0.95 * self._delta_t_batch and delta_t_median > 0.1)):\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36mmedian\u001b[0;34m(a, axis, out, overwrite_input, keepdims)\u001b[0m\n\u001b[1;32m   4100\u001b[0m     \"\"\"\n\u001b[1;32m   4101\u001b[0m     r, k = _ureduce(a, func=_median, axis=axis, out=out,\n\u001b[0;32m-> 4102\u001b[0;31m                     overwrite_input=overwrite_input)\n\u001b[0m\u001b[1;32m   4103\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4104\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36m_ureduce\u001b[0;34m(a, func, **kwargs)\u001b[0m\n\u001b[1;32m   4014\u001b[0m         \u001b[0mkeepdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4016\u001b[0;31m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4017\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4018\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36m_median\u001b[0;34m(a, axis, out, overwrite_input)\u001b[0m\n\u001b[1;32m   4152\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missubdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minexact\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msz\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4153\u001b[0m         \u001b[0;31m# warn and return nans like mean would\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4154\u001b[0;31m         \u001b[0mrout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpart\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4155\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_median_nancheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4156\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mmean\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m   2907\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2908\u001b[0m     return _methods._mean(a, axis=axis, dtype=dtype,\n\u001b[0;32m-> 2909\u001b[0;31m                           out=out, **kwargs)\n\u001b[0m\u001b[1;32m   2910\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_mean\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mis_float16_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0mrcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_count_reduce_items\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m     \u001b[0;31m# Make this warning show up first\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrcount\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "N = 20\n",
    "hidden_neurons = list(map(int, np.logspace(1, 3, N)))\n",
    "stats = []\n",
    "for idx, hid in enumerate(hidden_neurons):\n",
    "    # This seed or the seed parameter of initializer?\n",
    "    # https://keras.io/initializers/\n",
    "    np.random.seed(hash(student1 + student2) % 2**32)\n",
    "    stats.append(get_model_result(hid))\n",
    "    print(\"Completed train {}/{}\".format(idx+1, N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, (ax_loss, ax_acc) = plt.subplots(nrows=1, ncols=2)#, sharey=True)\n",
    "\n",
    "losses, accuracies = zip(*stats)\n",
    "\n",
    "ax_loss.boxplot(losses)\n",
    "ax_loss.set_title(\"Validation loss\")\n",
    "#ax_loss.set_yticklabels(hidden_neurons) # I am not sure it makes any sense: to build boxplot points are not kept in the same order\n",
    "\n",
    "\n",
    "ax_acc.boxplot(accuracies)\n",
    "ax_acc.set_title(\"Validation accuracy\")\n",
    "#ax_acc.set_yticklabels(hidden_neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, (ax_loss, ax_acc) = plt.subplots(nrows=1, ncols=2, sharey=True)\n",
    "\n",
    "ax_loss.plot(hidden_neurons, losses)\n",
    "ax_loss.set_title(\"Validation loss\")\n",
    "ax_acc.plot(hidden_neurons, accuracies)\n",
    "ax_acc.set_title(\"Validation accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: Comparison to deep models\n",
    "\n",
    "### Description\n",
    "\n",
    "Instead of choosing one hidden layer (with many neurons) you experiment here with multiple hidden layers (each with not so many neurons).\n",
    "\n",
    "1. Fit models with 2, 3 and 4 hidden layers with approximately the same number of parameters as a network with one hidden layer of 100 neurons. Hint: Calculate the number of parameters in a network with input dimensionality N_in, K hidden layers with N_h units, one output layer with N_out dimensions and solve for N_h. Confirm you result with the keras method model.summary().\n",
    "2. Run each model multiple times with different initial conditions and plot summary statistics of the best validation loss and accuracy versus the number of hidden layers.\n",
    "\n",
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit models with 2, 3 and 4 hidden layers with approximately the same number of parameters as a network with one hidden layer of 100 neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dense_factory(input_, output, weight_regularizer, bias_regularizer):\n",
    "    return Dense(output, input_shape=(input_,), activation=\"softmax\", kernel_initializer='random_uniform', kernel_regularizer=weight_regularizer, bias_regularizer=bias_regularizer)\n",
    "\n",
    "def model_factory(units, layers, weight_regularizer, bias_regularizer, dropout=None):\n",
    "    # Check input validity for dropout\n",
    "    if dropout is not None and len(dropout) < 2:\n",
    "        dropout.append(0)\n",
    "        \n",
    "    model_builder = Sequential()\n",
    "    \n",
    "    # Input layer\n",
    "    model_builder.add(dense_factory(x_train.shape[1], units, weight_regularizer, bias_regularizer)) \n",
    "    if dropout is not None:\n",
    "        model_builder.add(Dropout(dropout[0]))\n",
    "        \n",
    "    # Hidden layers\n",
    "    for i in range(layers-1):\n",
    "        model_builder.add(dense_factory((units,), units, weight_regularizer, bias_regularizer))\n",
    "        if dropout is not None:\n",
    "            model_builder.add(Dropout(dropout[1]))\n",
    "        \n",
    "    # Output layer\n",
    "    model_builder.add(dense_factory((units,), y_train.shape[1], weight_regularizer, bias_regularizer))\n",
    "    if dropout is not None:\n",
    "        model_builder.add(Dropout(dropout[1]))\n",
    "    \n",
    "    return model_builder\n",
    "\n",
    "# Dummy regularizer, used as non existing regularizer\n",
    "null_reg = keras.regularizers.l1(0)\n",
    "\n",
    "# Factories for required models\n",
    "size_per_layer_1_hidden = 100\n",
    "size_per_layer_2_hidden = 77\n",
    "size_per_layer_3_hidden = 66\n",
    "size_per_layer_4_hidden = 59\n",
    "\n",
    "def model_1_hidden_factory(weight_regularizer=None, bias_regularizer=None, dropout=None):\n",
    "    return model_factory(size_per_layer_1_hidden, 1, weight_regularizer, bias_regularizer, dropout)\n",
    "\n",
    "def model_2_hidden_factory(weight_regularizer=None, bias_regularizer=None, dropout=None):\n",
    "    return model_factory(size_per_layer_2_hidden, 2, weight_regularizer, bias_regularizer, dropout)\n",
    "\n",
    "def model_3_hidden_factory(weight_regularizer=None, bias_regularizer=None, dropout=None):\n",
    "    return model_factory(size_per_layer_3_hidden, 3, weight_regularizer, bias_regularizer, dropout)\n",
    "\n",
    "def model_4_hidden_factory(weight_regularizer=None, bias_regularizer=None, dropout=None):\n",
    "    return model_factory(size_per_layer_4_hidden, 4, weight_regularizer, bias_regularizer, dropout)\n",
    "\n",
    "def compile_model(model):\n",
    "    model.compile(\n",
    "        loss=keras.losses.categorical_crossentropy,\n",
    "        optimizer=Adam(lr=0.01),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "def fit_model(model, x_train=x_train, y_train=y_train, x_test=x_test, y_test=y_test):\n",
    "    h = model.fit(\n",
    "        x_train, y_train,\n",
    "        validation_data=(x_test, y_test),\n",
    "        epochs=80,\n",
    "        verbose=1,\n",
    "        callbacks=[keras.callbacks.EarlyStopping('val_loss', patience=5, min_delta=0.001)]\n",
    "    )   \n",
    "    return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confirm you result with the keras method model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 100)               25700     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 505       \n",
      "=================================================================\n",
      "Total params: 26,205\n",
      "Trainable params: 26,205\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 77)                19789     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 77)                6006      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 5)                 390       \n",
      "=================================================================\n",
      "Total params: 26,185\n",
      "Trainable params: 26,185\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 66)                16962     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 66)                4422      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 66)                4422      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 5)                 335       \n",
      "=================================================================\n",
      "Total params: 26,141\n",
      "Trainable params: 26,141\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_10 (Dense)             (None, 59)                15163     \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 59)                3540      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 59)                3540      \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 59)                3540      \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 5)                 300       \n",
      "=================================================================\n",
      "Total params: 26,083\n",
      "Trainable params: 26,083\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_1_hidden_factory().summary()\n",
    "model_2_hidden_factory().summary()\n",
    "model_3_hidden_factory().summary()\n",
    "model_4_hidden_factory().summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.\n",
    "#### Run each model multiple times with different initial conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model 1 / 4\n",
      "Inner step: 1 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 2s 76us/step - loss: 1.3334 - acc: 0.4002 - val_loss: 1.2195 - val_acc: 0.4806\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 2s 66us/step - loss: 1.1769 - acc: 0.4827 - val_loss: 1.1925 - val_acc: 0.5063\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 2s 56us/step - loss: 1.1583 - acc: 0.5007 - val_loss: 1.2088 - val_acc: 0.4931\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 2s 59us/step - loss: 1.1437 - acc: 0.5142 - val_loss: 1.2118 - val_acc: 0.4985\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 2s 60us/step - loss: 1.1448 - acc: 0.5198 - val_loss: 1.1916 - val_acc: 0.5193\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 2s 55us/step - loss: 1.1336 - acc: 0.5268 - val_loss: 1.1775 - val_acc: 0.5216\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 2s 55us/step - loss: 1.1337 - acc: 0.5255 - val_loss: 1.1773 - val_acc: 0.5251\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 2s 53us/step - loss: 1.1286 - acc: 0.5315 - val_loss: 1.1838 - val_acc: 0.5285\n",
      "Epoch 9/80\n",
      "29639/29639 [==============================] - 2s 55us/step - loss: 1.1294 - acc: 0.5307 - val_loss: 1.1755 - val_acc: 0.5286\n",
      "Epoch 10/80\n",
      "29639/29639 [==============================] - 2s 53us/step - loss: 1.1229 - acc: 0.5324 - val_loss: 1.1936 - val_acc: 0.5201\n",
      "Epoch 11/80\n",
      "29639/29639 [==============================] - 2s 58us/step - loss: 1.1245 - acc: 0.5333 - val_loss: 1.1713 - val_acc: 0.5294\n",
      "Epoch 12/80\n",
      "29639/29639 [==============================] - 2s 65us/step - loss: 1.1047 - acc: 0.5448 - val_loss: 1.1137 - val_acc: 0.5612\n",
      "Epoch 13/80\n",
      "29639/29639 [==============================] - 2s 54us/step - loss: 1.0240 - acc: 0.5779 - val_loss: 1.0547 - val_acc: 0.5777\n",
      "Epoch 14/80\n",
      "29639/29639 [==============================] - 2s 61us/step - loss: 0.9909 - acc: 0.5910 - val_loss: 1.0503 - val_acc: 0.5872\n",
      "Epoch 15/80\n",
      "29639/29639 [==============================] - 2s 60us/step - loss: 0.9836 - acc: 0.5948 - val_loss: 1.0477 - val_acc: 0.5785\n",
      "Epoch 16/80\n",
      "29639/29639 [==============================] - 2s 61us/step - loss: 0.9751 - acc: 0.6002 - val_loss: 1.0165 - val_acc: 0.6135\n",
      "Epoch 17/80\n",
      "29639/29639 [==============================] - 2s 57us/step - loss: 0.9544 - acc: 0.6203 - val_loss: 1.0049 - val_acc: 0.6121\n",
      "Epoch 18/80\n",
      "29639/29639 [==============================] - 2s 54us/step - loss: 0.9317 - acc: 0.6346 - val_loss: 1.0001 - val_acc: 0.6227\n",
      "Epoch 19/80\n",
      "29639/29639 [==============================] - 2s 53us/step - loss: 0.9107 - acc: 0.6410 - val_loss: 0.9881 - val_acc: 0.6284\n",
      "Epoch 20/80\n",
      "29639/29639 [==============================] - 2s 54us/step - loss: 0.9025 - acc: 0.6469 - val_loss: 1.0246 - val_acc: 0.6114\n",
      "Epoch 21/80\n",
      "29639/29639 [==============================] - 2s 54us/step - loss: 0.8974 - acc: 0.6501 - val_loss: 1.0409 - val_acc: 0.6100\n",
      "Epoch 22/80\n",
      "29639/29639 [==============================] - 2s 53us/step - loss: 0.8949 - acc: 0.6510 - val_loss: 1.1348 - val_acc: 0.5710\n",
      "Epoch 23/80\n",
      "29639/29639 [==============================] - 2s 52us/step - loss: 0.8824 - acc: 0.6574 - val_loss: 1.0317 - val_acc: 0.6191\n",
      "Epoch 24/80\n",
      "29639/29639 [==============================] - 2s 54us/step - loss: 0.8637 - acc: 0.6719 - val_loss: 0.9993 - val_acc: 0.6293\n",
      "Inner step: 2 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 2s 62us/step - loss: 1.4010 - acc: 0.3567 - val_loss: 1.3108 - val_acc: 0.4211\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 2s 55us/step - loss: 1.2647 - acc: 0.4198 - val_loss: 1.2950 - val_acc: 0.4234\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 2s 54us/step - loss: 1.2482 - acc: 0.4272 - val_loss: 1.2965 - val_acc: 0.4184\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 2s 55us/step - loss: 1.2458 - acc: 0.4316 - val_loss: 1.3317 - val_acc: 0.4070\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 2s 55us/step - loss: 1.2394 - acc: 0.4302 - val_loss: 1.2920 - val_acc: 0.4244\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 2s 55us/step - loss: 1.2360 - acc: 0.4347 - val_loss: 1.2832 - val_acc: 0.4215\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 2s 55us/step - loss: 1.2346 - acc: 0.4323 - val_loss: 1.2997 - val_acc: 0.4274\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 2s 52us/step - loss: 1.2308 - acc: 0.4397 - val_loss: 1.2907 - val_acc: 0.4221\n",
      "Epoch 9/80\n",
      "29639/29639 [==============================] - 2s 54us/step - loss: 1.2308 - acc: 0.4343 - val_loss: 1.3119 - val_acc: 0.4250\n",
      "Epoch 10/80\n",
      "29639/29639 [==============================] - 2s 55us/step - loss: 1.2312 - acc: 0.4341 - val_loss: 1.2912 - val_acc: 0.4234\n",
      "Epoch 11/80\n",
      "29639/29639 [==============================] - 2s 57us/step - loss: 1.2293 - acc: 0.4360 - val_loss: 1.2882 - val_acc: 0.4490\n",
      "Inner step: 3 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 2s 66us/step - loss: 1.5032 - acc: 0.3180 - val_loss: 1.4602 - val_acc: 0.3426\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 2s 54us/step - loss: 1.4380 - acc: 0.3611 - val_loss: 1.4520 - val_acc: 0.3645\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 2s 54us/step - loss: 1.4313 - acc: 0.3663 - val_loss: 1.4748 - val_acc: 0.3610\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 2s 55us/step - loss: 1.4288 - acc: 0.3678 - val_loss: 1.4552 - val_acc: 0.3649\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 2s 55us/step - loss: 1.4065 - acc: 0.3677 - val_loss: 1.3758 - val_acc: 0.3841\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 2s 54us/step - loss: 1.3336 - acc: 0.3775 - val_loss: 1.3968 - val_acc: 0.3852\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 2s 55us/step - loss: 1.3282 - acc: 0.3803 - val_loss: 1.3722 - val_acc: 0.3868\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 2s 56us/step - loss: 1.2207 - acc: 0.4569 - val_loss: 1.2710 - val_acc: 0.4751\n",
      "Epoch 9/80\n",
      "29639/29639 [==============================] - 2s 53us/step - loss: 1.1474 - acc: 0.4916 - val_loss: 1.1976 - val_acc: 0.4844\n",
      "Epoch 10/80\n",
      "29639/29639 [==============================] - 2s 56us/step - loss: 1.1424 - acc: 0.4934 - val_loss: 1.2149 - val_acc: 0.4782\n",
      "Epoch 11/80\n",
      "29639/29639 [==============================] - 2s 55us/step - loss: 1.1391 - acc: 0.4913 - val_loss: 1.1986 - val_acc: 0.4883\n",
      "Epoch 12/80\n",
      "29639/29639 [==============================] - 2s 54us/step - loss: 1.1386 - acc: 0.4925 - val_loss: 1.1873 - val_acc: 0.4864\n",
      "Epoch 13/80\n",
      "29639/29639 [==============================] - 2s 56us/step - loss: 1.1360 - acc: 0.4934 - val_loss: 1.1911 - val_acc: 0.5001\n",
      "Epoch 14/80\n",
      "29639/29639 [==============================] - 2s 54us/step - loss: 1.1332 - acc: 0.4938 - val_loss: 1.2029 - val_acc: 0.4815\n",
      "Epoch 15/80\n",
      "29639/29639 [==============================] - 2s 55us/step - loss: 1.1292 - acc: 0.5006 - val_loss: 1.1209 - val_acc: 0.5510\n",
      "Epoch 16/80\n",
      "29639/29639 [==============================] - 2s 54us/step - loss: 1.0273 - acc: 0.5680 - val_loss: 1.0684 - val_acc: 0.5752\n",
      "Epoch 17/80\n",
      "29639/29639 [==============================] - 2s 55us/step - loss: 0.9911 - acc: 0.5833 - val_loss: 1.1122 - val_acc: 0.5655\n",
      "Epoch 18/80\n",
      "29639/29639 [==============================] - 2s 54us/step - loss: 0.9715 - acc: 0.5972 - val_loss: 1.0257 - val_acc: 0.5988\n",
      "Epoch 19/80\n",
      "29639/29639 [==============================] - 2s 53us/step - loss: 0.9572 - acc: 0.6103 - val_loss: 1.0164 - val_acc: 0.6055\n",
      "Epoch 20/80\n",
      "29639/29639 [==============================] - 2s 52us/step - loss: 0.9468 - acc: 0.6158 - val_loss: 1.0308 - val_acc: 0.6084\n",
      "Epoch 21/80\n",
      "29639/29639 [==============================] - 2s 52us/step - loss: 0.9310 - acc: 0.6245 - val_loss: 1.0585 - val_acc: 0.6106\n",
      "Epoch 22/80\n",
      "29639/29639 [==============================] - 2s 53us/step - loss: 0.9220 - acc: 0.6327 - val_loss: 1.0483 - val_acc: 0.6154\n",
      "Epoch 23/80\n",
      "29639/29639 [==============================] - 2s 53us/step - loss: 0.9102 - acc: 0.6363 - val_loss: 1.0144 - val_acc: 0.6135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/80\n",
      "29639/29639 [==============================] - 2s 52us/step - loss: 0.9101 - acc: 0.6362 - val_loss: 0.9941 - val_acc: 0.6160\n",
      "Epoch 25/80\n",
      "29639/29639 [==============================] - 2s 53us/step - loss: 0.9054 - acc: 0.6385 - val_loss: 0.9735 - val_acc: 0.6299\n",
      "Epoch 26/80\n",
      "29639/29639 [==============================] - 2s 52us/step - loss: 0.8942 - acc: 0.6415 - val_loss: 0.9819 - val_acc: 0.6246\n",
      "Epoch 27/80\n",
      "29639/29639 [==============================] - 2s 52us/step - loss: 0.8963 - acc: 0.6428 - val_loss: 0.9855 - val_acc: 0.6207\n",
      "Epoch 28/80\n",
      "29639/29639 [==============================] - 2s 53us/step - loss: 0.8924 - acc: 0.6422 - val_loss: 0.9798 - val_acc: 0.6234\n",
      "Epoch 29/80\n",
      "29639/29639 [==============================] - 2s 53us/step - loss: 0.8859 - acc: 0.6436 - val_loss: 1.0115 - val_acc: 0.6089\n",
      "Epoch 30/80\n",
      "29639/29639 [==============================] - 2s 52us/step - loss: 0.8852 - acc: 0.6449 - val_loss: 1.0111 - val_acc: 0.6143\n",
      "Inner step: 4 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 2s 61us/step - loss: 1.4652 - acc: 0.3302 - val_loss: 1.4343 - val_acc: 0.3636\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 2s 52us/step - loss: 1.4138 - acc: 0.3604 - val_loss: 1.4308 - val_acc: 0.3652\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 2s 51us/step - loss: 1.4093 - acc: 0.3631 - val_loss: 1.4347 - val_acc: 0.3672\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 2s 51us/step - loss: 1.4068 - acc: 0.3640 - val_loss: 1.4365 - val_acc: 0.3656\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 2s 51us/step - loss: 1.4054 - acc: 0.3654 - val_loss: 1.4246 - val_acc: 0.3651\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 1s 51us/step - loss: 1.4050 - acc: 0.3637 - val_loss: 1.4500 - val_acc: 0.3619\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 2s 55us/step - loss: 1.4050 - acc: 0.3641 - val_loss: 1.4264 - val_acc: 0.3665\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 2s 57us/step - loss: 1.4052 - acc: 0.3643 - val_loss: 1.4305 - val_acc: 0.3663\n",
      "Epoch 9/80\n",
      "29639/29639 [==============================] - 2s 51us/step - loss: 1.3397 - acc: 0.4117 - val_loss: 1.2828 - val_acc: 0.4549\n",
      "Epoch 10/80\n",
      "29639/29639 [==============================] - 2s 51us/step - loss: 1.2394 - acc: 0.4652 - val_loss: 1.2664 - val_acc: 0.4657\n",
      "Epoch 11/80\n",
      "29639/29639 [==============================] - 2s 57us/step - loss: 1.2351 - acc: 0.4684 - val_loss: 1.3182 - val_acc: 0.4563\n",
      "Epoch 12/80\n",
      "29639/29639 [==============================] - 2s 52us/step - loss: 1.2287 - acc: 0.4695 - val_loss: 1.2595 - val_acc: 0.4663\n",
      "Epoch 13/80\n",
      "29639/29639 [==============================] - 2s 51us/step - loss: 1.2281 - acc: 0.4716 - val_loss: 1.2849 - val_acc: 0.4627\n",
      "Epoch 14/80\n",
      "29639/29639 [==============================] - 2s 51us/step - loss: 1.2265 - acc: 0.4718 - val_loss: 1.2758 - val_acc: 0.4622\n",
      "Epoch 15/80\n",
      "29639/29639 [==============================] - 2s 51us/step - loss: 1.2069 - acc: 0.4846 - val_loss: 1.2180 - val_acc: 0.4921\n",
      "Epoch 16/80\n",
      "29639/29639 [==============================] - 2s 51us/step - loss: 1.1731 - acc: 0.5020 - val_loss: 1.2794 - val_acc: 0.4891\n",
      "Epoch 17/80\n",
      "29639/29639 [==============================] - 2s 51us/step - loss: 1.0445 - acc: 0.5939 - val_loss: 1.0609 - val_acc: 0.6062\n",
      "Epoch 18/80\n",
      "29639/29639 [==============================] - 2s 51us/step - loss: 0.9879 - acc: 0.6239 - val_loss: 1.0546 - val_acc: 0.6092\n",
      "Epoch 19/80\n",
      "29639/29639 [==============================] - 2s 51us/step - loss: 0.9643 - acc: 0.6342 - val_loss: 1.0638 - val_acc: 0.6055\n",
      "Epoch 20/80\n",
      "29639/29639 [==============================] - 2s 51us/step - loss: 0.9567 - acc: 0.6364 - val_loss: 1.0205 - val_acc: 0.6224\n",
      "Epoch 21/80\n",
      "29639/29639 [==============================] - 2s 51us/step - loss: 0.9430 - acc: 0.6414 - val_loss: 1.0229 - val_acc: 0.6185\n",
      "Epoch 22/80\n",
      "29639/29639 [==============================] - 2s 51us/step - loss: 0.9240 - acc: 0.6523 - val_loss: 1.0179 - val_acc: 0.6343\n",
      "Epoch 23/80\n",
      "29639/29639 [==============================] - 2s 51us/step - loss: 0.8997 - acc: 0.6683 - val_loss: 1.0119 - val_acc: 0.6381\n",
      "Epoch 24/80\n",
      "29639/29639 [==============================] - 2s 57us/step - loss: 0.8813 - acc: 0.6779 - val_loss: 0.9720 - val_acc: 0.6536\n",
      "Epoch 25/80\n",
      "29639/29639 [==============================] - 2s 57us/step - loss: 0.8658 - acc: 0.6825 - val_loss: 0.9380 - val_acc: 0.6642\n",
      "Epoch 26/80\n",
      "29639/29639 [==============================] - 2s 55us/step - loss: 0.8466 - acc: 0.6937 - val_loss: 0.9308 - val_acc: 0.6703\n",
      "Epoch 27/80\n",
      "29639/29639 [==============================] - 2s 55us/step - loss: 0.8124 - acc: 0.7122 - val_loss: 0.8784 - val_acc: 0.6978\n",
      "Epoch 28/80\n",
      "29639/29639 [==============================] - 2s 55us/step - loss: 0.7820 - acc: 0.7275 - val_loss: 0.9076 - val_acc: 0.6783\n",
      "Epoch 29/80\n",
      "29639/29639 [==============================] - 2s 55us/step - loss: 0.7474 - acc: 0.7455 - val_loss: 0.8548 - val_acc: 0.7160\n",
      "Epoch 30/80\n",
      "29639/29639 [==============================] - 2s 55us/step - loss: 0.7211 - acc: 0.7597 - val_loss: 0.8023 - val_acc: 0.7410\n",
      "Epoch 31/80\n",
      "29639/29639 [==============================] - 2s 55us/step - loss: 0.6928 - acc: 0.7710 - val_loss: 0.8083 - val_acc: 0.7346\n",
      "Epoch 32/80\n",
      "29639/29639 [==============================] - 2s 54us/step - loss: 0.6778 - acc: 0.7772 - val_loss: 0.8174 - val_acc: 0.7392\n",
      "Epoch 33/80\n",
      "29639/29639 [==============================] - 2s 53us/step - loss: 0.6606 - acc: 0.7843 - val_loss: 0.7968 - val_acc: 0.7419\n",
      "Epoch 34/80\n",
      "29639/29639 [==============================] - 2s 55us/step - loss: 0.6573 - acc: 0.7861 - val_loss: 0.7782 - val_acc: 0.7434\n",
      "Epoch 35/80\n",
      "29639/29639 [==============================] - 2s 55us/step - loss: 0.6422 - acc: 0.7891 - val_loss: 0.7831 - val_acc: 0.7456\n",
      "Epoch 36/80\n",
      "29639/29639 [==============================] - 2s 54us/step - loss: 0.6330 - acc: 0.7945 - val_loss: 0.7654 - val_acc: 0.7543\n",
      "Epoch 37/80\n",
      "29639/29639 [==============================] - 2s 54us/step - loss: 0.6288 - acc: 0.7944 - val_loss: 0.8011 - val_acc: 0.7340\n",
      "Epoch 38/80\n",
      "29639/29639 [==============================] - 2s 56us/step - loss: 0.6169 - acc: 0.7988 - val_loss: 0.7573 - val_acc: 0.7579\n",
      "Epoch 39/80\n",
      "29639/29639 [==============================] - 2s 54us/step - loss: 0.6147 - acc: 0.8012 - val_loss: 0.8004 - val_acc: 0.7431\n",
      "Epoch 40/80\n",
      "29639/29639 [==============================] - 2s 56us/step - loss: 0.6106 - acc: 0.8015 - val_loss: 0.7902 - val_acc: 0.7546\n",
      "Epoch 41/80\n",
      "29639/29639 [==============================] - 2s 55us/step - loss: 0.6032 - acc: 0.8027 - val_loss: 0.7787 - val_acc: 0.7486\n",
      "Epoch 42/80\n",
      "29639/29639 [==============================] - 2s 54us/step - loss: 0.5991 - acc: 0.8066 - val_loss: 0.7700 - val_acc: 0.7509\n",
      "Epoch 43/80\n",
      "29639/29639 [==============================] - 2s 54us/step - loss: 0.5998 - acc: 0.8059 - val_loss: 0.7664 - val_acc: 0.7626\n",
      "Inner step: 5 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 2s 68us/step - loss: 1.4754 - acc: 0.3243 - val_loss: 1.4455 - val_acc: 0.3432\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 2s 56us/step - loss: 1.4126 - acc: 0.3607 - val_loss: 1.4517 - val_acc: 0.3622\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 2s 57us/step - loss: 1.4074 - acc: 0.3642 - val_loss: 1.4267 - val_acc: 0.3652\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 2s 59us/step - loss: 1.4064 - acc: 0.3637 - val_loss: 1.4413 - val_acc: 0.3665\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 2s 55us/step - loss: 1.4053 - acc: 0.3655 - val_loss: 1.4408 - val_acc: 0.3621\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 2s 58us/step - loss: 1.4049 - acc: 0.3656 - val_loss: 1.4379 - val_acc: 0.3614\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 2s 56us/step - loss: 1.4042 - acc: 0.3660 - val_loss: 1.4363 - val_acc: 0.3656\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 2s 57us/step - loss: 1.4045 - acc: 0.3658 - val_loss: 1.4324 - val_acc: 0.3662\n",
      "Inner step: 6 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29639/29639 [==============================] - 2s 68us/step - loss: 1.2146 - acc: 0.4847 - val_loss: 1.0976 - val_acc: 0.5643\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 2s 56us/step - loss: 1.0202 - acc: 0.5747 - val_loss: 1.0519 - val_acc: 0.5777\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 2s 57us/step - loss: 1.0015 - acc: 0.5820 - val_loss: 1.0355 - val_acc: 0.5830\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 2s 57us/step - loss: 0.9995 - acc: 0.5824 - val_loss: 1.0491 - val_acc: 0.5791\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 2s 57us/step - loss: 0.9861 - acc: 0.5866 - val_loss: 1.0424 - val_acc: 0.5798\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 2s 57us/step - loss: 0.9815 - acc: 0.5901 - val_loss: 1.0821 - val_acc: 0.5668\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 2s 56us/step - loss: 0.9811 - acc: 0.5884 - val_loss: 1.0562 - val_acc: 0.5777\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 2s 58us/step - loss: 0.9803 - acc: 0.5939 - val_loss: 1.0451 - val_acc: 0.5760\n",
      "Inner step: 7 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 2s 69us/step - loss: 1.4145 - acc: 0.3461 - val_loss: 1.3799 - val_acc: 0.3819\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 2s 56us/step - loss: 1.3387 - acc: 0.3726 - val_loss: 1.3664 - val_acc: 0.3829\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 2s 56us/step - loss: 1.3309 - acc: 0.3810 - val_loss: 1.3690 - val_acc: 0.3878\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 2s 56us/step - loss: 1.3272 - acc: 0.3795 - val_loss: 1.3632 - val_acc: 0.3853\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 2s 57us/step - loss: 1.3267 - acc: 0.3800 - val_loss: 1.3759 - val_acc: 0.3858\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 2s 57us/step - loss: 1.3224 - acc: 0.3810 - val_loss: 1.3643 - val_acc: 0.3890\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 2s 57us/step - loss: 1.3234 - acc: 0.3875 - val_loss: 1.3684 - val_acc: 0.3848\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 2s 57us/step - loss: 1.3185 - acc: 0.3832 - val_loss: 1.3584 - val_acc: 0.3878\n",
      "Epoch 9/80\n",
      "29639/29639 [==============================] - 2s 56us/step - loss: 1.3234 - acc: 0.3866 - val_loss: 1.3661 - val_acc: 0.3871\n",
      "Epoch 10/80\n",
      "29639/29639 [==============================] - 2s 57us/step - loss: 1.3208 - acc: 0.3864 - val_loss: 1.3747 - val_acc: 0.3854\n",
      "Epoch 11/80\n",
      "29639/29639 [==============================] - 2s 57us/step - loss: 1.3194 - acc: 0.3861 - val_loss: 1.3697 - val_acc: 0.3873\n",
      "Epoch 12/80\n",
      "29639/29639 [==============================] - 2s 56us/step - loss: 1.3183 - acc: 0.3881 - val_loss: 1.3671 - val_acc: 0.3894\n",
      "Epoch 13/80\n",
      "29639/29639 [==============================] - 2s 57us/step - loss: 1.3172 - acc: 0.3856 - val_loss: 1.3742 - val_acc: 0.3895\n",
      "Inner step: 8 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 2s 75us/step - loss: 1.4227 - acc: 0.3401 - val_loss: 1.3722 - val_acc: 0.3803\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 2s 57us/step - loss: 1.3366 - acc: 0.3775 - val_loss: 1.3714 - val_acc: 0.3860\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 2s 55us/step - loss: 1.3306 - acc: 0.3813 - val_loss: 1.3799 - val_acc: 0.3811\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 2s 52us/step - loss: 1.3282 - acc: 0.3830 - val_loss: 1.3671 - val_acc: 0.3821\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 2s 52us/step - loss: 1.3250 - acc: 0.3836 - val_loss: 1.3799 - val_acc: 0.3780\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 2s 51us/step - loss: 1.3251 - acc: 0.3826 - val_loss: 1.3668 - val_acc: 0.3865\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 2s 52us/step - loss: 1.3217 - acc: 0.3886 - val_loss: 1.3810 - val_acc: 0.3852\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 2s 51us/step - loss: 1.3239 - acc: 0.3848 - val_loss: 1.3818 - val_acc: 0.3841\n",
      "Epoch 9/80\n",
      "29639/29639 [==============================] - 2s 51us/step - loss: 1.3223 - acc: 0.3844 - val_loss: 1.3762 - val_acc: 0.3891\n",
      "Inner step: 9 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 2s 65us/step - loss: 1.4227 - acc: 0.3435 - val_loss: 1.3765 - val_acc: 0.3716\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 2s 53us/step - loss: 1.3374 - acc: 0.3793 - val_loss: 1.3705 - val_acc: 0.3768\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 2s 52us/step - loss: 1.3300 - acc: 0.3770 - val_loss: 1.3823 - val_acc: 0.3779\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 2s 52us/step - loss: 1.3280 - acc: 0.3840 - val_loss: 1.3740 - val_acc: 0.3845\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 2s 52us/step - loss: 1.3253 - acc: 0.3817 - val_loss: 1.3679 - val_acc: 0.3884\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 2s 52us/step - loss: 1.3239 - acc: 0.3840 - val_loss: 1.3785 - val_acc: 0.3861\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 2s 52us/step - loss: 1.3214 - acc: 0.3821 - val_loss: 1.3612 - val_acc: 0.3866\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 2s 51us/step - loss: 1.3205 - acc: 0.3844 - val_loss: 1.3752 - val_acc: 0.3877\n",
      "Epoch 9/80\n",
      "29639/29639 [==============================] - 2s 52us/step - loss: 1.3209 - acc: 0.3850 - val_loss: 1.3657 - val_acc: 0.3759\n",
      "Epoch 10/80\n",
      "29639/29639 [==============================] - 2s 52us/step - loss: 1.3178 - acc: 0.3859 - val_loss: 1.3599 - val_acc: 0.3874\n",
      "Epoch 11/80\n",
      "29639/29639 [==============================] - 2s 52us/step - loss: 1.3189 - acc: 0.3864 - val_loss: 1.3857 - val_acc: 0.3833\n",
      "Epoch 12/80\n",
      "29639/29639 [==============================] - 2s 52us/step - loss: 1.3187 - acc: 0.3872 - val_loss: 1.3690 - val_acc: 0.3848\n",
      "Epoch 13/80\n",
      "29639/29639 [==============================] - 2s 52us/step - loss: 1.3167 - acc: 0.3896 - val_loss: 1.3659 - val_acc: 0.3820\n",
      "Epoch 14/80\n",
      "29639/29639 [==============================] - 2s 52us/step - loss: 1.3184 - acc: 0.3890 - val_loss: 1.3746 - val_acc: 0.3871\n",
      "Epoch 15/80\n",
      "29639/29639 [==============================] - 2s 52us/step - loss: 1.3178 - acc: 0.3897 - val_loss: 1.3878 - val_acc: 0.3868\n",
      "Inner step: 10 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 2s 66us/step - loss: 1.4187 - acc: 0.3387 - val_loss: 1.3721 - val_acc: 0.3831\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 2s 53us/step - loss: 1.3376 - acc: 0.3762 - val_loss: 1.3585 - val_acc: 0.3775\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 2s 53us/step - loss: 1.3301 - acc: 0.3758 - val_loss: 1.3674 - val_acc: 0.3792\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 2s 53us/step - loss: 1.3280 - acc: 0.3777 - val_loss: 1.3626 - val_acc: 0.3768\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 2s 52us/step - loss: 1.3255 - acc: 0.3810 - val_loss: 1.3637 - val_acc: 0.3837\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 2s 54us/step - loss: 1.3237 - acc: 0.3786 - val_loss: 1.3641 - val_acc: 0.3784\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 2s 53us/step - loss: 1.3222 - acc: 0.3801 - val_loss: 1.3662 - val_acc: 0.3833\n",
      "Inner step: 11 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 2s 75us/step - loss: 1.3118 - acc: 0.4158 - val_loss: 1.2121 - val_acc: 0.4851\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 2s 54us/step - loss: 1.1691 - acc: 0.4938 - val_loss: 1.1998 - val_acc: 0.5073\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 2s 54us/step - loss: 1.1523 - acc: 0.5112 - val_loss: 1.2270 - val_acc: 0.4875\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 2s 54us/step - loss: 1.1417 - acc: 0.5214 - val_loss: 1.2243 - val_acc: 0.5060\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 2s 54us/step - loss: 1.1368 - acc: 0.5262 - val_loss: 1.1949 - val_acc: 0.5212\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 2s 54us/step - loss: 1.1331 - acc: 0.5290 - val_loss: 1.1727 - val_acc: 0.5276\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 2s 54us/step - loss: 1.1284 - acc: 0.5318 - val_loss: 1.1954 - val_acc: 0.5181\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 2s 55us/step - loss: 1.1070 - acc: 0.5419 - val_loss: 1.1227 - val_acc: 0.5555\n",
      "Epoch 9/80\n",
      "29639/29639 [==============================] - 2s 54us/step - loss: 1.0384 - acc: 0.5735 - val_loss: 1.1956 - val_acc: 0.5378\n",
      "Epoch 10/80\n",
      "29639/29639 [==============================] - 2s 54us/step - loss: 1.0034 - acc: 0.5880 - val_loss: 1.0866 - val_acc: 0.5778\n",
      "Epoch 11/80\n",
      "29639/29639 [==============================] - 2s 54us/step - loss: 0.9693 - acc: 0.6128 - val_loss: 1.0161 - val_acc: 0.6181\n",
      "Epoch 12/80\n",
      "29639/29639 [==============================] - 2s 55us/step - loss: 0.9308 - acc: 0.6379 - val_loss: 1.0282 - val_acc: 0.6213\n",
      "Epoch 13/80\n",
      "29639/29639 [==============================] - 2s 56us/step - loss: 0.9110 - acc: 0.6469 - val_loss: 1.0059 - val_acc: 0.6225\n",
      "Epoch 14/80\n",
      "29639/29639 [==============================] - 2s 54us/step - loss: 0.9074 - acc: 0.6493 - val_loss: 1.0353 - val_acc: 0.6120\n",
      "Epoch 15/80\n",
      "29639/29639 [==============================] - 2s 54us/step - loss: 0.8951 - acc: 0.6515 - val_loss: 1.0150 - val_acc: 0.6204\n",
      "Epoch 16/80\n",
      "29639/29639 [==============================] - 2s 54us/step - loss: 0.8932 - acc: 0.6537 - val_loss: 1.0011 - val_acc: 0.6236\n",
      "Epoch 17/80\n",
      "29639/29639 [==============================] - 2s 55us/step - loss: 0.8934 - acc: 0.6557 - val_loss: 0.9823 - val_acc: 0.6287\n",
      "Epoch 18/80\n",
      "29639/29639 [==============================] - 2s 54us/step - loss: 0.8884 - acc: 0.6546 - val_loss: 0.9833 - val_acc: 0.6348\n",
      "Epoch 19/80\n",
      "29639/29639 [==============================] - 2s 55us/step - loss: 0.8819 - acc: 0.6593 - val_loss: 1.0276 - val_acc: 0.6114\n",
      "Epoch 20/80\n",
      "29639/29639 [==============================] - 2s 55us/step - loss: 0.8838 - acc: 0.6565 - val_loss: 0.9822 - val_acc: 0.6343\n",
      "Epoch 21/80\n",
      "29639/29639 [==============================] - 2s 54us/step - loss: 0.8758 - acc: 0.6596 - val_loss: 0.9831 - val_acc: 0.6217\n",
      "Epoch 22/80\n",
      "29639/29639 [==============================] - 2s 54us/step - loss: 0.8699 - acc: 0.6621 - val_loss: 0.9791 - val_acc: 0.6210\n",
      "Epoch 23/80\n",
      "29639/29639 [==============================] - 2s 54us/step - loss: 0.8665 - acc: 0.6628 - val_loss: 0.9627 - val_acc: 0.6304\n",
      "Epoch 24/80\n",
      "29639/29639 [==============================] - 2s 54us/step - loss: 0.8576 - acc: 0.6664 - val_loss: 0.9512 - val_acc: 0.6466\n",
      "Epoch 25/80\n",
      "29639/29639 [==============================] - 2s 54us/step - loss: 0.8514 - acc: 0.6671 - val_loss: 0.9687 - val_acc: 0.6418\n",
      "Epoch 26/80\n",
      "29639/29639 [==============================] - 2s 59us/step - loss: 0.8320 - acc: 0.6770 - val_loss: 0.9175 - val_acc: 0.6653\n",
      "Epoch 27/80\n",
      "29639/29639 [==============================] - 2s 58us/step - loss: 0.7989 - acc: 0.6970 - val_loss: 0.8757 - val_acc: 0.6900\n",
      "Epoch 28/80\n",
      "29639/29639 [==============================] - 2s 59us/step - loss: 0.7703 - acc: 0.7144 - val_loss: 0.8723 - val_acc: 0.7041\n",
      "Epoch 29/80\n",
      "29639/29639 [==============================] - 2s 58us/step - loss: 0.7430 - acc: 0.7370 - val_loss: 0.8959 - val_acc: 0.7153\n",
      "Epoch 30/80\n",
      "29639/29639 [==============================] - 2s 58us/step - loss: 0.7273 - acc: 0.7535 - val_loss: 0.8477 - val_acc: 0.7272\n",
      "Epoch 31/80\n",
      "29639/29639 [==============================] - 2s 62us/step - loss: 0.7111 - acc: 0.7626 - val_loss: 0.7916 - val_acc: 0.7518\n",
      "Epoch 32/80\n",
      "29639/29639 [==============================] - 2s 69us/step - loss: 0.6901 - acc: 0.7752 - val_loss: 0.8131 - val_acc: 0.7458\n",
      "Epoch 33/80\n",
      "29639/29639 [==============================] - 2s 76us/step - loss: 0.6879 - acc: 0.7792 - val_loss: 0.8130 - val_acc: 0.7498\n",
      "Epoch 34/80\n",
      "29639/29639 [==============================] - 2s 72us/step - loss: 0.6759 - acc: 0.7849 - val_loss: 0.8288 - val_acc: 0.7387\n",
      "Epoch 35/80\n",
      "29639/29639 [==============================] - 2s 70us/step - loss: 0.6651 - acc: 0.7896 - val_loss: 0.8352 - val_acc: 0.7444\n",
      "Epoch 36/80\n",
      "29639/29639 [==============================] - 2s 70us/step - loss: 0.6558 - acc: 0.7943 - val_loss: 0.8118 - val_acc: 0.7491\n",
      "Inner step: 12 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 2s 81us/step - loss: 1.5085 - acc: 0.2967 - val_loss: 1.4371 - val_acc: 0.3601\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 2s 64us/step - loss: 1.3167 - acc: 0.4163 - val_loss: 1.3204 - val_acc: 0.4316\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 2s 54us/step - loss: 1.2557 - acc: 0.4589 - val_loss: 1.2957 - val_acc: 0.4631\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 2s 58us/step - loss: 1.2409 - acc: 0.4839 - val_loss: 1.2782 - val_acc: 0.4925\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 2s 52us/step - loss: 1.2242 - acc: 0.5001 - val_loss: 1.2581 - val_acc: 0.5059\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 2s 57us/step - loss: 1.2206 - acc: 0.5095 - val_loss: 1.2904 - val_acc: 0.4872\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 2s 53us/step - loss: 1.2006 - acc: 0.5180 - val_loss: 1.2514 - val_acc: 0.4947\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 2s 56us/step - loss: 1.1357 - acc: 0.5363 - val_loss: 1.1739 - val_acc: 0.5276\n",
      "Epoch 9/80\n",
      "29639/29639 [==============================] - 2s 57us/step - loss: 1.1055 - acc: 0.5439 - val_loss: 1.1606 - val_acc: 0.5353\n",
      "Epoch 10/80\n",
      "29639/29639 [==============================] - 2s 61us/step - loss: 1.0964 - acc: 0.5464 - val_loss: 1.1777 - val_acc: 0.5327\n",
      "Epoch 11/80\n",
      "29639/29639 [==============================] - 2s 59us/step - loss: 1.0906 - acc: 0.5479 - val_loss: 1.1493 - val_acc: 0.5384\n",
      "Epoch 12/80\n",
      "29639/29639 [==============================] - 2s 66us/step - loss: 1.0881 - acc: 0.5493 - val_loss: 1.1541 - val_acc: 0.5395\n",
      "Epoch 13/80\n",
      "29639/29639 [==============================] - 2s 60us/step - loss: 1.0901 - acc: 0.5488 - val_loss: 1.1525 - val_acc: 0.5390\n",
      "Epoch 14/80\n",
      "29639/29639 [==============================] - 2s 55us/step - loss: 1.0795 - acc: 0.5561 - val_loss: 1.1273 - val_acc: 0.5595\n",
      "Epoch 15/80\n",
      "29639/29639 [==============================] - 2s 67us/step - loss: 1.0500 - acc: 0.5791 - val_loss: 1.1831 - val_acc: 0.5443\n",
      "Epoch 16/80\n",
      "29639/29639 [==============================] - 2s 55us/step - loss: 1.0358 - acc: 0.5867 - val_loss: 1.1201 - val_acc: 0.5681\n",
      "Epoch 17/80\n",
      "29639/29639 [==============================] - 2s 53us/step - loss: 1.0230 - acc: 0.5953 - val_loss: 1.1189 - val_acc: 0.5737\n",
      "Epoch 18/80\n",
      "29639/29639 [==============================] - 2s 53us/step - loss: 0.9833 - acc: 0.6282 - val_loss: 1.0630 - val_acc: 0.6092\n",
      "Epoch 19/80\n",
      "29639/29639 [==============================] - 2s 54us/step - loss: 0.9319 - acc: 0.6555 - val_loss: 1.0231 - val_acc: 0.6355\n",
      "Epoch 20/80\n",
      "29639/29639 [==============================] - 2s 56us/step - loss: 0.9051 - acc: 0.6677 - val_loss: 1.0221 - val_acc: 0.6375\n",
      "Epoch 21/80\n",
      "29639/29639 [==============================] - 2s 55us/step - loss: 0.8985 - acc: 0.6694 - val_loss: 1.0116 - val_acc: 0.6402\n",
      "Epoch 22/80\n",
      "29639/29639 [==============================] - 2s 57us/step - loss: 0.8862 - acc: 0.6723 - val_loss: 0.9771 - val_acc: 0.6491\n",
      "Epoch 23/80\n",
      "29639/29639 [==============================] - 2s 55us/step - loss: 0.8780 - acc: 0.6743 - val_loss: 1.0473 - val_acc: 0.6240\n",
      "Epoch 24/80\n",
      "29639/29639 [==============================] - 2s 52us/step - loss: 0.8655 - acc: 0.6783 - val_loss: 0.9939 - val_acc: 0.6383\n",
      "Epoch 25/80\n",
      "29639/29639 [==============================] - 2s 51us/step - loss: 0.8578 - acc: 0.6793 - val_loss: 0.9926 - val_acc: 0.6356\n",
      "Epoch 26/80\n",
      "29639/29639 [==============================] - 1s 50us/step - loss: 0.8178 - acc: 0.6978 - val_loss: 0.8973 - val_acc: 0.6868\n",
      "Epoch 27/80\n",
      "29639/29639 [==============================] - 2s 52us/step - loss: 0.7723 - acc: 0.7218 - val_loss: 0.9141 - val_acc: 0.6841\n",
      "Epoch 28/80\n",
      "29639/29639 [==============================] - 2s 55us/step - loss: 0.7559 - acc: 0.7270 - val_loss: 0.9039 - val_acc: 0.6892\n",
      "Epoch 29/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29639/29639 [==============================] - 2s 62us/step - loss: 0.7405 - acc: 0.7339 - val_loss: 0.8649 - val_acc: 0.7015\n",
      "Epoch 30/80\n",
      "29639/29639 [==============================] - 2s 54us/step - loss: 0.7332 - acc: 0.7363 - val_loss: 0.8804 - val_acc: 0.6974\n",
      "Epoch 31/80\n",
      "29639/29639 [==============================] - 1s 50us/step - loss: 0.7325 - acc: 0.7354 - val_loss: 0.8950 - val_acc: 0.6978\n",
      "Epoch 32/80\n",
      "29639/29639 [==============================] - 2s 55us/step - loss: 0.7225 - acc: 0.7402 - val_loss: 0.9063 - val_acc: 0.6878\n",
      "Epoch 33/80\n",
      "29639/29639 [==============================] - 2s 61us/step - loss: 0.7165 - acc: 0.7422 - val_loss: 0.8707 - val_acc: 0.6977\n",
      "Epoch 34/80\n",
      "29639/29639 [==============================] - 2s 57us/step - loss: 0.7180 - acc: 0.7423 - val_loss: 0.8686 - val_acc: 0.7037\n",
      "Inner step: 13 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 2s 81us/step - loss: 1.2066 - acc: 0.4935 - val_loss: 1.0824 - val_acc: 0.5644\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 2s 52us/step - loss: 1.0154 - acc: 0.5737 - val_loss: 1.0535 - val_acc: 0.5748\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 2s 51us/step - loss: 1.0044 - acc: 0.5759 - val_loss: 1.0940 - val_acc: 0.5478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 2s 52us/step - loss: 0.9936 - acc: 0.5805 - val_loss: 1.0635 - val_acc: 0.5663\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 2s 53us/step - loss: 0.9912 - acc: 0.5824 - val_loss: 1.0398 - val_acc: 0.5775\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 2s 52us/step - loss: 0.9880 - acc: 0.5846 - val_loss: 1.0586 - val_acc: 0.5746\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 2s 51us/step - loss: 0.9812 - acc: 0.5874 - val_loss: 1.0427 - val_acc: 0.5815\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 2s 63us/step - loss: 0.9800 - acc: 0.5891 - val_loss: 1.0580 - val_acc: 0.5728\n",
      "Epoch 9/80\n",
      "29639/29639 [==============================] - 2s 64us/step - loss: 0.9804 - acc: 0.5876 - val_loss: 1.0530 - val_acc: 0.5773\n",
      "Epoch 10/80\n",
      "29639/29639 [==============================] - 2s 67us/step - loss: 0.9751 - acc: 0.5917 - val_loss: 1.0617 - val_acc: 0.5768\n",
      "Inner step: 14 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 2s 77us/step - loss: 1.3358 - acc: 0.4126 - val_loss: 1.2677 - val_acc: 0.4738\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 2s 56us/step - loss: 1.1920 - acc: 0.4762 - val_loss: 1.2302 - val_acc: 0.4717\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 2s 62us/step - loss: 1.1722 - acc: 0.4741 - val_loss: 1.2297 - val_acc: 0.4696\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 2s 57us/step - loss: 1.1646 - acc: 0.4743 - val_loss: 1.2041 - val_acc: 0.4776\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 2s 58us/step - loss: 1.1550 - acc: 0.4798 - val_loss: 1.2295 - val_acc: 0.4720\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 2s 62us/step - loss: 1.1563 - acc: 0.4780 - val_loss: 1.1950 - val_acc: 0.4744\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 2s 55us/step - loss: 1.1468 - acc: 0.4847 - val_loss: 1.2084 - val_acc: 0.4763\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 2s 55us/step - loss: 1.1519 - acc: 0.4826 - val_loss: 1.2045 - val_acc: 0.4749\n",
      "Epoch 9/80\n",
      "29639/29639 [==============================] - 2s 58us/step - loss: 1.1432 - acc: 0.4863 - val_loss: 1.2064 - val_acc: 0.4773\n",
      "Epoch 10/80\n",
      "29639/29639 [==============================] - 2s 54us/step - loss: 1.1436 - acc: 0.4847 - val_loss: 1.2052 - val_acc: 0.4767\n",
      "Epoch 11/80\n",
      "29639/29639 [==============================] - 2s 57us/step - loss: 1.1428 - acc: 0.4848 - val_loss: 1.2173 - val_acc: 0.4744\n",
      "Inner step: 15 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 2s 74us/step - loss: 1.5070 - acc: 0.3182 - val_loss: 1.4578 - val_acc: 0.3636\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 2s 53us/step - loss: 1.4405 - acc: 0.3635 - val_loss: 1.4622 - val_acc: 0.3600\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 2s 54us/step - loss: 1.4339 - acc: 0.3673 - val_loss: 1.4543 - val_acc: 0.3635\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 2s 54us/step - loss: 1.3739 - acc: 0.3722 - val_loss: 1.3835 - val_acc: 0.3767\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 2s 54us/step - loss: 1.3329 - acc: 0.3767 - val_loss: 1.3677 - val_acc: 0.3832\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 2s 57us/step - loss: 1.3256 - acc: 0.3807 - val_loss: 1.3751 - val_acc: 0.3811\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 2s 63us/step - loss: 1.3245 - acc: 0.3835 - val_loss: 1.3678 - val_acc: 0.3782\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 2s 55us/step - loss: 1.3220 - acc: 0.3818 - val_loss: 1.3665 - val_acc: 0.3823\n",
      "Epoch 9/80\n",
      "29639/29639 [==============================] - 2s 54us/step - loss: 1.3228 - acc: 0.3839 - val_loss: 1.3594 - val_acc: 0.3873\n",
      "Epoch 10/80\n",
      "29639/29639 [==============================] - 2s 60us/step - loss: 1.3208 - acc: 0.3842 - val_loss: 1.3744 - val_acc: 0.3857\n",
      "Epoch 11/80\n",
      "29639/29639 [==============================] - 2s 55us/step - loss: 1.3210 - acc: 0.3858 - val_loss: 1.3589 - val_acc: 0.3880\n",
      "Epoch 12/80\n",
      "29639/29639 [==============================] - 2s 56us/step - loss: 1.3199 - acc: 0.3837 - val_loss: 1.3735 - val_acc: 0.3798\n",
      "Epoch 13/80\n",
      "29639/29639 [==============================] - 2s 57us/step - loss: 1.3181 - acc: 0.3839 - val_loss: 1.3616 - val_acc: 0.3881\n",
      "Epoch 14/80\n",
      "29639/29639 [==============================] - 2s 61us/step - loss: 1.3187 - acc: 0.3871 - val_loss: 1.3682 - val_acc: 0.3861\n",
      "Inner step: 16 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 2s 72us/step - loss: 1.3332 - acc: 0.4078 - val_loss: 1.2302 - val_acc: 0.4723\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 2s 54us/step - loss: 1.1823 - acc: 0.4723 - val_loss: 1.2026 - val_acc: 0.4708\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 2s 67us/step - loss: 1.1693 - acc: 0.4781 - val_loss: 1.2435 - val_acc: 0.4608\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 2s 62us/step - loss: 1.1601 - acc: 0.4796 - val_loss: 1.2135 - val_acc: 0.4762\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 2s 56us/step - loss: 1.1552 - acc: 0.4828 - val_loss: 1.2215 - val_acc: 0.4784\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 2s 57us/step - loss: 1.1524 - acc: 0.4833 - val_loss: 1.2133 - val_acc: 0.4792\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 2s 60us/step - loss: 1.1534 - acc: 0.4836 - val_loss: 1.2198 - val_acc: 0.4715\n",
      "Inner step: 17 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 2s 77us/step - loss: 1.4308 - acc: 0.3418 - val_loss: 1.3675 - val_acc: 0.3831\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 2s 57us/step - loss: 1.3350 - acc: 0.3793 - val_loss: 1.3854 - val_acc: 0.3856\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 2s 56us/step - loss: 1.3309 - acc: 0.3821 - val_loss: 1.3664 - val_acc: 0.3857\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 2s 64us/step - loss: 1.3273 - acc: 0.3832 - val_loss: 1.3618 - val_acc: 0.3851\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 2s 59us/step - loss: 1.3230 - acc: 0.3853 - val_loss: 1.3743 - val_acc: 0.3873\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 2s 63us/step - loss: 1.3239 - acc: 0.3867 - val_loss: 1.3825 - val_acc: 0.3877\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 2s 78us/step - loss: 1.3241 - acc: 0.3859 - val_loss: 1.3740 - val_acc: 0.3904\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 2s 62us/step - loss: 1.3200 - acc: 0.3859 - val_loss: 1.3749 - val_acc: 0.3847\n",
      "Epoch 9/80\n",
      "29639/29639 [==============================] - 2s 58us/step - loss: 1.3197 - acc: 0.3882 - val_loss: 1.3772 - val_acc: 0.3811\n",
      "Inner step: 18 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29639/29639 [==============================] - 2s 75us/step - loss: 1.3263 - acc: 0.4103 - val_loss: 1.2300 - val_acc: 0.4635\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 2s 57us/step - loss: 1.1832 - acc: 0.4697 - val_loss: 1.2097 - val_acc: 0.4673\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 2s 53us/step - loss: 1.1637 - acc: 0.4755 - val_loss: 1.2026 - val_acc: 0.4707\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 2s 56us/step - loss: 1.1627 - acc: 0.4777 - val_loss: 1.1964 - val_acc: 0.4789\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 2s 53us/step - loss: 1.1561 - acc: 0.4833 - val_loss: 1.1992 - val_acc: 0.4771\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 2s 52us/step - loss: 1.1473 - acc: 0.4856 - val_loss: 1.2109 - val_acc: 0.4751\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 2s 59us/step - loss: 1.1496 - acc: 0.4851 - val_loss: 1.2342 - val_acc: 0.4645\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 2s 54us/step - loss: 1.1478 - acc: 0.4887 - val_loss: 1.1965 - val_acc: 0.4772\n",
      "Epoch 9/80\n",
      "29639/29639 [==============================] - 2s 54us/step - loss: 1.1443 - acc: 0.4878 - val_loss: 1.1942 - val_acc: 0.4809\n",
      "Epoch 10/80\n",
      "29639/29639 [==============================] - 2s 54us/step - loss: 1.1441 - acc: 0.4859 - val_loss: 1.2063 - val_acc: 0.4773\n",
      "Epoch 11/80\n",
      "29639/29639 [==============================] - 2s 60us/step - loss: 1.1435 - acc: 0.4867 - val_loss: 1.2061 - val_acc: 0.4827\n",
      "Epoch 12/80\n",
      "29639/29639 [==============================] - 2s 53us/step - loss: 1.1414 - acc: 0.4864 - val_loss: 1.2117 - val_acc: 0.4807\n",
      "Epoch 13/80\n",
      "29639/29639 [==============================] - 2s 52us/step - loss: 1.1404 - acc: 0.4857 - val_loss: 1.2023 - val_acc: 0.4801\n",
      "Epoch 14/80\n",
      "29639/29639 [==============================] - 2s 54us/step - loss: 1.1376 - acc: 0.4875 - val_loss: 1.1996 - val_acc: 0.4791\n",
      "Inner step: 19 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 2s 82us/step - loss: 1.2868 - acc: 0.4377 - val_loss: 1.2305 - val_acc: 0.4700\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 2s 57us/step - loss: 1.1624 - acc: 0.4890 - val_loss: 1.2099 - val_acc: 0.4883\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 2s 57us/step - loss: 1.1511 - acc: 0.4994 - val_loss: 1.1903 - val_acc: 0.5041\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 2s 60us/step - loss: 1.1444 - acc: 0.5041 - val_loss: 1.1849 - val_acc: 0.5137\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 2s 60us/step - loss: 1.1419 - acc: 0.5044 - val_loss: 1.1932 - val_acc: 0.4834\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 2s 55us/step - loss: 1.1403 - acc: 0.5038 - val_loss: 1.2072 - val_acc: 0.5176\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 2s 53us/step - loss: 1.1394 - acc: 0.5046 - val_loss: 1.1901 - val_acc: 0.5043\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 2s 65us/step - loss: 1.1364 - acc: 0.5075 - val_loss: 1.1934 - val_acc: 0.5185\n",
      "Epoch 9/80\n",
      "29639/29639 [==============================] - 2s 62us/step - loss: 1.1328 - acc: 0.5108 - val_loss: 1.2519 - val_acc: 0.5013\n",
      "Inner step: 20 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 2s 77us/step - loss: 1.4934 - acc: 0.3238 - val_loss: 1.4558 - val_acc: 0.3527\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 2s 72us/step - loss: 1.3746 - acc: 0.3644 - val_loss: 1.3744 - val_acc: 0.3758\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 2s 62us/step - loss: 1.3342 - acc: 0.3760 - val_loss: 1.3686 - val_acc: 0.3830\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 2s 64us/step - loss: 1.3291 - acc: 0.3765 - val_loss: 1.3671 - val_acc: 0.3762\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 2s 56us/step - loss: 1.3234 - acc: 0.3796 - val_loss: 1.3704 - val_acc: 0.3764\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 2s 62us/step - loss: 1.3230 - acc: 0.3821 - val_loss: 1.3691 - val_acc: 0.3816\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 2s 62us/step - loss: 1.3215 - acc: 0.3830 - val_loss: 1.3606 - val_acc: 0.3860\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 2s 68us/step - loss: 1.3219 - acc: 0.3831 - val_loss: 1.3653 - val_acc: 0.3788\n",
      "Epoch 9/80\n",
      "29639/29639 [==============================] - 2s 56us/step - loss: 1.3211 - acc: 0.3824 - val_loss: 1.3642 - val_acc: 0.3853\n",
      "Epoch 10/80\n",
      "29639/29639 [==============================] - 2s 56us/step - loss: 1.3177 - acc: 0.3857 - val_loss: 1.3707 - val_acc: 0.3792\n",
      "Epoch 11/80\n",
      "29639/29639 [==============================] - 2s 65us/step - loss: 1.3180 - acc: 0.3851 - val_loss: 1.3700 - val_acc: 0.3791\n",
      "Epoch 12/80\n",
      "29639/29639 [==============================] - 2s 60us/step - loss: 1.3188 - acc: 0.3853 - val_loss: 1.3610 - val_acc: 0.3802\n",
      "Inner step: 21 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 2s 78us/step - loss: 1.4163 - acc: 0.3417 - val_loss: 1.3633 - val_acc: 0.3836\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 2s 54us/step - loss: 1.3351 - acc: 0.3766 - val_loss: 1.3673 - val_acc: 0.3841\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 2s 61us/step - loss: 1.3295 - acc: 0.3793 - val_loss: 1.3892 - val_acc: 0.3816\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 2s 74us/step - loss: 1.3293 - acc: 0.3830 - val_loss: 1.3667 - val_acc: 0.3860\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 2s 62us/step - loss: 1.3256 - acc: 0.3828 - val_loss: 1.3604 - val_acc: 0.3847\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 2s 64us/step - loss: 1.3235 - acc: 0.3807 - val_loss: 1.3750 - val_acc: 0.3857\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 2s 54us/step - loss: 1.3219 - acc: 0.3861 - val_loss: 1.3785 - val_acc: 0.3822\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 2s 57us/step - loss: 1.3202 - acc: 0.3865 - val_loss: 1.3824 - val_acc: 0.3847\n",
      "Epoch 9/80\n",
      "29639/29639 [==============================] - 2s 57us/step - loss: 1.3200 - acc: 0.3850 - val_loss: 1.3637 - val_acc: 0.3895\n",
      "Epoch 10/80\n",
      "29639/29639 [==============================] - 2s 52us/step - loss: 1.3202 - acc: 0.3878 - val_loss: 1.3772 - val_acc: 0.3876\n",
      "Inner step: 22 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 2s 76us/step - loss: 1.4915 - acc: 0.3177 - val_loss: 1.4579 - val_acc: 0.3563\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 2s 56us/step - loss: 1.4375 - acc: 0.3600 - val_loss: 1.4547 - val_acc: 0.3595\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 2s 55us/step - loss: 1.4320 - acc: 0.3642 - val_loss: 1.4583 - val_acc: 0.3640\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 2s 56us/step - loss: 1.4293 - acc: 0.3662 - val_loss: 1.4479 - val_acc: 0.3668\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 2s 55us/step - loss: 1.4262 - acc: 0.3676 - val_loss: 1.4486 - val_acc: 0.3680\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 2s 56us/step - loss: 1.3761 - acc: 0.3742 - val_loss: 1.3761 - val_acc: 0.3813\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 2s 55us/step - loss: 1.3309 - acc: 0.3781 - val_loss: 1.3689 - val_acc: 0.3799\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 2s 55us/step - loss: 1.3276 - acc: 0.3812 - val_loss: 1.3665 - val_acc: 0.3823\n",
      "Epoch 9/80\n",
      "29639/29639 [==============================] - 2s 55us/step - loss: 1.3231 - acc: 0.3833 - val_loss: 1.3711 - val_acc: 0.3884\n",
      "Epoch 10/80\n",
      "29639/29639 [==============================] - 2s 56us/step - loss: 1.3216 - acc: 0.3823 - val_loss: 1.3699 - val_acc: 0.3890\n",
      "Epoch 11/80\n",
      "29639/29639 [==============================] - 2s 54us/step - loss: 1.3218 - acc: 0.3843 - val_loss: 1.3590 - val_acc: 0.3770\n",
      "Epoch 12/80\n",
      "29639/29639 [==============================] - 2s 56us/step - loss: 1.3197 - acc: 0.3824 - val_loss: 1.3612 - val_acc: 0.3788\n",
      "Epoch 13/80\n",
      "29639/29639 [==============================] - 2s 56us/step - loss: 1.3205 - acc: 0.3857 - val_loss: 1.3662 - val_acc: 0.3860\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/80\n",
      "29639/29639 [==============================] - 2s 58us/step - loss: 1.3188 - acc: 0.3881 - val_loss: 1.3734 - val_acc: 0.3857\n",
      "Epoch 15/80\n",
      "29639/29639 [==============================] - 2s 78us/step - loss: 1.3175 - acc: 0.3834 - val_loss: 1.3651 - val_acc: 0.3878\n",
      "Epoch 16/80\n",
      "29639/29639 [==============================] - 2s 56us/step - loss: 1.1900 - acc: 0.4764 - val_loss: 1.2170 - val_acc: 0.4908\n",
      "Epoch 17/80\n",
      "29639/29639 [==============================] - 2s 65us/step - loss: 1.1457 - acc: 0.4923 - val_loss: 1.2107 - val_acc: 0.4949\n",
      "Epoch 18/80\n",
      "29639/29639 [==============================] - 2s 54us/step - loss: 1.1273 - acc: 0.5113 - val_loss: 1.1737 - val_acc: 0.5182\n",
      "Epoch 19/80\n",
      "29639/29639 [==============================] - 2s 61us/step - loss: 1.0968 - acc: 0.5288 - val_loss: 1.1813 - val_acc: 0.5119\n",
      "Epoch 20/80\n",
      "29639/29639 [==============================] - 2s 66us/step - loss: 1.0828 - acc: 0.5378 - val_loss: 1.1425 - val_acc: 0.5293\n",
      "Epoch 21/80\n",
      "29639/29639 [==============================] - 2s 60us/step - loss: 1.0724 - acc: 0.5440 - val_loss: 1.1592 - val_acc: 0.5251\n",
      "Epoch 22/80\n",
      "29639/29639 [==============================] - 2s 52us/step - loss: 1.0671 - acc: 0.5449 - val_loss: 1.1401 - val_acc: 0.5288\n",
      "Epoch 23/80\n",
      "29639/29639 [==============================] - 2s 65us/step - loss: 1.0414 - acc: 0.5677 - val_loss: 1.0593 - val_acc: 0.6005\n",
      "Epoch 24/80\n",
      "29639/29639 [==============================] - 2s 55us/step - loss: 0.9286 - acc: 0.6395 - val_loss: 0.9953 - val_acc: 0.6272\n",
      "Epoch 25/80\n",
      "29639/29639 [==============================] - 2s 58us/step - loss: 0.8761 - acc: 0.6608 - val_loss: 1.0002 - val_acc: 0.6218\n",
      "Epoch 26/80\n",
      "29639/29639 [==============================] - 2s 54us/step - loss: 0.8520 - acc: 0.6709 - val_loss: 0.9764 - val_acc: 0.6325\n",
      "Epoch 27/80\n",
      "29639/29639 [==============================] - 2s 72us/step - loss: 0.8402 - acc: 0.6775 - val_loss: 0.9724 - val_acc: 0.6417\n",
      "Epoch 28/80\n",
      "29639/29639 [==============================] - 2s 56us/step - loss: 0.8290 - acc: 0.6815 - val_loss: 0.9509 - val_acc: 0.6492\n",
      "Epoch 29/80\n",
      "29639/29639 [==============================] - 2s 61us/step - loss: 0.8290 - acc: 0.6808 - val_loss: 0.9465 - val_acc: 0.6520\n",
      "Epoch 30/80\n",
      "29639/29639 [==============================] - 2s 56us/step - loss: 0.8264 - acc: 0.6827 - val_loss: 0.9678 - val_acc: 0.6468\n",
      "Epoch 31/80\n",
      "29639/29639 [==============================] - 2s 58us/step - loss: 0.8244 - acc: 0.6841 - val_loss: 0.9429 - val_acc: 0.6482\n",
      "Epoch 32/80\n",
      "29639/29639 [==============================] - 2s 54us/step - loss: 0.8154 - acc: 0.6927 - val_loss: 0.9080 - val_acc: 0.6878\n",
      "Epoch 33/80\n",
      "29639/29639 [==============================] - 2s 55us/step - loss: 0.7741 - acc: 0.7228 - val_loss: 0.8974 - val_acc: 0.6949\n",
      "Epoch 34/80\n",
      "29639/29639 [==============================] - 2s 58us/step - loss: 0.7499 - acc: 0.7341 - val_loss: 0.9043 - val_acc: 0.6949\n",
      "Epoch 35/80\n",
      "29639/29639 [==============================] - 2s 59us/step - loss: 0.7348 - acc: 0.7386 - val_loss: 0.8617 - val_acc: 0.7071\n",
      "Epoch 36/80\n",
      "29639/29639 [==============================] - 2s 60us/step - loss: 0.7317 - acc: 0.7410 - val_loss: 0.8566 - val_acc: 0.7114\n",
      "Epoch 37/80\n",
      "29639/29639 [==============================] - 2s 57us/step - loss: 0.7239 - acc: 0.7451 - val_loss: 0.8648 - val_acc: 0.7081\n",
      "Epoch 38/80\n",
      "29639/29639 [==============================] - 2s 54us/step - loss: 0.7219 - acc: 0.7459 - val_loss: 0.8937 - val_acc: 0.6974\n",
      "Epoch 39/80\n",
      "29639/29639 [==============================] - 2s 62us/step - loss: 0.7134 - acc: 0.7497 - val_loss: 0.9065 - val_acc: 0.7038\n",
      "Epoch 40/80\n",
      "29639/29639 [==============================] - 2s 59us/step - loss: 0.7140 - acc: 0.7494 - val_loss: 0.8501 - val_acc: 0.7222\n",
      "Epoch 41/80\n",
      "29639/29639 [==============================] - 2s 66us/step - loss: 0.7068 - acc: 0.7525 - val_loss: 0.8600 - val_acc: 0.7163\n",
      "Epoch 42/80\n",
      "29639/29639 [==============================] - 2s 60us/step - loss: 0.7057 - acc: 0.7548 - val_loss: 0.8617 - val_acc: 0.7127\n",
      "Epoch 43/80\n",
      "29639/29639 [==============================] - 2s 60us/step - loss: 0.7068 - acc: 0.7530 - val_loss: 0.8502 - val_acc: 0.7274\n",
      "Epoch 44/80\n",
      "29639/29639 [==============================] - 2s 63us/step - loss: 0.6950 - acc: 0.7604 - val_loss: 0.8421 - val_acc: 0.7251\n",
      "Epoch 45/80\n",
      "29639/29639 [==============================] - 2s 57us/step - loss: 0.7017 - acc: 0.7566 - val_loss: 0.8719 - val_acc: 0.7206\n",
      "Epoch 46/80\n",
      "29639/29639 [==============================] - 2s 55us/step - loss: 0.6949 - acc: 0.7599 - val_loss: 0.8720 - val_acc: 0.7168\n",
      "Epoch 47/80\n",
      "29639/29639 [==============================] - 2s 61us/step - loss: 0.6938 - acc: 0.7599 - val_loss: 0.8668 - val_acc: 0.7173\n",
      "Epoch 48/80\n",
      "29639/29639 [==============================] - 2s 68us/step - loss: 0.6869 - acc: 0.7639 - val_loss: 0.8815 - val_acc: 0.7192\n",
      "Epoch 49/80\n",
      "29639/29639 [==============================] - 2s 57us/step - loss: 0.6867 - acc: 0.7623 - val_loss: 0.8580 - val_acc: 0.7242\n",
      "Inner step: 23 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 2s 80us/step - loss: 1.3517 - acc: 0.4074 - val_loss: 1.2706 - val_acc: 0.4603\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 2s 54us/step - loss: 1.1902 - acc: 0.4760 - val_loss: 1.2153 - val_acc: 0.4789\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 2s 52us/step - loss: 1.1674 - acc: 0.4796 - val_loss: 1.2799 - val_acc: 0.4603\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 2s 52us/step - loss: 1.1591 - acc: 0.4820 - val_loss: 1.2308 - val_acc: 0.4722\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 2s 52us/step - loss: 1.1550 - acc: 0.4852 - val_loss: 1.2353 - val_acc: 0.4683\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 2s 52us/step - loss: 1.1538 - acc: 0.4868 - val_loss: 1.2101 - val_acc: 0.4789\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 2s 52us/step - loss: 1.1525 - acc: 0.4863 - val_loss: 1.1980 - val_acc: 0.4787\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 2s 53us/step - loss: 1.1488 - acc: 0.4856 - val_loss: 1.2130 - val_acc: 0.4757\n",
      "Epoch 9/80\n",
      "29639/29639 [==============================] - 2s 52us/step - loss: 1.1477 - acc: 0.4878 - val_loss: 1.1909 - val_acc: 0.4779\n",
      "Epoch 10/80\n",
      "29639/29639 [==============================] - 2s 57us/step - loss: 1.1468 - acc: 0.4887 - val_loss: 1.2234 - val_acc: 0.4674\n",
      "Epoch 11/80\n",
      "29639/29639 [==============================] - 2s 57us/step - loss: 1.1433 - acc: 0.4879 - val_loss: 1.1915 - val_acc: 0.4787\n",
      "Epoch 12/80\n",
      "29639/29639 [==============================] - 2s 53us/step - loss: 1.1397 - acc: 0.4903 - val_loss: 1.2189 - val_acc: 0.4771\n",
      "Epoch 13/80\n",
      "29639/29639 [==============================] - 2s 55us/step - loss: 1.1409 - acc: 0.4878 - val_loss: 1.2495 - val_acc: 0.4727\n",
      "Epoch 14/80\n",
      "29639/29639 [==============================] - 2s 56us/step - loss: 1.1391 - acc: 0.4891 - val_loss: 1.2387 - val_acc: 0.4665\n",
      "Inner step: 24 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 2s 83us/step - loss: 1.3008 - acc: 0.4287 - val_loss: 1.2139 - val_acc: 0.4931\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 2s 57us/step - loss: 1.1663 - acc: 0.4905 - val_loss: 1.1851 - val_acc: 0.5099\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 2s 53us/step - loss: 1.1549 - acc: 0.4949 - val_loss: 1.1941 - val_acc: 0.5044\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 2s 53us/step - loss: 1.1474 - acc: 0.4995 - val_loss: 1.1835 - val_acc: 0.4976\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 2s 54us/step - loss: 1.1461 - acc: 0.4998 - val_loss: 1.2032 - val_acc: 0.4928\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 2s 53us/step - loss: 1.1416 - acc: 0.5028 - val_loss: 1.1925 - val_acc: 0.5048\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 2s 55us/step - loss: 1.1358 - acc: 0.5069 - val_loss: 1.2092 - val_acc: 0.5061\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 2s 54us/step - loss: 1.1374 - acc: 0.5072 - val_loss: 1.1760 - val_acc: 0.5010\n",
      "Epoch 9/80\n",
      "29639/29639 [==============================] - 2s 53us/step - loss: 1.1366 - acc: 0.5030 - val_loss: 1.2387 - val_acc: 0.5051\n",
      "Epoch 10/80\n",
      "29639/29639 [==============================] - 2s 53us/step - loss: 1.1293 - acc: 0.5073 - val_loss: 1.1940 - val_acc: 0.5097\n",
      "Epoch 11/80\n",
      "29639/29639 [==============================] - 2s 56us/step - loss: 1.1327 - acc: 0.5065 - val_loss: 1.1813 - val_acc: 0.5084\n",
      "Epoch 12/80\n",
      "29639/29639 [==============================] - 2s 54us/step - loss: 1.1347 - acc: 0.5051 - val_loss: 1.1961 - val_acc: 0.5040\n",
      "Epoch 13/80\n",
      "29639/29639 [==============================] - 2s 54us/step - loss: 1.1295 - acc: 0.5093 - val_loss: 1.1797 - val_acc: 0.5119\n",
      "Inner step: 25 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 2s 82us/step - loss: 1.4171 - acc: 0.3531 - val_loss: 1.3717 - val_acc: 0.3779\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 2s 55us/step - loss: 1.2336 - acc: 0.4508 - val_loss: 1.2015 - val_acc: 0.4834\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 2s 56us/step - loss: 1.1556 - acc: 0.4861 - val_loss: 1.1923 - val_acc: 0.4864\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 2s 56us/step - loss: 1.1495 - acc: 0.4893 - val_loss: 1.2155 - val_acc: 0.4823\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 2s 55us/step - loss: 1.1445 - acc: 0.4891 - val_loss: 1.1872 - val_acc: 0.4888\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 2s 55us/step - loss: 1.1413 - acc: 0.4906 - val_loss: 1.2020 - val_acc: 0.4926\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 2s 55us/step - loss: 1.1373 - acc: 0.4940 - val_loss: 1.1865 - val_acc: 0.4900\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 2s 55us/step - loss: 1.1368 - acc: 0.4943 - val_loss: 1.1948 - val_acc: 0.5012\n",
      "Epoch 9/80\n",
      "29639/29639 [==============================] - 2s 56us/step - loss: 1.1341 - acc: 0.4948 - val_loss: 1.2205 - val_acc: 0.4961\n",
      "Epoch 10/80\n",
      "29639/29639 [==============================] - 2s 57us/step - loss: 1.1313 - acc: 0.4989 - val_loss: 1.1837 - val_acc: 0.4880\n",
      "Epoch 11/80\n",
      "29639/29639 [==============================] - 2s 55us/step - loss: 1.1305 - acc: 0.4972 - val_loss: 1.1882 - val_acc: 0.4889\n",
      "Epoch 12/80\n",
      "29639/29639 [==============================] - 2s 55us/step - loss: 1.1309 - acc: 0.5004 - val_loss: 1.2391 - val_acc: 0.4884\n",
      "Epoch 13/80\n",
      "29639/29639 [==============================] - 2s 56us/step - loss: 1.0947 - acc: 0.5339 - val_loss: 1.1396 - val_acc: 0.5210\n",
      "Epoch 14/80\n",
      "29639/29639 [==============================] - 2s 55us/step - loss: 1.0646 - acc: 0.5460 - val_loss: 1.2057 - val_acc: 0.5137\n",
      "Epoch 15/80\n",
      "29639/29639 [==============================] - 2s 55us/step - loss: 1.0488 - acc: 0.5511 - val_loss: 1.1277 - val_acc: 0.5291\n",
      "Epoch 16/80\n",
      "29639/29639 [==============================] - 2s 56us/step - loss: 1.0468 - acc: 0.5507 - val_loss: 1.1216 - val_acc: 0.5503\n",
      "Epoch 17/80\n",
      "29639/29639 [==============================] - 2s 55us/step - loss: 1.0417 - acc: 0.5537 - val_loss: 1.1281 - val_acc: 0.5281\n",
      "Epoch 18/80\n",
      "29639/29639 [==============================] - 2s 56us/step - loss: 1.0363 - acc: 0.5550 - val_loss: 1.1381 - val_acc: 0.5356\n",
      "Epoch 19/80\n",
      "29639/29639 [==============================] - 2s 61us/step - loss: 1.0336 - acc: 0.5609 - val_loss: 1.1671 - val_acc: 0.5248\n",
      "Epoch 20/80\n",
      "29639/29639 [==============================] - 2s 55us/step - loss: 1.0258 - acc: 0.5651 - val_loss: 1.1282 - val_acc: 0.5343\n",
      "Epoch 21/80\n",
      "29639/29639 [==============================] - 2s 55us/step - loss: 1.0164 - acc: 0.5734 - val_loss: 1.1100 - val_acc: 0.5546\n",
      "Epoch 22/80\n",
      "29639/29639 [==============================] - 2s 56us/step - loss: 1.0053 - acc: 0.5784 - val_loss: 1.1039 - val_acc: 0.5550\n",
      "Epoch 23/80\n",
      "29639/29639 [==============================] - 2s 60us/step - loss: 0.9947 - acc: 0.5854 - val_loss: 1.1069 - val_acc: 0.5564\n",
      "Epoch 24/80\n",
      "29639/29639 [==============================] - 2s 59us/step - loss: 0.9835 - acc: 0.5911 - val_loss: 1.1085 - val_acc: 0.5596\n",
      "Epoch 25/80\n",
      "29639/29639 [==============================] - 2s 60us/step - loss: 0.9844 - acc: 0.5902 - val_loss: 1.0973 - val_acc: 0.5732\n",
      "Epoch 26/80\n",
      "29639/29639 [==============================] - 2s 60us/step - loss: 0.9801 - acc: 0.5915 - val_loss: 1.1247 - val_acc: 0.5555\n",
      "Epoch 27/80\n",
      "29639/29639 [==============================] - 2s 59us/step - loss: 0.9723 - acc: 0.6026 - val_loss: 1.1495 - val_acc: 0.5838\n",
      "Epoch 28/80\n",
      "29639/29639 [==============================] - 2s 59us/step - loss: 0.9424 - acc: 0.6355 - val_loss: 1.0947 - val_acc: 0.6232\n",
      "Epoch 29/80\n",
      "29639/29639 [==============================] - 2s 60us/step - loss: 0.8788 - acc: 0.6707 - val_loss: 1.0086 - val_acc: 0.6344\n",
      "Epoch 30/80\n",
      "29639/29639 [==============================] - 2s 60us/step - loss: 0.8497 - acc: 0.6797 - val_loss: 1.0043 - val_acc: 0.6301\n",
      "Epoch 31/80\n",
      "29639/29639 [==============================] - 2s 59us/step - loss: 0.8230 - acc: 0.6978 - val_loss: 0.9128 - val_acc: 0.6880\n",
      "Epoch 32/80\n",
      "29639/29639 [==============================] - 2s 59us/step - loss: 0.7573 - acc: 0.7370 - val_loss: 0.9068 - val_acc: 0.7030\n",
      "Epoch 33/80\n",
      "29639/29639 [==============================] - 2s 59us/step - loss: 0.7436 - acc: 0.7454 - val_loss: 0.9181 - val_acc: 0.6978\n",
      "Epoch 34/80\n",
      "29639/29639 [==============================] - 2s 59us/step - loss: 0.7332 - acc: 0.7498 - val_loss: 0.8705 - val_acc: 0.7122\n",
      "Epoch 35/80\n",
      "29639/29639 [==============================] - 2s 59us/step - loss: 0.7279 - acc: 0.7525 - val_loss: 0.8781 - val_acc: 0.7075\n",
      "Epoch 36/80\n",
      "29639/29639 [==============================] - 2s 59us/step - loss: 0.7179 - acc: 0.7556 - val_loss: 0.8598 - val_acc: 0.7129\n",
      "Epoch 37/80\n",
      "29639/29639 [==============================] - 2s 59us/step - loss: 0.7109 - acc: 0.7559 - val_loss: 0.8538 - val_acc: 0.7140\n",
      "Epoch 38/80\n",
      "29639/29639 [==============================] - 2s 59us/step - loss: 0.7113 - acc: 0.7555 - val_loss: 0.8818 - val_acc: 0.7001\n",
      "Epoch 39/80\n",
      "29639/29639 [==============================] - 2s 59us/step - loss: 0.6986 - acc: 0.7596 - val_loss: 0.8566 - val_acc: 0.7157\n",
      "Epoch 40/80\n",
      "29639/29639 [==============================] - 2s 59us/step - loss: 0.7005 - acc: 0.7570 - val_loss: 0.8962 - val_acc: 0.6935\n",
      "Epoch 41/80\n",
      "29639/29639 [==============================] - 2s 59us/step - loss: 0.6986 - acc: 0.7561 - val_loss: 0.8684 - val_acc: 0.6955\n",
      "Epoch 42/80\n",
      "29639/29639 [==============================] - 2s 59us/step - loss: 0.6924 - acc: 0.7569 - val_loss: 0.8496 - val_acc: 0.7127\n",
      "Epoch 43/80\n",
      "29639/29639 [==============================] - 2s 59us/step - loss: 0.6879 - acc: 0.7626 - val_loss: 0.8565 - val_acc: 0.7071\n",
      "Epoch 44/80\n",
      "29639/29639 [==============================] - 2s 59us/step - loss: 0.6852 - acc: 0.7617 - val_loss: 0.8335 - val_acc: 0.7193\n",
      "Epoch 45/80\n",
      "29639/29639 [==============================] - 2s 59us/step - loss: 0.6733 - acc: 0.7668 - val_loss: 0.8282 - val_acc: 0.7217\n",
      "Epoch 46/80\n",
      "29639/29639 [==============================] - 2s 60us/step - loss: 0.6679 - acc: 0.7708 - val_loss: 0.8391 - val_acc: 0.7408\n",
      "Epoch 47/80\n",
      "29639/29639 [==============================] - 2s 59us/step - loss: 0.6520 - acc: 0.7814 - val_loss: 0.8100 - val_acc: 0.7491\n",
      "Epoch 48/80\n",
      "29639/29639 [==============================] - 2s 60us/step - loss: 0.6299 - acc: 0.7909 - val_loss: 0.7935 - val_acc: 0.7489\n",
      "Epoch 49/80\n",
      "29639/29639 [==============================] - 2s 60us/step - loss: 0.6183 - acc: 0.7971 - val_loss: 0.7779 - val_acc: 0.7606\n",
      "Epoch 50/80\n",
      "29639/29639 [==============================] - 2s 60us/step - loss: 0.6072 - acc: 0.8010 - val_loss: 0.8245 - val_acc: 0.7380\n",
      "Epoch 51/80\n",
      "29639/29639 [==============================] - 2s 59us/step - loss: 0.6027 - acc: 0.8025 - val_loss: 0.7996 - val_acc: 0.7528\n",
      "Epoch 52/80\n",
      "29639/29639 [==============================] - 2s 59us/step - loss: 0.5947 - acc: 0.8058 - val_loss: 0.7775 - val_acc: 0.7627\n",
      "Epoch 53/80\n",
      "29639/29639 [==============================] - 2s 60us/step - loss: 0.5943 - acc: 0.8057 - val_loss: 0.7831 - val_acc: 0.7636\n",
      "Epoch 54/80\n",
      "29639/29639 [==============================] - 2s 59us/step - loss: 0.5910 - acc: 0.8057 - val_loss: 0.7969 - val_acc: 0.7525\n",
      "Inner step: 26 / 40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 2s 84us/step - loss: 1.4115 - acc: 0.3447 - val_loss: 1.3777 - val_acc: 0.3607\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 2s 56us/step - loss: 1.3395 - acc: 0.3787 - val_loss: 1.3639 - val_acc: 0.3775\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 2s 57us/step - loss: 1.3312 - acc: 0.3789 - val_loss: 1.3662 - val_acc: 0.3828\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 2s 56us/step - loss: 1.3288 - acc: 0.3812 - val_loss: 1.3635 - val_acc: 0.3846\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 2s 65us/step - loss: 1.3254 - acc: 0.3835 - val_loss: 1.3752 - val_acc: 0.3850\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 2s 60us/step - loss: 1.3236 - acc: 0.3848 - val_loss: 1.3851 - val_acc: 0.3862\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 2s 55us/step - loss: 1.3229 - acc: 0.3850 - val_loss: 1.3637 - val_acc: 0.3830\n",
      "Inner step: 27 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 4s 122us/step - loss: 1.4443 - acc: 0.3491 - val_loss: 1.3355 - val_acc: 0.4004\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 2s 68us/step - loss: 1.2323 - acc: 0.4555 - val_loss: 1.2068 - val_acc: 0.4694\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 2s 60us/step - loss: 1.1761 - acc: 0.4742 - val_loss: 1.2318 - val_acc: 0.4685\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 2s 57us/step - loss: 1.1643 - acc: 0.4794 - val_loss: 1.1933 - val_acc: 0.4770\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 2s 56us/step - loss: 1.1615 - acc: 0.4796 - val_loss: 1.2566 - val_acc: 0.4669\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 2s 60us/step - loss: 1.1560 - acc: 0.4830 - val_loss: 1.2235 - val_acc: 0.4751\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 2s 56us/step - loss: 1.1468 - acc: 0.4856 - val_loss: 1.2177 - val_acc: 0.4776\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 2s 65us/step - loss: 1.1526 - acc: 0.4832 - val_loss: 1.2061 - val_acc: 0.4765\n",
      "Epoch 9/80\n",
      "29639/29639 [==============================] - 2s 59us/step - loss: 1.1493 - acc: 0.4849 - val_loss: 1.1961 - val_acc: 0.4810\n",
      "Inner step: 28 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 2s 82us/step - loss: 1.4154 - acc: 0.3488 - val_loss: 1.3723 - val_acc: 0.3815\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 2s 57us/step - loss: 1.3366 - acc: 0.3768 - val_loss: 1.3726 - val_acc: 0.3825\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 2s 57us/step - loss: 1.3317 - acc: 0.3812 - val_loss: 1.3723 - val_acc: 0.3863\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 2s 56us/step - loss: 1.3268 - acc: 0.3818 - val_loss: 1.3663 - val_acc: 0.3894\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 2s 58us/step - loss: 1.3252 - acc: 0.3834 - val_loss: 1.3653 - val_acc: 0.3768\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 2s 53us/step - loss: 1.3256 - acc: 0.3806 - val_loss: 1.3669 - val_acc: 0.3796\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 2s 57us/step - loss: 1.3227 - acc: 0.3846 - val_loss: 1.3626 - val_acc: 0.3840\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 2s 54us/step - loss: 1.3226 - acc: 0.3842 - val_loss: 1.3580 - val_acc: 0.3866\n",
      "Epoch 9/80\n",
      "29639/29639 [==============================] - 2s 55us/step - loss: 1.3221 - acc: 0.3856 - val_loss: 1.3680 - val_acc: 0.3811\n",
      "Epoch 10/80\n",
      "29639/29639 [==============================] - 2s 56us/step - loss: 1.3193 - acc: 0.3873 - val_loss: 1.3737 - val_acc: 0.3896\n",
      "Epoch 11/80\n",
      "29639/29639 [==============================] - 2s 57us/step - loss: 1.3177 - acc: 0.3863 - val_loss: 1.3663 - val_acc: 0.3791\n",
      "Epoch 12/80\n",
      "29639/29639 [==============================] - 2s 55us/step - loss: 1.3185 - acc: 0.3835 - val_loss: 1.3561 - val_acc: 0.3874\n",
      "Epoch 13/80\n",
      "29639/29639 [==============================] - 2s 53us/step - loss: 1.3184 - acc: 0.3870 - val_loss: 1.3763 - val_acc: 0.3852\n",
      "Epoch 14/80\n",
      "29639/29639 [==============================] - 2s 55us/step - loss: 1.3026 - acc: 0.4025 - val_loss: 1.3441 - val_acc: 0.4147\n",
      "Epoch 15/80\n",
      "29639/29639 [==============================] - 2s 54us/step - loss: 1.2592 - acc: 0.4313 - val_loss: 1.3215 - val_acc: 0.4226\n",
      "Epoch 16/80\n",
      "29639/29639 [==============================] - 2s 57us/step - loss: 1.2456 - acc: 0.4388 - val_loss: 1.3366 - val_acc: 0.4156\n",
      "Epoch 17/80\n",
      "29639/29639 [==============================] - 2s 61us/step - loss: 1.2360 - acc: 0.4423 - val_loss: 1.3033 - val_acc: 0.4236\n",
      "Epoch 18/80\n",
      "29639/29639 [==============================] - 2s 59us/step - loss: 1.1923 - acc: 0.4739 - val_loss: 1.1830 - val_acc: 0.5259\n",
      "Epoch 19/80\n",
      "29639/29639 [==============================] - 2s 53us/step - loss: 1.0704 - acc: 0.5446 - val_loss: 1.1268 - val_acc: 0.5380\n",
      "Epoch 20/80\n",
      "29639/29639 [==============================] - 2s 53us/step - loss: 1.0473 - acc: 0.5543 - val_loss: 1.1353 - val_acc: 0.5424\n",
      "Epoch 21/80\n",
      "29639/29639 [==============================] - 2s 56us/step - loss: 1.0289 - acc: 0.5695 - val_loss: 1.0650 - val_acc: 0.5990\n",
      "Epoch 22/80\n",
      "29639/29639 [==============================] - 2s 59us/step - loss: 0.9476 - acc: 0.6303 - val_loss: 0.9985 - val_acc: 0.6348\n",
      "Epoch 23/80\n",
      "29639/29639 [==============================] - 2s 53us/step - loss: 0.8987 - acc: 0.6572 - val_loss: 0.9939 - val_acc: 0.6248\n",
      "Epoch 24/80\n",
      "29639/29639 [==============================] - 2s 53us/step - loss: 0.8726 - acc: 0.6639 - val_loss: 1.0158 - val_acc: 0.6157\n",
      "Epoch 25/80\n",
      "29639/29639 [==============================] - 2s 57us/step - loss: 0.8585 - acc: 0.6652 - val_loss: 1.0009 - val_acc: 0.6212\n",
      "Epoch 26/80\n",
      "29639/29639 [==============================] - 2s 57us/step - loss: 0.8480 - acc: 0.6711 - val_loss: 0.9635 - val_acc: 0.6427\n",
      "Epoch 27/80\n",
      "29639/29639 [==============================] - 2s 57us/step - loss: 0.8405 - acc: 0.6761 - val_loss: 0.9571 - val_acc: 0.6470\n",
      "Epoch 28/80\n",
      "29639/29639 [==============================] - 2s 58us/step - loss: 0.8318 - acc: 0.6784 - val_loss: 0.9738 - val_acc: 0.6378\n",
      "Epoch 29/80\n",
      "29639/29639 [==============================] - 2s 57us/step - loss: 0.8365 - acc: 0.6776 - val_loss: 0.9705 - val_acc: 0.6434\n",
      "Epoch 30/80\n",
      "29639/29639 [==============================] - 2s 56us/step - loss: 0.8244 - acc: 0.6816 - val_loss: 0.9903 - val_acc: 0.6295\n",
      "Epoch 31/80\n",
      "29639/29639 [==============================] - 2s 56us/step - loss: 0.8216 - acc: 0.6821 - val_loss: 0.9521 - val_acc: 0.6492\n",
      "Epoch 32/80\n",
      "29639/29639 [==============================] - 2s 56us/step - loss: 0.8233 - acc: 0.6827 - val_loss: 0.9546 - val_acc: 0.6445\n",
      "Epoch 33/80\n",
      "29639/29639 [==============================] - 2s 56us/step - loss: 0.8165 - acc: 0.6851 - val_loss: 0.9520 - val_acc: 0.6519\n",
      "Epoch 34/80\n",
      "29639/29639 [==============================] - 2s 56us/step - loss: 0.8183 - acc: 0.6846 - val_loss: 0.9798 - val_acc: 0.6463\n",
      "Epoch 35/80\n",
      "29639/29639 [==============================] - 2s 58us/step - loss: 0.8125 - acc: 0.6866 - val_loss: 0.9762 - val_acc: 0.6400\n",
      "Epoch 36/80\n",
      "29639/29639 [==============================] - 2s 56us/step - loss: 0.8130 - acc: 0.6874 - val_loss: 0.9631 - val_acc: 0.6471\n",
      "Inner step: 29 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 2s 84us/step - loss: 1.4337 - acc: 0.3401 - val_loss: 1.3983 - val_acc: 0.3752\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 2s 57us/step - loss: 1.3403 - acc: 0.3760 - val_loss: 1.3670 - val_acc: 0.3825\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 2s 58us/step - loss: 1.3309 - acc: 0.3804 - val_loss: 1.3753 - val_acc: 0.3814\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 2s 57us/step - loss: 1.3270 - acc: 0.3799 - val_loss: 1.3757 - val_acc: 0.3842\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 2s 57us/step - loss: 1.3245 - acc: 0.3828 - val_loss: 1.3652 - val_acc: 0.3876\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 2s 57us/step - loss: 1.3255 - acc: 0.3818 - val_loss: 1.3631 - val_acc: 0.3784\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 2s 57us/step - loss: 1.3227 - acc: 0.3865 - val_loss: 1.3644 - val_acc: 0.3867\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 2s 57us/step - loss: 1.3220 - acc: 0.3842 - val_loss: 1.3593 - val_acc: 0.3859\n",
      "Epoch 9/80\n",
      "29639/29639 [==============================] - 2s 59us/step - loss: 1.3191 - acc: 0.3852 - val_loss: 1.3698 - val_acc: 0.3843\n",
      "Epoch 10/80\n",
      "29639/29639 [==============================] - 2s 58us/step - loss: 1.3200 - acc: 0.3879 - val_loss: 1.3562 - val_acc: 0.3811\n",
      "Epoch 11/80\n",
      "29639/29639 [==============================] - 2s 58us/step - loss: 1.3180 - acc: 0.3862 - val_loss: 1.3834 - val_acc: 0.3840\n",
      "Epoch 12/80\n",
      "29639/29639 [==============================] - 2s 58us/step - loss: 1.3174 - acc: 0.3876 - val_loss: 1.3738 - val_acc: 0.3884\n",
      "Epoch 13/80\n",
      "29639/29639 [==============================] - 2s 58us/step - loss: 1.3170 - acc: 0.3867 - val_loss: 1.3727 - val_acc: 0.3884\n",
      "Epoch 14/80\n",
      "29639/29639 [==============================] - 2s 59us/step - loss: 1.2948 - acc: 0.4132 - val_loss: 1.3175 - val_acc: 0.4260\n",
      "Epoch 15/80\n",
      "29639/29639 [==============================] - 2s 54us/step - loss: 1.2560 - acc: 0.4332 - val_loss: 1.3088 - val_acc: 0.4212\n",
      "Epoch 16/80\n",
      "29639/29639 [==============================] - 2s 53us/step - loss: 1.2416 - acc: 0.4381 - val_loss: 1.3189 - val_acc: 0.4145\n",
      "Epoch 17/80\n",
      "29639/29639 [==============================] - 2s 54us/step - loss: 1.2362 - acc: 0.4421 - val_loss: 1.3220 - val_acc: 0.4255\n",
      "Epoch 18/80\n",
      "29639/29639 [==============================] - 2s 53us/step - loss: 1.1529 - acc: 0.5038 - val_loss: 1.1470 - val_acc: 0.5244\n",
      "Epoch 19/80\n",
      "29639/29639 [==============================] - 2s 53us/step - loss: 1.0725 - acc: 0.5436 - val_loss: 1.1583 - val_acc: 0.5200\n",
      "Epoch 20/80\n",
      "29639/29639 [==============================] - 2s 53us/step - loss: 1.0631 - acc: 0.5440 - val_loss: 1.1490 - val_acc: 0.5198\n",
      "Epoch 21/80\n",
      "29639/29639 [==============================] - 2s 56us/step - loss: 1.0554 - acc: 0.5461 - val_loss: 1.1229 - val_acc: 0.5286\n",
      "Epoch 22/80\n",
      "29639/29639 [==============================] - 2s 58us/step - loss: 1.0500 - acc: 0.5486 - val_loss: 1.1472 - val_acc: 0.5163\n",
      "Epoch 23/80\n",
      "29639/29639 [==============================] - 2s 57us/step - loss: 1.0452 - acc: 0.5517 - val_loss: 1.1211 - val_acc: 0.5406\n",
      "Epoch 24/80\n",
      "29639/29639 [==============================] - 2s 54us/step - loss: 1.0325 - acc: 0.5607 - val_loss: 1.1244 - val_acc: 0.5399\n",
      "Epoch 25/80\n",
      "29639/29639 [==============================] - 2s 53us/step - loss: 1.0217 - acc: 0.5674 - val_loss: 1.1114 - val_acc: 0.5585\n",
      "Epoch 26/80\n",
      "29639/29639 [==============================] - 2s 57us/step - loss: 0.9895 - acc: 0.6024 - val_loss: 1.0563 - val_acc: 0.6174\n",
      "Epoch 27/80\n",
      "29639/29639 [==============================] - 2s 58us/step - loss: 0.9106 - acc: 0.6532 - val_loss: 1.0244 - val_acc: 0.6162\n",
      "Epoch 28/80\n",
      "29639/29639 [==============================] - 2s 57us/step - loss: 0.8648 - acc: 0.6665 - val_loss: 0.9659 - val_acc: 0.6439\n",
      "Epoch 29/80\n",
      "29639/29639 [==============================] - 2s 57us/step - loss: 0.8425 - acc: 0.6746 - val_loss: 0.9568 - val_acc: 0.6443\n",
      "Epoch 30/80\n",
      "29639/29639 [==============================] - 2s 61us/step - loss: 0.8385 - acc: 0.6767 - val_loss: 0.9668 - val_acc: 0.6436\n",
      "Epoch 31/80\n",
      "29639/29639 [==============================] - 2s 58us/step - loss: 0.8268 - acc: 0.6789 - val_loss: 0.9671 - val_acc: 0.6441\n",
      "Epoch 32/80\n",
      "29639/29639 [==============================] - 2s 64us/step - loss: 0.8181 - acc: 0.6829 - val_loss: 0.9739 - val_acc: 0.6387\n",
      "Epoch 33/80\n",
      "29639/29639 [==============================] - 2s 58us/step - loss: 0.8171 - acc: 0.6821 - val_loss: 0.9773 - val_acc: 0.6398\n",
      "Epoch 34/80\n",
      "29639/29639 [==============================] - 2s 60us/step - loss: 0.8111 - acc: 0.6862 - val_loss: 0.9584 - val_acc: 0.6450\n",
      "Inner step: 30 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 3s 85us/step - loss: 1.4154 - acc: 0.3420 - val_loss: 1.3712 - val_acc: 0.3751\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 2s 59us/step - loss: 1.3380 - acc: 0.3762 - val_loss: 1.3791 - val_acc: 0.3833\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 2s 58us/step - loss: 1.3309 - acc: 0.3772 - val_loss: 1.3793 - val_acc: 0.3812\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 2s 58us/step - loss: 1.3262 - acc: 0.3792 - val_loss: 1.3695 - val_acc: 0.3866\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 2s 62us/step - loss: 1.3245 - acc: 0.3804 - val_loss: 1.3820 - val_acc: 0.3801\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 2s 65us/step - loss: 1.3234 - acc: 0.3819 - val_loss: 1.3817 - val_acc: 0.3778\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 2s 65us/step - loss: 1.3218 - acc: 0.3828 - val_loss: 1.3668 - val_acc: 0.3834\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 2s 64us/step - loss: 1.3213 - acc: 0.3858 - val_loss: 1.3665 - val_acc: 0.3869\n",
      "Epoch 9/80\n",
      "29639/29639 [==============================] - 2s 62us/step - loss: 1.3202 - acc: 0.3836 - val_loss: 1.3593 - val_acc: 0.3860\n",
      "Epoch 10/80\n",
      "29639/29639 [==============================] - 2s 61us/step - loss: 1.3211 - acc: 0.3836 - val_loss: 1.3824 - val_acc: 0.3845\n",
      "Epoch 11/80\n",
      "29639/29639 [==============================] - 2s 60us/step - loss: 1.3181 - acc: 0.3875 - val_loss: 1.3618 - val_acc: 0.3793\n",
      "Epoch 12/80\n",
      "29639/29639 [==============================] - 2s 57us/step - loss: 1.3175 - acc: 0.3868 - val_loss: 1.3600 - val_acc: 0.3876\n",
      "Epoch 13/80\n",
      "29639/29639 [==============================] - 2s 66us/step - loss: 1.3200 - acc: 0.3864 - val_loss: 1.3577 - val_acc: 0.3849\n",
      "Epoch 14/80\n",
      "29639/29639 [==============================] - 2s 60us/step - loss: 1.3169 - acc: 0.3865 - val_loss: 1.3771 - val_acc: 0.3905\n",
      "Epoch 15/80\n",
      "29639/29639 [==============================] - 2s 64us/step - loss: 1.3156 - acc: 0.3895 - val_loss: 1.3768 - val_acc: 0.3857\n",
      "Epoch 16/80\n",
      "29639/29639 [==============================] - 2s 59us/step - loss: 1.3186 - acc: 0.3884 - val_loss: 1.3591 - val_acc: 0.3842\n",
      "Epoch 17/80\n",
      "29639/29639 [==============================] - 2s 71us/step - loss: 1.1880 - acc: 0.4795 - val_loss: 1.2089 - val_acc: 0.4877\n",
      "Epoch 18/80\n",
      "29639/29639 [==============================] - 2s 62us/step - loss: 1.1392 - acc: 0.5070 - val_loss: 1.1865 - val_acc: 0.5067\n",
      "Epoch 19/80\n",
      "29639/29639 [==============================] - 2s 74us/step - loss: 1.1009 - acc: 0.5317 - val_loss: 1.1982 - val_acc: 0.5173\n",
      "Epoch 20/80\n",
      "29639/29639 [==============================] - 2s 70us/step - loss: 1.0772 - acc: 0.5436 - val_loss: 1.1486 - val_acc: 0.5203\n",
      "Epoch 21/80\n",
      "29639/29639 [==============================] - 2s 67us/step - loss: 1.0509 - acc: 0.5568 - val_loss: 1.0732 - val_acc: 0.5874\n",
      "Epoch 22/80\n",
      "29639/29639 [==============================] - 2s 69us/step - loss: 0.9843 - acc: 0.6037 - val_loss: 1.0221 - val_acc: 0.5946\n",
      "Epoch 23/80\n",
      "29639/29639 [==============================] - 2s 64us/step - loss: 0.9299 - acc: 0.6223 - val_loss: 1.0225 - val_acc: 0.5938\n",
      "Epoch 24/80\n",
      "29639/29639 [==============================] - 2s 59us/step - loss: 0.9112 - acc: 0.6320 - val_loss: 0.9791 - val_acc: 0.6264\n",
      "Epoch 25/80\n",
      "29639/29639 [==============================] - 2s 67us/step - loss: 0.8880 - acc: 0.6481 - val_loss: 0.9683 - val_acc: 0.6366\n",
      "Epoch 26/80\n",
      "29639/29639 [==============================] - 2s 65us/step - loss: 0.8674 - acc: 0.6637 - val_loss: 0.9485 - val_acc: 0.6470\n",
      "Epoch 27/80\n",
      "29639/29639 [==============================] - 2s 58us/step - loss: 0.8446 - acc: 0.6742 - val_loss: 0.9824 - val_acc: 0.6320\n",
      "Epoch 28/80\n",
      "29639/29639 [==============================] - 2s 61us/step - loss: 0.8423 - acc: 0.6761 - val_loss: 0.9312 - val_acc: 0.6519\n",
      "Epoch 29/80\n",
      "29639/29639 [==============================] - 2s 66us/step - loss: 0.8290 - acc: 0.6811 - val_loss: 0.9539 - val_acc: 0.6498\n",
      "Epoch 30/80\n",
      "29639/29639 [==============================] - 2s 59us/step - loss: 0.8287 - acc: 0.6812 - val_loss: 0.9285 - val_acc: 0.6530\n",
      "Epoch 31/80\n",
      "29639/29639 [==============================] - 2s 66us/step - loss: 0.8196 - acc: 0.6846 - val_loss: 1.0274 - val_acc: 0.6381\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/80\n",
      "29639/29639 [==============================] - 2s 58us/step - loss: 0.8165 - acc: 0.6839 - val_loss: 0.9507 - val_acc: 0.6492\n",
      "Epoch 33/80\n",
      "29639/29639 [==============================] - 2s 58us/step - loss: 0.8126 - acc: 0.6871 - val_loss: 0.9393 - val_acc: 0.6533\n",
      "Epoch 34/80\n",
      "29639/29639 [==============================] - 2s 58us/step - loss: 0.8072 - acc: 0.6887 - val_loss: 0.9464 - val_acc: 0.6532\n",
      "Epoch 35/80\n",
      "29639/29639 [==============================] - 2s 63us/step - loss: 0.8088 - acc: 0.6891 - val_loss: 0.9455 - val_acc: 0.6500\n",
      "Inner step: 31 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 3s 87us/step - loss: 1.4286 - acc: 0.3431 - val_loss: 1.3756 - val_acc: 0.3729\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 2s 59us/step - loss: 1.3418 - acc: 0.3730 - val_loss: 1.3773 - val_acc: 0.3726\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 2s 60us/step - loss: 1.3324 - acc: 0.3718 - val_loss: 1.3734 - val_acc: 0.3648\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 2s 59us/step - loss: 1.3297 - acc: 0.3776 - val_loss: 1.3651 - val_acc: 0.3803\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 2s 62us/step - loss: 1.3265 - acc: 0.3777 - val_loss: 1.3741 - val_acc: 0.3843\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 2s 69us/step - loss: 1.3269 - acc: 0.3797 - val_loss: 1.3625 - val_acc: 0.3751\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 2s 64us/step - loss: 1.3237 - acc: 0.3816 - val_loss: 1.3774 - val_acc: 0.3860\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 2s 57us/step - loss: 1.3231 - acc: 0.3828 - val_loss: 1.3785 - val_acc: 0.3862\n",
      "Epoch 9/80\n",
      "29639/29639 [==============================] - 2s 57us/step - loss: 1.3223 - acc: 0.3858 - val_loss: 1.3652 - val_acc: 0.3777\n",
      "Epoch 10/80\n",
      "29639/29639 [==============================] - 2s 61us/step - loss: 1.3205 - acc: 0.3842 - val_loss: 1.3652 - val_acc: 0.3881\n",
      "Epoch 11/80\n",
      "29639/29639 [==============================] - 2s 56us/step - loss: 1.3036 - acc: 0.3999 - val_loss: 1.3149 - val_acc: 0.4224\n",
      "Epoch 12/80\n",
      "29639/29639 [==============================] - 2s 64us/step - loss: 1.2456 - acc: 0.4472 - val_loss: 1.1939 - val_acc: 0.5143\n",
      "Epoch 13/80\n",
      "29639/29639 [==============================] - 2s 74us/step - loss: 1.1024 - acc: 0.5362 - val_loss: 1.2086 - val_acc: 0.5174\n",
      "Epoch 14/80\n",
      "29639/29639 [==============================] - 2s 67us/step - loss: 1.0743 - acc: 0.5470 - val_loss: 1.2093 - val_acc: 0.5039\n",
      "Epoch 15/80\n",
      "29639/29639 [==============================] - 2s 84us/step - loss: 1.0587 - acc: 0.5488 - val_loss: 1.1246 - val_acc: 0.5345\n",
      "Epoch 16/80\n",
      "29639/29639 [==============================] - 2s 75us/step - loss: 1.0396 - acc: 0.5603 - val_loss: 1.0693 - val_acc: 0.5829\n",
      "Epoch 17/80\n",
      "29639/29639 [==============================] - 2s 56us/step - loss: 0.9458 - acc: 0.6244 - val_loss: 0.9975 - val_acc: 0.6081\n",
      "Epoch 18/80\n",
      "29639/29639 [==============================] - 2s 61us/step - loss: 0.8907 - acc: 0.6518 - val_loss: 0.9556 - val_acc: 0.6411\n",
      "Epoch 19/80\n",
      "29639/29639 [==============================] - 2s 65us/step - loss: 0.8667 - acc: 0.6659 - val_loss: 0.9550 - val_acc: 0.6403\n",
      "Epoch 20/80\n",
      "29639/29639 [==============================] - 2s 56us/step - loss: 0.8526 - acc: 0.6711 - val_loss: 0.9405 - val_acc: 0.6456\n",
      "Epoch 21/80\n",
      "29639/29639 [==============================] - 2s 55us/step - loss: 0.8447 - acc: 0.6741 - val_loss: 0.9951 - val_acc: 0.6275\n",
      "Epoch 22/80\n",
      "29639/29639 [==============================] - 2s 58us/step - loss: 0.8382 - acc: 0.6776 - val_loss: 0.9482 - val_acc: 0.6476\n",
      "Epoch 23/80\n",
      "29639/29639 [==============================] - 2s 55us/step - loss: 0.8365 - acc: 0.6773 - val_loss: 0.9784 - val_acc: 0.6448\n",
      "Epoch 24/80\n",
      "29639/29639 [==============================] - 2s 53us/step - loss: 0.8331 - acc: 0.6802 - val_loss: 0.9835 - val_acc: 0.6384\n",
      "Epoch 25/80\n",
      "29639/29639 [==============================] - 2s 54us/step - loss: 0.8292 - acc: 0.6827 - val_loss: 0.9695 - val_acc: 0.6507\n",
      "Inner step: 32 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 2s 84us/step - loss: 1.3959 - acc: 0.3628 - val_loss: 1.3533 - val_acc: 0.3920\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 2s 54us/step - loss: 1.2636 - acc: 0.4223 - val_loss: 1.3442 - val_acc: 0.3995\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 2s 53us/step - loss: 1.2505 - acc: 0.4255 - val_loss: 1.3186 - val_acc: 0.4189\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 2s 57us/step - loss: 1.2431 - acc: 0.4323 - val_loss: 1.2899 - val_acc: 0.4237\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 2s 57us/step - loss: 1.2387 - acc: 0.4310 - val_loss: 1.3265 - val_acc: 0.4215\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 2s 55us/step - loss: 1.2393 - acc: 0.4329 - val_loss: 1.2958 - val_acc: 0.4248\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 2s 54us/step - loss: 1.2327 - acc: 0.4338 - val_loss: 1.3584 - val_acc: 0.4041\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 2s 58us/step - loss: 1.2303 - acc: 0.4362 - val_loss: 1.3140 - val_acc: 0.4151\n",
      "Epoch 9/80\n",
      "29639/29639 [==============================] - 2s 58us/step - loss: 1.2288 - acc: 0.4370 - val_loss: 1.2978 - val_acc: 0.4192\n",
      "Inner step: 33 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 3s 88us/step - loss: 1.4297 - acc: 0.3410 - val_loss: 1.3526 - val_acc: 0.3972\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 2s 55us/step - loss: 1.2740 - acc: 0.4193 - val_loss: 1.3156 - val_acc: 0.4198\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 2s 54us/step - loss: 1.2513 - acc: 0.4270 - val_loss: 1.2950 - val_acc: 0.4281\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 2s 56us/step - loss: 1.2430 - acc: 0.4281 - val_loss: 1.2965 - val_acc: 0.4234\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 2s 54us/step - loss: 1.2428 - acc: 0.4276 - val_loss: 1.3170 - val_acc: 0.4243\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 2s 54us/step - loss: 1.2355 - acc: 0.4346 - val_loss: 1.3036 - val_acc: 0.4232\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 2s 54us/step - loss: 1.2329 - acc: 0.4377 - val_loss: 1.2901 - val_acc: 0.4249\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 2s 55us/step - loss: 1.2299 - acc: 0.4408 - val_loss: 1.3013 - val_acc: 0.4201\n",
      "Epoch 9/80\n",
      "29639/29639 [==============================] - 2s 54us/step - loss: 1.2292 - acc: 0.4373 - val_loss: 1.2925 - val_acc: 0.4214\n",
      "Epoch 10/80\n",
      "29639/29639 [==============================] - 2s 55us/step - loss: 1.2291 - acc: 0.4354 - val_loss: 1.2991 - val_acc: 0.4224\n",
      "Epoch 11/80\n",
      "29639/29639 [==============================] - 2s 66us/step - loss: 1.2304 - acc: 0.4388 - val_loss: 1.3352 - val_acc: 0.4155\n",
      "Epoch 12/80\n",
      "29639/29639 [==============================] - 2s 64us/step - loss: 1.2282 - acc: 0.4422 - val_loss: 1.3189 - val_acc: 0.4173\n",
      "Inner step: 34 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 3s 96us/step - loss: 1.4864 - acc: 0.3152 - val_loss: 1.4402 - val_acc: 0.3568\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 2s 58us/step - loss: 1.4169 - acc: 0.3563 - val_loss: 1.4382 - val_acc: 0.3567\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 2s 59us/step - loss: 1.4100 - acc: 0.3639 - val_loss: 1.4434 - val_acc: 0.3572\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 2s 66us/step - loss: 1.4067 - acc: 0.3642 - val_loss: 1.4225 - val_acc: 0.3646\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 2s 61us/step - loss: 1.4052 - acc: 0.3638 - val_loss: 1.4335 - val_acc: 0.3632\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 2s 64us/step - loss: 1.4061 - acc: 0.3650 - val_loss: 1.4333 - val_acc: 0.3657\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 2s 61us/step - loss: 1.4034 - acc: 0.3656 - val_loss: 1.4253 - val_acc: 0.3640\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 2s 59us/step - loss: 1.3904 - acc: 0.3677 - val_loss: 1.4092 - val_acc: 0.3631\n",
      "Epoch 9/80\n",
      "29639/29639 [==============================] - 2s 64us/step - loss: 1.3339 - acc: 0.3966 - val_loss: 1.3464 - val_acc: 0.4069\n",
      "Epoch 10/80\n",
      "29639/29639 [==============================] - 2s 71us/step - loss: 1.3181 - acc: 0.4042 - val_loss: 1.3269 - val_acc: 0.4553\n",
      "Epoch 11/80\n",
      "29639/29639 [==============================] - 2s 65us/step - loss: 1.1798 - acc: 0.4863 - val_loss: 1.1827 - val_acc: 0.4945\n",
      "Epoch 12/80\n",
      "29639/29639 [==============================] - 2s 64us/step - loss: 1.1447 - acc: 0.5094 - val_loss: 1.1728 - val_acc: 0.5002\n",
      "Epoch 13/80\n",
      "29639/29639 [==============================] - 2s 62us/step - loss: 1.1343 - acc: 0.5171 - val_loss: 1.1724 - val_acc: 0.5064\n",
      "Epoch 14/80\n",
      "29639/29639 [==============================] - 2s 70us/step - loss: 1.0904 - acc: 0.5501 - val_loss: 1.0295 - val_acc: 0.6098\n",
      "Epoch 15/80\n",
      "29639/29639 [==============================] - 2s 69us/step - loss: 0.9729 - acc: 0.6280 - val_loss: 0.9859 - val_acc: 0.6331\n",
      "Epoch 16/80\n",
      "29639/29639 [==============================] - 2s 69us/step - loss: 0.9265 - acc: 0.6436 - val_loss: 0.9924 - val_acc: 0.6215\n",
      "Epoch 17/80\n",
      "29639/29639 [==============================] - 2s 57us/step - loss: 0.9170 - acc: 0.6413 - val_loss: 0.9578 - val_acc: 0.6348\n",
      "Epoch 18/80\n",
      "29639/29639 [==============================] - 2s 60us/step - loss: 0.9017 - acc: 0.6442 - val_loss: 0.9615 - val_acc: 0.6419\n",
      "Epoch 19/80\n",
      "29639/29639 [==============================] - 2s 61us/step - loss: 0.8956 - acc: 0.6466 - val_loss: 0.9724 - val_acc: 0.6404\n",
      "Epoch 20/80\n",
      "29639/29639 [==============================] - 2s 58us/step - loss: 0.8811 - acc: 0.6618 - val_loss: 0.9222 - val_acc: 0.6631\n",
      "Epoch 21/80\n",
      "29639/29639 [==============================] - 2s 58us/step - loss: 0.8609 - acc: 0.6739 - val_loss: 0.9031 - val_acc: 0.6746\n",
      "Epoch 22/80\n",
      "29639/29639 [==============================] - 2s 67us/step - loss: 0.8410 - acc: 0.6849 - val_loss: 0.9113 - val_acc: 0.6729\n",
      "Epoch 23/80\n",
      "29639/29639 [==============================] - 2s 63us/step - loss: 0.8260 - acc: 0.6888 - val_loss: 0.9295 - val_acc: 0.6710\n",
      "Epoch 24/80\n",
      "29639/29639 [==============================] - 2s 58us/step - loss: 0.8153 - acc: 0.6930 - val_loss: 0.8945 - val_acc: 0.6800\n",
      "Epoch 25/80\n",
      "29639/29639 [==============================] - 2s 62us/step - loss: 0.8112 - acc: 0.6938 - val_loss: 0.9043 - val_acc: 0.6700\n",
      "Epoch 26/80\n",
      "29639/29639 [==============================] - 2s 60us/step - loss: 0.8063 - acc: 0.6967 - val_loss: 0.9228 - val_acc: 0.6701\n",
      "Epoch 27/80\n",
      "29639/29639 [==============================] - 2s 57us/step - loss: 0.8035 - acc: 0.6974 - val_loss: 0.8990 - val_acc: 0.6688\n",
      "Epoch 28/80\n",
      "29639/29639 [==============================] - 2s 67us/step - loss: 0.7955 - acc: 0.7004 - val_loss: 0.8919 - val_acc: 0.6797\n",
      "Epoch 29/80\n",
      "29639/29639 [==============================] - 2s 63us/step - loss: 0.7933 - acc: 0.7005 - val_loss: 0.9229 - val_acc: 0.6658\n",
      "Epoch 30/80\n",
      "29639/29639 [==============================] - 2s 58us/step - loss: 0.7931 - acc: 0.6999 - val_loss: 0.8986 - val_acc: 0.6792\n",
      "Epoch 31/80\n",
      "29639/29639 [==============================] - 2s 57us/step - loss: 0.7899 - acc: 0.7032 - val_loss: 0.9299 - val_acc: 0.6703\n",
      "Epoch 32/80\n",
      "29639/29639 [==============================] - 2s 63us/step - loss: 0.7879 - acc: 0.7014 - val_loss: 0.9408 - val_acc: 0.6648\n",
      "Epoch 33/80\n",
      "29639/29639 [==============================] - 2s 66us/step - loss: 0.7813 - acc: 0.7051 - val_loss: 0.9475 - val_acc: 0.6653\n",
      "Inner step: 35 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 3s 95us/step - loss: 1.3649 - acc: 0.4004 - val_loss: 1.2351 - val_acc: 0.4531\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 2s 55us/step - loss: 1.1732 - acc: 0.4835 - val_loss: 1.1908 - val_acc: 0.4823\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 2s 54us/step - loss: 1.1530 - acc: 0.4971 - val_loss: 1.2317 - val_acc: 0.5076\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 2s 55us/step - loss: 1.1470 - acc: 0.5002 - val_loss: 1.1847 - val_acc: 0.5087\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 2s 55us/step - loss: 1.1422 - acc: 0.5004 - val_loss: 1.1839 - val_acc: 0.5055\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 2s 55us/step - loss: 1.1401 - acc: 0.5008 - val_loss: 1.1896 - val_acc: 0.4863\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 2s 54us/step - loss: 1.1365 - acc: 0.5029 - val_loss: 1.2117 - val_acc: 0.4975\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 2s 59us/step - loss: 1.1348 - acc: 0.5046 - val_loss: 1.1878 - val_acc: 0.5094\n",
      "Epoch 9/80\n",
      "29639/29639 [==============================] - 2s 63us/step - loss: 1.1339 - acc: 0.5056 - val_loss: 1.1799 - val_acc: 0.4996\n",
      "Epoch 10/80\n",
      "29639/29639 [==============================] - 2s 61us/step - loss: 1.1311 - acc: 0.5101 - val_loss: 1.1821 - val_acc: 0.5229\n",
      "Epoch 11/80\n",
      "29639/29639 [==============================] - 2s 57us/step - loss: 1.1319 - acc: 0.5098 - val_loss: 1.1829 - val_acc: 0.5293\n",
      "Epoch 12/80\n",
      "29639/29639 [==============================] - 2s 57us/step - loss: 1.1033 - acc: 0.5345 - val_loss: 1.1670 - val_acc: 0.5213\n",
      "Epoch 13/80\n",
      "29639/29639 [==============================] - 2s 58us/step - loss: 1.0758 - acc: 0.5540 - val_loss: 1.1458 - val_acc: 0.5205\n",
      "Epoch 14/80\n",
      "29639/29639 [==============================] - 2s 58us/step - loss: 1.0627 - acc: 0.5536 - val_loss: 1.1306 - val_acc: 0.5229\n",
      "Epoch 15/80\n",
      "29639/29639 [==============================] - 2s 62us/step - loss: 1.0571 - acc: 0.5513 - val_loss: 1.1683 - val_acc: 0.5329\n",
      "Epoch 16/80\n",
      "29639/29639 [==============================] - 2s 66us/step - loss: 1.0491 - acc: 0.5553 - val_loss: 1.1526 - val_acc: 0.5219\n",
      "Epoch 17/80\n",
      "29639/29639 [==============================] - 2s 60us/step - loss: 1.0461 - acc: 0.5605 - val_loss: 1.1315 - val_acc: 0.5685\n",
      "Epoch 18/80\n",
      "29639/29639 [==============================] - 2s 59us/step - loss: 0.9550 - acc: 0.6286 - val_loss: 0.9847 - val_acc: 0.6482\n",
      "Epoch 19/80\n",
      "29639/29639 [==============================] - 2s 64us/step - loss: 0.8775 - acc: 0.6726 - val_loss: 0.9402 - val_acc: 0.6605\n",
      "Epoch 20/80\n",
      "29639/29639 [==============================] - 2s 59us/step - loss: 0.8283 - acc: 0.6912 - val_loss: 0.9292 - val_acc: 0.6640\n",
      "Epoch 21/80\n",
      "29639/29639 [==============================] - 2s 59us/step - loss: 0.8081 - acc: 0.6976 - val_loss: 0.9564 - val_acc: 0.6458\n",
      "Epoch 22/80\n",
      "29639/29639 [==============================] - 2s 59us/step - loss: 0.7911 - acc: 0.7045 - val_loss: 0.9216 - val_acc: 0.6707\n",
      "Epoch 23/80\n",
      "29639/29639 [==============================] - 2s 56us/step - loss: 0.7818 - acc: 0.7127 - val_loss: 0.8699 - val_acc: 0.6901\n",
      "Epoch 24/80\n",
      "29639/29639 [==============================] - 2s 54us/step - loss: 0.7734 - acc: 0.7154 - val_loss: 0.8790 - val_acc: 0.6947\n",
      "Epoch 25/80\n",
      "29639/29639 [==============================] - 2s 57us/step - loss: 0.7663 - acc: 0.7228 - val_loss: 0.9701 - val_acc: 0.6591\n",
      "Epoch 26/80\n",
      "29639/29639 [==============================] - 2s 59us/step - loss: 0.7552 - acc: 0.7313 - val_loss: 0.8714 - val_acc: 0.7021\n",
      "Epoch 27/80\n",
      "29639/29639 [==============================] - 2s 58us/step - loss: 0.7421 - acc: 0.7342 - val_loss: 0.8549 - val_acc: 0.7089\n",
      "Epoch 28/80\n",
      "29639/29639 [==============================] - 2s 54us/step - loss: 0.7383 - acc: 0.7385 - val_loss: 0.8671 - val_acc: 0.6982\n",
      "Epoch 29/80\n",
      "29639/29639 [==============================] - 2s 54us/step - loss: 0.7308 - acc: 0.7420 - val_loss: 0.8429 - val_acc: 0.7076\n",
      "Epoch 30/80\n",
      "29639/29639 [==============================] - 2s 55us/step - loss: 0.7224 - acc: 0.7421 - val_loss: 0.9107 - val_acc: 0.6954\n",
      "Epoch 31/80\n",
      "29639/29639 [==============================] - 2s 54us/step - loss: 0.7162 - acc: 0.7456 - val_loss: 0.8559 - val_acc: 0.7041\n",
      "Epoch 32/80\n",
      "29639/29639 [==============================] - 2s 54us/step - loss: 0.7102 - acc: 0.7484 - val_loss: 0.8422 - val_acc: 0.7158\n",
      "Epoch 33/80\n",
      "29639/29639 [==============================] - 2s 59us/step - loss: 0.7109 - acc: 0.7483 - val_loss: 0.8439 - val_acc: 0.7092\n",
      "Epoch 34/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29639/29639 [==============================] - 2s 58us/step - loss: 0.7042 - acc: 0.7496 - val_loss: 0.8748 - val_acc: 0.6984\n",
      "Inner step: 36 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 3s 90us/step - loss: 1.4835 - acc: 0.3303 - val_loss: 1.4542 - val_acc: 0.3534\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 2s 59us/step - loss: 1.4356 - acc: 0.3617 - val_loss: 1.4557 - val_acc: 0.3642\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 2s 59us/step - loss: 1.4313 - acc: 0.3647 - val_loss: 1.4537 - val_acc: 0.3680\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 2s 60us/step - loss: 1.4296 - acc: 0.3669 - val_loss: 1.4499 - val_acc: 0.3638\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 2s 60us/step - loss: 1.4276 - acc: 0.3673 - val_loss: 1.4541 - val_acc: 0.3669\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 2s 60us/step - loss: 1.3865 - acc: 0.3708 - val_loss: 1.3697 - val_acc: 0.3843\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 2s 59us/step - loss: 1.3349 - acc: 0.3763 - val_loss: 1.3767 - val_acc: 0.3708\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 2s 63us/step - loss: 1.3277 - acc: 0.3793 - val_loss: 1.3590 - val_acc: 0.3783\n",
      "Epoch 9/80\n",
      "29639/29639 [==============================] - 2s 64us/step - loss: 1.3219 - acc: 0.3815 - val_loss: 1.3607 - val_acc: 0.3845\n",
      "Epoch 10/80\n",
      "29639/29639 [==============================] - 2s 63us/step - loss: 1.2823 - acc: 0.4152 - val_loss: 1.2162 - val_acc: 0.4848\n",
      "Epoch 11/80\n",
      "29639/29639 [==============================] - 2s 62us/step - loss: 1.1538 - acc: 0.4928 - val_loss: 1.1780 - val_acc: 0.5065\n",
      "Epoch 12/80\n",
      "29639/29639 [==============================] - 2s 63us/step - loss: 1.1191 - acc: 0.5164 - val_loss: 1.1737 - val_acc: 0.5170\n",
      "Epoch 13/80\n",
      "29639/29639 [==============================] - 2s 58us/step - loss: 1.0859 - acc: 0.5373 - val_loss: 1.1412 - val_acc: 0.5308\n",
      "Epoch 14/80\n",
      "29639/29639 [==============================] - 2s 60us/step - loss: 1.0701 - acc: 0.5418 - val_loss: 1.1667 - val_acc: 0.5338\n",
      "Epoch 15/80\n",
      "29639/29639 [==============================] - 2s 66us/step - loss: 1.0532 - acc: 0.5550 - val_loss: 1.0946 - val_acc: 0.5817\n",
      "Epoch 16/80\n",
      "29639/29639 [==============================] - 2s 76us/step - loss: 0.9813 - acc: 0.6029 - val_loss: 1.0627 - val_acc: 0.5848\n",
      "Epoch 17/80\n",
      "29639/29639 [==============================] - 2s 69us/step - loss: 0.9186 - acc: 0.6328 - val_loss: 0.9792 - val_acc: 0.6235\n",
      "Epoch 18/80\n",
      "29639/29639 [==============================] - 2s 59us/step - loss: 0.8769 - acc: 0.6604 - val_loss: 0.9753 - val_acc: 0.6419\n",
      "Epoch 19/80\n",
      "29639/29639 [==============================] - 2s 59us/step - loss: 0.8481 - acc: 0.6744 - val_loss: 0.9530 - val_acc: 0.6449\n",
      "Epoch 20/80\n",
      "29639/29639 [==============================] - 2s 70us/step - loss: 0.8434 - acc: 0.6767 - val_loss: 0.9506 - val_acc: 0.6495\n",
      "Epoch 21/80\n",
      "29639/29639 [==============================] - 2s 60us/step - loss: 0.8324 - acc: 0.6812 - val_loss: 0.9746 - val_acc: 0.6474\n",
      "Epoch 22/80\n",
      "29639/29639 [==============================] - 2s 56us/step - loss: 0.8300 - acc: 0.6838 - val_loss: 0.9586 - val_acc: 0.6398\n",
      "Epoch 23/80\n",
      "29639/29639 [==============================] - 2s 58us/step - loss: 0.8258 - acc: 0.6853 - val_loss: 0.9636 - val_acc: 0.6531\n",
      "Epoch 24/80\n",
      "29639/29639 [==============================] - 2s 55us/step - loss: 0.8251 - acc: 0.6852 - val_loss: 0.9643 - val_acc: 0.6454\n",
      "Epoch 25/80\n",
      "29639/29639 [==============================] - 2s 55us/step - loss: 0.8191 - acc: 0.6881 - val_loss: 0.9631 - val_acc: 0.6539\n",
      "Inner step: 37 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 3s 88us/step - loss: 1.4757 - acc: 0.3263 - val_loss: 1.4336 - val_acc: 0.3602\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 2s 55us/step - loss: 1.4146 - acc: 0.3612 - val_loss: 1.4387 - val_acc: 0.3482\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 2s 63us/step - loss: 1.4097 - acc: 0.3625 - val_loss: 1.4277 - val_acc: 0.3659\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 2s 58us/step - loss: 1.4081 - acc: 0.3637 - val_loss: 1.4363 - val_acc: 0.3671\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 2s 55us/step - loss: 1.4042 - acc: 0.3653 - val_loss: 1.4123 - val_acc: 0.3872\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 2s 59us/step - loss: 1.2708 - acc: 0.4528 - val_loss: 1.2840 - val_acc: 0.4572\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 2s 59us/step - loss: 1.2371 - acc: 0.4673 - val_loss: 1.2818 - val_acc: 0.4637\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 2s 59us/step - loss: 1.2324 - acc: 0.4696 - val_loss: 1.2820 - val_acc: 0.4640\n",
      "Epoch 9/80\n",
      "29639/29639 [==============================] - 2s 62us/step - loss: 1.2294 - acc: 0.4697 - val_loss: 1.2807 - val_acc: 0.4666\n",
      "Epoch 10/80\n",
      "29639/29639 [==============================] - 2s 61us/step - loss: 1.2225 - acc: 0.4747 - val_loss: 1.2488 - val_acc: 0.4785\n",
      "Epoch 11/80\n",
      "29639/29639 [==============================] - 2s 61us/step - loss: 1.1757 - acc: 0.5039 - val_loss: 1.2411 - val_acc: 0.4845\n",
      "Epoch 12/80\n",
      "29639/29639 [==============================] - 2s 56us/step - loss: 1.1640 - acc: 0.5065 - val_loss: 1.2228 - val_acc: 0.4839\n",
      "Epoch 13/80\n",
      "29639/29639 [==============================] - 2s 59us/step - loss: 1.1481 - acc: 0.5210 - val_loss: 1.2000 - val_acc: 0.4996\n",
      "Epoch 14/80\n",
      "29639/29639 [==============================] - 2s 57us/step - loss: 1.1252 - acc: 0.5387 - val_loss: 1.1915 - val_acc: 0.5133\n",
      "Epoch 15/80\n",
      "29639/29639 [==============================] - 2s 61us/step - loss: 1.1097 - acc: 0.5426 - val_loss: 1.2117 - val_acc: 0.5328\n",
      "Epoch 16/80\n",
      "29639/29639 [==============================] - 2s 55us/step - loss: 1.1042 - acc: 0.5461 - val_loss: 1.1820 - val_acc: 0.5119\n",
      "Epoch 17/80\n",
      "29639/29639 [==============================] - 2s 58us/step - loss: 1.1025 - acc: 0.5437 - val_loss: 1.2087 - val_acc: 0.5036\n",
      "Epoch 18/80\n",
      "29639/29639 [==============================] - 2s 68us/step - loss: 1.0967 - acc: 0.5432 - val_loss: 1.1850 - val_acc: 0.5107\n",
      "Epoch 19/80\n",
      "29639/29639 [==============================] - 2s 62us/step - loss: 1.0875 - acc: 0.5496 - val_loss: 1.1633 - val_acc: 0.5088\n",
      "Epoch 20/80\n",
      "29639/29639 [==============================] - 2s 60us/step - loss: 1.0774 - acc: 0.5532 - val_loss: 1.1207 - val_acc: 0.5510\n",
      "Epoch 21/80\n",
      "29639/29639 [==============================] - 2s 63us/step - loss: 0.9298 - acc: 0.6533 - val_loss: 0.9284 - val_acc: 0.6747\n",
      "Epoch 22/80\n",
      "29639/29639 [==============================] - 2s 63us/step - loss: 0.8421 - acc: 0.6943 - val_loss: 0.9156 - val_acc: 0.6837\n",
      "Epoch 23/80\n",
      "29639/29639 [==============================] - 2s 63us/step - loss: 0.8099 - acc: 0.7121 - val_loss: 0.9020 - val_acc: 0.6902\n",
      "Epoch 24/80\n",
      "29639/29639 [==============================] - 2s 60us/step - loss: 0.7818 - acc: 0.7223 - val_loss: 0.9022 - val_acc: 0.6920\n",
      "Epoch 25/80\n",
      "29639/29639 [==============================] - 2s 56us/step - loss: 0.7699 - acc: 0.7261 - val_loss: 0.8775 - val_acc: 0.7042\n",
      "Epoch 26/80\n",
      "29639/29639 [==============================] - 2s 65us/step - loss: 0.7528 - acc: 0.7337 - val_loss: 0.9028 - val_acc: 0.6960\n",
      "Epoch 27/80\n",
      "29639/29639 [==============================] - 2s 60us/step - loss: 0.7431 - acc: 0.7363 - val_loss: 0.8624 - val_acc: 0.7108\n",
      "Epoch 28/80\n",
      "29639/29639 [==============================] - 2s 57us/step - loss: 0.7331 - acc: 0.7404 - val_loss: 0.8959 - val_acc: 0.7013\n",
      "Epoch 29/80\n",
      "29639/29639 [==============================] - 2s 57us/step - loss: 0.7288 - acc: 0.7422 - val_loss: 0.8754 - val_acc: 0.7017\n",
      "Epoch 30/80\n",
      "29639/29639 [==============================] - 2s 57us/step - loss: 0.7251 - acc: 0.7425 - val_loss: 0.8631 - val_acc: 0.7090\n",
      "Epoch 31/80\n",
      "29639/29639 [==============================] - 2s 55us/step - loss: 0.7140 - acc: 0.7486 - val_loss: 0.8734 - val_acc: 0.7052\n",
      "Epoch 32/80\n",
      "29639/29639 [==============================] - 2s 63us/step - loss: 0.7111 - acc: 0.7476 - val_loss: 0.8753 - val_acc: 0.7058\n",
      "Inner step: 38 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29639/29639 [==============================] - 3s 117us/step - loss: 1.4161 - acc: 0.3440 - val_loss: 1.3713 - val_acc: 0.3644\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 2s 60us/step - loss: 1.3384 - acc: 0.3744 - val_loss: 1.3687 - val_acc: 0.3822\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 2s 83us/step - loss: 1.3319 - acc: 0.3786 - val_loss: 1.3864 - val_acc: 0.3840\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 2s 64us/step - loss: 1.3293 - acc: 0.3809 - val_loss: 1.3788 - val_acc: 0.3822\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 2s 58us/step - loss: 1.3241 - acc: 0.3846 - val_loss: 1.4135 - val_acc: 0.3739\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 2s 59us/step - loss: 1.3230 - acc: 0.3848 - val_loss: 1.3687 - val_acc: 0.3844\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 2s 57us/step - loss: 1.3229 - acc: 0.3862 - val_loss: 1.3678 - val_acc: 0.3857\n",
      "Inner step: 39 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 3s 91us/step - loss: 1.4211 - acc: 0.3422 - val_loss: 1.3676 - val_acc: 0.3816\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 2s 60us/step - loss: 1.3385 - acc: 0.3763 - val_loss: 1.3669 - val_acc: 0.3857\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 2s 61us/step - loss: 1.3290 - acc: 0.3797 - val_loss: 1.3865 - val_acc: 0.3781\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 2s 55us/step - loss: 1.3295 - acc: 0.3798 - val_loss: 1.3628 - val_acc: 0.3796\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 2s 55us/step - loss: 1.3266 - acc: 0.3823 - val_loss: 1.3610 - val_acc: 0.3822\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 2s 56us/step - loss: 1.3233 - acc: 0.3846 - val_loss: 1.3593 - val_acc: 0.3780\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 2s 58us/step - loss: 1.3232 - acc: 0.3834 - val_loss: 1.3668 - val_acc: 0.3868\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 2s 57us/step - loss: 1.3210 - acc: 0.3855 - val_loss: 1.3617 - val_acc: 0.3765\n",
      "Epoch 9/80\n",
      "29639/29639 [==============================] - 2s 60us/step - loss: 1.3197 - acc: 0.3835 - val_loss: 1.3874 - val_acc: 0.3842\n",
      "Epoch 10/80\n",
      "29639/29639 [==============================] - 2s 61us/step - loss: 1.3201 - acc: 0.3840 - val_loss: 1.3941 - val_acc: 0.3847\n",
      "Epoch 11/80\n",
      "29639/29639 [==============================] - 2s 57us/step - loss: 1.2632 - acc: 0.4306 - val_loss: 1.2244 - val_acc: 0.4815\n",
      "Epoch 12/80\n",
      "29639/29639 [==============================] - 2s 56us/step - loss: 1.1573 - acc: 0.4930 - val_loss: 1.2380 - val_acc: 0.4830\n",
      "Epoch 13/80\n",
      "29639/29639 [==============================] - 2s 56us/step - loss: 1.1398 - acc: 0.5022 - val_loss: 1.1779 - val_acc: 0.5138\n",
      "Epoch 14/80\n",
      "29639/29639 [==============================] - 2s 56us/step - loss: 1.0967 - acc: 0.5364 - val_loss: 1.1646 - val_acc: 0.5174\n",
      "Epoch 15/80\n",
      "29639/29639 [==============================] - 2s 56us/step - loss: 1.0746 - acc: 0.5432 - val_loss: 1.1941 - val_acc: 0.5060\n",
      "Epoch 16/80\n",
      "29639/29639 [==============================] - 2s 58us/step - loss: 1.0684 - acc: 0.5471 - val_loss: 1.1543 - val_acc: 0.5201\n",
      "Epoch 17/80\n",
      "29639/29639 [==============================] - 2s 57us/step - loss: 1.0618 - acc: 0.5474 - val_loss: 1.1637 - val_acc: 0.5225\n",
      "Epoch 18/80\n",
      "29639/29639 [==============================] - 2s 57us/step - loss: 1.0586 - acc: 0.5459 - val_loss: 1.1241 - val_acc: 0.5286\n",
      "Epoch 19/80\n",
      "29639/29639 [==============================] - 2s 56us/step - loss: 1.0550 - acc: 0.5470 - val_loss: 1.1217 - val_acc: 0.5243\n",
      "Epoch 20/80\n",
      "29639/29639 [==============================] - 2s 62us/step - loss: 1.0524 - acc: 0.5446 - val_loss: 1.1443 - val_acc: 0.5271\n",
      "Epoch 21/80\n",
      "29639/29639 [==============================] - 2s 59us/step - loss: 1.0469 - acc: 0.5472 - val_loss: 1.1242 - val_acc: 0.5326\n",
      "Epoch 22/80\n",
      "29639/29639 [==============================] - 2s 58us/step - loss: 1.0454 - acc: 0.5467 - val_loss: 1.1414 - val_acc: 0.5220\n",
      "Epoch 23/80\n",
      "29639/29639 [==============================] - 2s 61us/step - loss: 1.0438 - acc: 0.5458 - val_loss: 1.1413 - val_acc: 0.5311\n",
      "Epoch 24/80\n",
      "29639/29639 [==============================] - 2s 61us/step - loss: 1.0385 - acc: 0.5504 - val_loss: 1.1074 - val_acc: 0.5351\n",
      "Epoch 25/80\n",
      "29639/29639 [==============================] - 2s 60us/step - loss: 1.0307 - acc: 0.5624 - val_loss: 1.1100 - val_acc: 0.5577\n",
      "Epoch 26/80\n",
      "29639/29639 [==============================] - 2s 59us/step - loss: 1.0178 - acc: 0.5687 - val_loss: 1.1066 - val_acc: 0.5534\n",
      "Epoch 27/80\n",
      "29639/29639 [==============================] - 2s 58us/step - loss: 1.0073 - acc: 0.5781 - val_loss: 1.1093 - val_acc: 0.5544\n",
      "Epoch 28/80\n",
      "29639/29639 [==============================] - 2s 60us/step - loss: 0.9991 - acc: 0.5840 - val_loss: 1.1274 - val_acc: 0.5463\n",
      "Epoch 29/80\n",
      "29639/29639 [==============================] - 2s 57us/step - loss: 0.9916 - acc: 0.5882 - val_loss: 1.1484 - val_acc: 0.5514\n",
      "Inner step: 40 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 3s 95us/step - loss: 1.4173 - acc: 0.3435 - val_loss: 1.3719 - val_acc: 0.3856\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 2s 57us/step - loss: 1.3361 - acc: 0.3734 - val_loss: 1.3609 - val_acc: 0.3750\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 2s 60us/step - loss: 1.3298 - acc: 0.3771 - val_loss: 1.3653 - val_acc: 0.3857\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 2s 58us/step - loss: 1.3271 - acc: 0.3824 - val_loss: 1.3625 - val_acc: 0.3816\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 2s 58us/step - loss: 1.3264 - acc: 0.3801 - val_loss: 1.3727 - val_acc: 0.3830\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 2s 59us/step - loss: 1.3236 - acc: 0.3829 - val_loss: 1.3662 - val_acc: 0.3832\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 2s 59us/step - loss: 1.3220 - acc: 0.3810 - val_loss: 1.3573 - val_acc: 0.3831\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 2s 58us/step - loss: 1.3217 - acc: 0.3828 - val_loss: 1.3784 - val_acc: 0.3724\n",
      "Epoch 9/80\n",
      "29639/29639 [==============================] - 2s 57us/step - loss: 1.3186 - acc: 0.3840 - val_loss: 1.3779 - val_acc: 0.3821\n",
      "Epoch 10/80\n",
      "29639/29639 [==============================] - 2s 57us/step - loss: 1.3197 - acc: 0.3867 - val_loss: 1.3807 - val_acc: 0.3820\n",
      "Epoch 11/80\n",
      "29639/29639 [==============================] - 2s 57us/step - loss: 1.3193 - acc: 0.3831 - val_loss: 1.3806 - val_acc: 0.3908\n",
      "Epoch 12/80\n",
      "29639/29639 [==============================] - 2s 58us/step - loss: 1.2830 - acc: 0.4188 - val_loss: 1.3162 - val_acc: 0.4218\n",
      "Epoch 13/80\n",
      "29639/29639 [==============================] - 2s 59us/step - loss: 1.2549 - acc: 0.4331 - val_loss: 1.3304 - val_acc: 0.4189\n",
      "Epoch 14/80\n",
      "29639/29639 [==============================] - 2s 58us/step - loss: 1.2462 - acc: 0.4409 - val_loss: 1.3205 - val_acc: 0.4179\n",
      "Epoch 15/80\n",
      "29639/29639 [==============================] - 2s 57us/step - loss: 1.2056 - acc: 0.4701 - val_loss: 1.1656 - val_acc: 0.5209\n",
      "Epoch 16/80\n",
      "29639/29639 [==============================] - 2s 60us/step - loss: 1.0865 - acc: 0.5415 - val_loss: 1.1598 - val_acc: 0.5228\n",
      "Epoch 17/80\n",
      "29639/29639 [==============================] - 2s 57us/step - loss: 1.0681 - acc: 0.5466 - val_loss: 1.1361 - val_acc: 0.5255\n",
      "Epoch 18/80\n",
      "29639/29639 [==============================] - 2s 58us/step - loss: 1.0594 - acc: 0.5470 - val_loss: 1.1368 - val_acc: 0.5226\n",
      "Epoch 19/80\n",
      "29639/29639 [==============================] - 2s 58us/step - loss: 1.0520 - acc: 0.5475 - val_loss: 1.1415 - val_acc: 0.5215\n",
      "Epoch 20/80\n",
      "29639/29639 [==============================] - 2s 56us/step - loss: 1.0443 - acc: 0.5500 - val_loss: 1.1442 - val_acc: 0.5298\n",
      "Epoch 21/80\n",
      "29639/29639 [==============================] - 2s 56us/step - loss: 1.0361 - acc: 0.5546 - val_loss: 1.1618 - val_acc: 0.5357\n",
      "Epoch 22/80\n",
      "29639/29639 [==============================] - 2s 58us/step - loss: 1.0276 - acc: 0.5650 - val_loss: 1.1077 - val_acc: 0.5525\n",
      "Epoch 23/80\n",
      "29639/29639 [==============================] - 2s 57us/step - loss: 1.0133 - acc: 0.5766 - val_loss: 1.1118 - val_acc: 0.5558\n",
      "Epoch 24/80\n",
      "29639/29639 [==============================] - 2s 57us/step - loss: 1.0036 - acc: 0.5797 - val_loss: 1.0854 - val_acc: 0.5636\n",
      "Epoch 25/80\n",
      "29639/29639 [==============================] - 2s 56us/step - loss: 0.9992 - acc: 0.5821 - val_loss: 1.0881 - val_acc: 0.5577\n",
      "Epoch 26/80\n",
      "29639/29639 [==============================] - 2s 57us/step - loss: 0.9727 - acc: 0.6011 - val_loss: 1.0700 - val_acc: 0.5913\n",
      "Epoch 27/80\n",
      "29639/29639 [==============================] - 2s 57us/step - loss: 0.9199 - acc: 0.6294 - val_loss: 0.9784 - val_acc: 0.6392\n",
      "Epoch 28/80\n",
      "29639/29639 [==============================] - 2s 57us/step - loss: 0.8889 - acc: 0.6478 - val_loss: 0.9847 - val_acc: 0.6291\n",
      "Epoch 29/80\n",
      "29639/29639 [==============================] - 2s 57us/step - loss: 0.8733 - acc: 0.6555 - val_loss: 1.0071 - val_acc: 0.6050\n",
      "Epoch 30/80\n",
      "29639/29639 [==============================] - 2s 57us/step - loss: 0.8612 - acc: 0.6631 - val_loss: 0.9615 - val_acc: 0.6553\n",
      "Epoch 31/80\n",
      "29639/29639 [==============================] - 2s 57us/step - loss: 0.8557 - acc: 0.6662 - val_loss: 0.9905 - val_acc: 0.6349\n",
      "Epoch 32/80\n",
      "29639/29639 [==============================] - 2s 56us/step - loss: 0.8494 - acc: 0.6723 - val_loss: 0.9793 - val_acc: 0.6403\n",
      "Epoch 33/80\n",
      "29639/29639 [==============================] - 2s 56us/step - loss: 0.8444 - acc: 0.6743 - val_loss: 0.9730 - val_acc: 0.6433\n",
      "Epoch 34/80\n",
      "29639/29639 [==============================] - 2s 57us/step - loss: 0.8377 - acc: 0.6763 - val_loss: 1.0017 - val_acc: 0.6209\n",
      "Epoch 35/80\n",
      "29639/29639 [==============================] - 2s 59us/step - loss: 0.8263 - acc: 0.6753 - val_loss: 0.9890 - val_acc: 0.6311\n",
      "Training model 2 / 4\n",
      "Inner step: 1 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 3s 98us/step - loss: 1.5425 - acc: 0.2843 - val_loss: 1.5062 - val_acc: 0.3095\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 2s 62us/step - loss: 1.5062 - acc: 0.3044 - val_loss: 1.5070 - val_acc: 0.3068\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 2s 62us/step - loss: 1.5040 - acc: 0.3062 - val_loss: 1.5061 - val_acc: 0.3079\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 2s 61us/step - loss: 1.4753 - acc: 0.3426 - val_loss: 1.4464 - val_acc: 0.3862\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 2s 61us/step - loss: 1.3878 - acc: 0.3903 - val_loss: 1.3390 - val_acc: 0.4040\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 2s 63us/step - loss: 1.2491 - acc: 0.4466 - val_loss: 1.3036 - val_acc: 0.4393\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 2s 62us/step - loss: 1.1641 - acc: 0.4943 - val_loss: 1.1871 - val_acc: 0.5051\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 2s 62us/step - loss: 1.1023 - acc: 0.5423 - val_loss: 1.2240 - val_acc: 0.4929\n",
      "Epoch 9/80\n",
      "29639/29639 [==============================] - 2s 61us/step - loss: 1.0177 - acc: 0.5887 - val_loss: 1.0815 - val_acc: 0.5729\n",
      "Epoch 10/80\n",
      "29639/29639 [==============================] - 2s 63us/step - loss: 0.9648 - acc: 0.6134 - val_loss: 1.1037 - val_acc: 0.5641\n",
      "Epoch 11/80\n",
      "29639/29639 [==============================] - 2s 63us/step - loss: 0.9454 - acc: 0.6254 - val_loss: 1.0805 - val_acc: 0.5790\n",
      "Epoch 12/80\n",
      "29639/29639 [==============================] - 2s 64us/step - loss: 0.9269 - acc: 0.6387 - val_loss: 1.0205 - val_acc: 0.6104\n",
      "Epoch 13/80\n",
      "29639/29639 [==============================] - 2s 63us/step - loss: 0.9012 - acc: 0.6578 - val_loss: 1.0441 - val_acc: 0.6001\n",
      "Epoch 14/80\n",
      "29639/29639 [==============================] - 2s 61us/step - loss: 0.8724 - acc: 0.6726 - val_loss: 0.9918 - val_acc: 0.6259\n",
      "Epoch 15/80\n",
      "29639/29639 [==============================] - 2s 62us/step - loss: 0.8390 - acc: 0.6909 - val_loss: 1.0014 - val_acc: 0.6222\n",
      "Epoch 16/80\n",
      "29639/29639 [==============================] - 2s 62us/step - loss: 0.8144 - acc: 0.7066 - val_loss: 0.9625 - val_acc: 0.6573\n",
      "Epoch 17/80\n",
      "29639/29639 [==============================] - 2s 63us/step - loss: 0.7923 - acc: 0.7165 - val_loss: 0.9130 - val_acc: 0.6831\n",
      "Epoch 18/80\n",
      "29639/29639 [==============================] - 2s 63us/step - loss: 0.7657 - acc: 0.7331 - val_loss: 0.9046 - val_acc: 0.6703\n",
      "Epoch 19/80\n",
      "29639/29639 [==============================] - 2s 62us/step - loss: 0.7421 - acc: 0.7435 - val_loss: 0.9244 - val_acc: 0.6690\n",
      "Epoch 20/80\n",
      "29639/29639 [==============================] - 2s 62us/step - loss: 0.7274 - acc: 0.7478 - val_loss: 0.8839 - val_acc: 0.6882\n",
      "Epoch 21/80\n",
      "29639/29639 [==============================] - 2s 62us/step - loss: 0.7174 - acc: 0.7520 - val_loss: 0.8731 - val_acc: 0.6941\n",
      "Epoch 22/80\n",
      "29639/29639 [==============================] - 2s 62us/step - loss: 0.7075 - acc: 0.7556 - val_loss: 0.8901 - val_acc: 0.6790\n",
      "Epoch 23/80\n",
      "29639/29639 [==============================] - 2s 61us/step - loss: 0.6914 - acc: 0.7621 - val_loss: 0.8753 - val_acc: 0.6930\n",
      "Epoch 24/80\n",
      "29639/29639 [==============================] - 2s 62us/step - loss: 0.6841 - acc: 0.7655 - val_loss: 0.7920 - val_acc: 0.7299\n",
      "Epoch 25/80\n",
      "29639/29639 [==============================] - 2s 62us/step - loss: 0.6829 - acc: 0.7652 - val_loss: 0.8062 - val_acc: 0.7280\n",
      "Epoch 26/80\n",
      "29639/29639 [==============================] - 2s 63us/step - loss: 0.6779 - acc: 0.7680 - val_loss: 0.8218 - val_acc: 0.7295\n",
      "Epoch 27/80\n",
      "29639/29639 [==============================] - 2s 62us/step - loss: 0.6666 - acc: 0.7722 - val_loss: 0.8057 - val_acc: 0.7235\n",
      "Epoch 28/80\n",
      "29639/29639 [==============================] - 2s 62us/step - loss: 0.6654 - acc: 0.7733 - val_loss: 0.8103 - val_acc: 0.7329\n",
      "Epoch 29/80\n",
      "29639/29639 [==============================] - 2s 65us/step - loss: 0.6531 - acc: 0.7769 - val_loss: 0.8264 - val_acc: 0.7237\n",
      "Inner step: 2 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 3s 104us/step - loss: 1.5000 - acc: 0.3025 - val_loss: 1.4487 - val_acc: 0.3484\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 2s 64us/step - loss: 1.3931 - acc: 0.3927 - val_loss: 1.3871 - val_acc: 0.4170\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 2s 64us/step - loss: 1.3257 - acc: 0.4403 - val_loss: 1.2856 - val_acc: 0.4608\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 2s 63us/step - loss: 1.2585 - acc: 0.4634 - val_loss: 1.2571 - val_acc: 0.4634\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 2s 62us/step - loss: 1.2186 - acc: 0.4836 - val_loss: 1.2244 - val_acc: 0.4979\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 2s 60us/step - loss: 1.1776 - acc: 0.5161 - val_loss: 1.2071 - val_acc: 0.4969\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 2s 62us/step - loss: 1.1255 - acc: 0.5350 - val_loss: 1.1607 - val_acc: 0.5389\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 2s 63us/step - loss: 1.1070 - acc: 0.5401 - val_loss: 1.1485 - val_acc: 0.5278\n",
      "Epoch 9/80\n",
      "29639/29639 [==============================] - 2s 63us/step - loss: 1.0924 - acc: 0.5436 - val_loss: 1.1623 - val_acc: 0.5343\n",
      "Epoch 10/80\n",
      "29639/29639 [==============================] - 2s 64us/step - loss: 1.0859 - acc: 0.5487 - val_loss: 1.1603 - val_acc: 0.5400\n",
      "Epoch 11/80\n",
      "29639/29639 [==============================] - 2s 61us/step - loss: 1.0765 - acc: 0.5528 - val_loss: 1.1297 - val_acc: 0.5409\n",
      "Epoch 12/80\n",
      "29639/29639 [==============================] - 2s 66us/step - loss: 1.0650 - acc: 0.5580 - val_loss: 1.1451 - val_acc: 0.5380\n",
      "Epoch 13/80\n",
      "29639/29639 [==============================] - 2s 63us/step - loss: 1.0556 - acc: 0.5632 - val_loss: 1.1139 - val_acc: 0.5497\n",
      "Epoch 14/80\n",
      "29639/29639 [==============================] - 2s 61us/step - loss: 1.0481 - acc: 0.5724 - val_loss: 1.1276 - val_acc: 0.5629\n",
      "Epoch 15/80\n",
      "29639/29639 [==============================] - 2s 68us/step - loss: 1.0260 - acc: 0.5866 - val_loss: 1.0791 - val_acc: 0.5875\n",
      "Epoch 16/80\n",
      "29639/29639 [==============================] - 2s 63us/step - loss: 0.9970 - acc: 0.6108 - val_loss: 1.0742 - val_acc: 0.5992\n",
      "Epoch 17/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29639/29639 [==============================] - 2s 62us/step - loss: 0.9699 - acc: 0.6288 - val_loss: 1.0312 - val_acc: 0.6117\n",
      "Epoch 18/80\n",
      "29639/29639 [==============================] - 2s 64us/step - loss: 0.9473 - acc: 0.6373 - val_loss: 1.0456 - val_acc: 0.6157\n",
      "Epoch 19/80\n",
      "29639/29639 [==============================] - 2s 60us/step - loss: 0.9243 - acc: 0.6503 - val_loss: 1.0142 - val_acc: 0.6195\n",
      "Epoch 20/80\n",
      "29639/29639 [==============================] - 2s 60us/step - loss: 0.9157 - acc: 0.6518 - val_loss: 1.0792 - val_acc: 0.6007\n",
      "Epoch 21/80\n",
      "29639/29639 [==============================] - 2s 67us/step - loss: 0.9123 - acc: 0.6562 - val_loss: 1.0050 - val_acc: 0.6312\n",
      "Epoch 22/80\n",
      "29639/29639 [==============================] - 2s 71us/step - loss: 0.8917 - acc: 0.6667 - val_loss: 0.9874 - val_acc: 0.6348\n",
      "Epoch 23/80\n",
      "29639/29639 [==============================] - 2s 63us/step - loss: 0.8795 - acc: 0.6739 - val_loss: 0.9899 - val_acc: 0.6325\n",
      "Epoch 24/80\n",
      "29639/29639 [==============================] - 2s 62us/step - loss: 0.8629 - acc: 0.6811 - val_loss: 0.9448 - val_acc: 0.6510\n",
      "Epoch 25/80\n",
      "29639/29639 [==============================] - 2s 66us/step - loss: 0.8309 - acc: 0.7001 - val_loss: 1.0005 - val_acc: 0.6541\n",
      "Epoch 26/80\n",
      "29639/29639 [==============================] - 2s 64us/step - loss: 0.8082 - acc: 0.7113 - val_loss: 0.9182 - val_acc: 0.6667\n",
      "Epoch 27/80\n",
      "29639/29639 [==============================] - 2s 62us/step - loss: 0.7818 - acc: 0.7230 - val_loss: 0.9555 - val_acc: 0.6717\n",
      "Epoch 28/80\n",
      "29639/29639 [==============================] - 2s 61us/step - loss: 0.7583 - acc: 0.7355 - val_loss: 0.8785 - val_acc: 0.7018\n",
      "Epoch 29/80\n",
      "29639/29639 [==============================] - 2s 61us/step - loss: 0.7378 - acc: 0.7506 - val_loss: 0.8858 - val_acc: 0.6997\n",
      "Epoch 30/80\n",
      "29639/29639 [==============================] - 2s 63us/step - loss: 0.7144 - acc: 0.7614 - val_loss: 0.8612 - val_acc: 0.7122\n",
      "Epoch 31/80\n",
      "29639/29639 [==============================] - 2s 62us/step - loss: 0.7033 - acc: 0.7629 - val_loss: 0.8435 - val_acc: 0.7240\n",
      "Epoch 32/80\n",
      "29639/29639 [==============================] - 2s 61us/step - loss: 0.6895 - acc: 0.7690 - val_loss: 0.8693 - val_acc: 0.7014\n",
      "Epoch 33/80\n",
      "29639/29639 [==============================] - 2s 70us/step - loss: 0.6760 - acc: 0.7746 - val_loss: 0.8513 - val_acc: 0.7228\n",
      "Epoch 34/80\n",
      "29639/29639 [==============================] - 2s 63us/step - loss: 0.6673 - acc: 0.7799 - val_loss: 0.8715 - val_acc: 0.7055\n",
      "Epoch 35/80\n",
      "29639/29639 [==============================] - 2s 61us/step - loss: 0.6593 - acc: 0.7816 - val_loss: 0.8266 - val_acc: 0.7207\n",
      "Epoch 36/80\n",
      "29639/29639 [==============================] - 2s 65us/step - loss: 0.6521 - acc: 0.7842 - val_loss: 0.7892 - val_acc: 0.7486\n",
      "Epoch 37/80\n",
      "29639/29639 [==============================] - 2s 61us/step - loss: 0.6514 - acc: 0.7827 - val_loss: 0.8172 - val_acc: 0.7359\n",
      "Epoch 38/80\n",
      "29639/29639 [==============================] - 2s 60us/step - loss: 0.6391 - acc: 0.7876 - val_loss: 0.8222 - val_acc: 0.7300\n",
      "Epoch 39/80\n",
      "29639/29639 [==============================] - 2s 67us/step - loss: 0.6311 - acc: 0.7931 - val_loss: 0.7770 - val_acc: 0.7487\n",
      "Epoch 40/80\n",
      "29639/29639 [==============================] - 2s 61us/step - loss: 0.6256 - acc: 0.7930 - val_loss: 0.7867 - val_acc: 0.7396\n",
      "Epoch 41/80\n",
      "29639/29639 [==============================] - 2s 62us/step - loss: 0.6239 - acc: 0.7932 - val_loss: 0.8177 - val_acc: 0.7298\n",
      "Epoch 42/80\n",
      "29639/29639 [==============================] - 2s 62us/step - loss: 0.6200 - acc: 0.7958 - val_loss: 0.8101 - val_acc: 0.7364\n",
      "Epoch 43/80\n",
      "29639/29639 [==============================] - 2s 64us/step - loss: 0.6136 - acc: 0.7978 - val_loss: 0.7898 - val_acc: 0.7486\n",
      "Epoch 44/80\n",
      "29639/29639 [==============================] - 2s 63us/step - loss: 0.6060 - acc: 0.8008 - val_loss: 0.8010 - val_acc: 0.7463\n",
      "Inner step: 3 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 3s 105us/step - loss: 1.5495 - acc: 0.2872 - val_loss: 1.5171 - val_acc: 0.2872\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 2s 67us/step - loss: 1.5109 - acc: 0.3033 - val_loss: 1.5234 - val_acc: 0.3112\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 2s 67us/step - loss: 1.4801 - acc: 0.3224 - val_loss: 1.4286 - val_acc: 0.3652\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 2s 66us/step - loss: 1.4148 - acc: 0.3613 - val_loss: 1.4276 - val_acc: 0.3673\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 2s 66us/step - loss: 1.4023 - acc: 0.3627 - val_loss: 1.4243 - val_acc: 0.3611\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 2s 65us/step - loss: 1.3921 - acc: 0.3665 - val_loss: 1.4154 - val_acc: 0.3751\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 2s 66us/step - loss: 1.3767 - acc: 0.3900 - val_loss: 1.4075 - val_acc: 0.4011\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 2s 65us/step - loss: 1.3589 - acc: 0.4121 - val_loss: 1.4201 - val_acc: 0.3958\n",
      "Epoch 9/80\n",
      "29639/29639 [==============================] - 2s 69us/step - loss: 1.3518 - acc: 0.4103 - val_loss: 1.4195 - val_acc: 0.3920\n",
      "Epoch 10/80\n",
      "29639/29639 [==============================] - 2s 68us/step - loss: 1.3365 - acc: 0.4255 - val_loss: 1.3870 - val_acc: 0.4116\n",
      "Epoch 11/80\n",
      "29639/29639 [==============================] - 2s 64us/step - loss: 1.2624 - acc: 0.4842 - val_loss: 1.2578 - val_acc: 0.4975\n",
      "Epoch 12/80\n",
      "29639/29639 [==============================] - 2s 69us/step - loss: 1.1733 - acc: 0.5183 - val_loss: 1.2148 - val_acc: 0.5065\n",
      "Epoch 13/80\n",
      "29639/29639 [==============================] - 2s 61us/step - loss: 1.1247 - acc: 0.5380 - val_loss: 1.1850 - val_acc: 0.5240\n",
      "Epoch 14/80\n",
      "29639/29639 [==============================] - 2s 65us/step - loss: 1.0928 - acc: 0.5506 - val_loss: 1.1643 - val_acc: 0.5384\n",
      "Epoch 15/80\n",
      "29639/29639 [==============================] - 2s 62us/step - loss: 1.0584 - acc: 0.5717 - val_loss: 1.1431 - val_acc: 0.5619\n",
      "Epoch 16/80\n",
      "29639/29639 [==============================] - 2s 65us/step - loss: 1.0362 - acc: 0.5852 - val_loss: 1.0987 - val_acc: 0.5747\n",
      "Epoch 17/80\n",
      "29639/29639 [==============================] - 2s 66us/step - loss: 1.0198 - acc: 0.5964 - val_loss: 1.0915 - val_acc: 0.5757\n",
      "Epoch 18/80\n",
      "29639/29639 [==============================] - 2s 62us/step - loss: 0.9988 - acc: 0.6099 - val_loss: 1.1338 - val_acc: 0.5771\n",
      "Epoch 19/80\n",
      "29639/29639 [==============================] - 2s 59us/step - loss: 0.9908 - acc: 0.6152 - val_loss: 1.0599 - val_acc: 0.5991\n",
      "Epoch 20/80\n",
      "29639/29639 [==============================] - 2s 59us/step - loss: 0.9726 - acc: 0.6246 - val_loss: 1.0894 - val_acc: 0.5780\n",
      "Epoch 21/80\n",
      "29639/29639 [==============================] - 2s 65us/step - loss: 0.9627 - acc: 0.6271 - val_loss: 1.0882 - val_acc: 0.5994\n",
      "Epoch 22/80\n",
      "29639/29639 [==============================] - 2s 67us/step - loss: 0.9586 - acc: 0.6288 - val_loss: 1.0257 - val_acc: 0.6112\n",
      "Epoch 23/80\n",
      "29639/29639 [==============================] - 2s 61us/step - loss: 0.9517 - acc: 0.6349 - val_loss: 1.0311 - val_acc: 0.6150\n",
      "Epoch 24/80\n",
      "29639/29639 [==============================] - 2s 67us/step - loss: 0.9440 - acc: 0.6380 - val_loss: 1.0381 - val_acc: 0.6092\n",
      "Epoch 25/80\n",
      "29639/29639 [==============================] - 2s 66us/step - loss: 0.9320 - acc: 0.6474 - val_loss: 1.0255 - val_acc: 0.6325\n",
      "Epoch 26/80\n",
      "29639/29639 [==============================] - 2s 66us/step - loss: 0.9201 - acc: 0.6571 - val_loss: 0.9863 - val_acc: 0.6359\n",
      "Epoch 27/80\n",
      "29639/29639 [==============================] - 2s 60us/step - loss: 0.9075 - acc: 0.6644 - val_loss: 0.9875 - val_acc: 0.6543\n",
      "Epoch 28/80\n",
      "29639/29639 [==============================] - 2s 60us/step - loss: 0.8790 - acc: 0.6836 - val_loss: 0.9418 - val_acc: 0.6652\n",
      "Epoch 29/80\n",
      "29639/29639 [==============================] - 2s 60us/step - loss: 0.8349 - acc: 0.7068 - val_loss: 0.9423 - val_acc: 0.6750\n",
      "Epoch 30/80\n",
      "29639/29639 [==============================] - 2s 59us/step - loss: 0.7944 - acc: 0.7258 - val_loss: 0.9063 - val_acc: 0.6872\n",
      "Epoch 31/80\n",
      "29639/29639 [==============================] - 2s 61us/step - loss: 0.7758 - acc: 0.7343 - val_loss: 0.9082 - val_acc: 0.6916\n",
      "Epoch 32/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29639/29639 [==============================] - 2s 70us/step - loss: 0.7585 - acc: 0.7414 - val_loss: 0.8495 - val_acc: 0.7129\n",
      "Epoch 33/80\n",
      "29639/29639 [==============================] - 2s 71us/step - loss: 0.7349 - acc: 0.7529 - val_loss: 0.8491 - val_acc: 0.7096\n",
      "Epoch 34/80\n",
      "29639/29639 [==============================] - 2s 73us/step - loss: 0.7188 - acc: 0.7574 - val_loss: 0.9669 - val_acc: 0.6811\n",
      "Epoch 35/80\n",
      "29639/29639 [==============================] - 2s 66us/step - loss: 0.7127 - acc: 0.7621 - val_loss: 0.8420 - val_acc: 0.7234\n",
      "Epoch 36/80\n",
      "29639/29639 [==============================] - 2s 65us/step - loss: 0.7045 - acc: 0.7633 - val_loss: 0.8005 - val_acc: 0.7370\n",
      "Epoch 37/80\n",
      "29639/29639 [==============================] - 2s 62us/step - loss: 0.6954 - acc: 0.7682 - val_loss: 0.8476 - val_acc: 0.7106\n",
      "Epoch 38/80\n",
      "29639/29639 [==============================] - 2s 67us/step - loss: 0.6889 - acc: 0.7709 - val_loss: 0.8266 - val_acc: 0.7268\n",
      "Epoch 39/80\n",
      "29639/29639 [==============================] - 2s 69us/step - loss: 0.6834 - acc: 0.7718 - val_loss: 0.8056 - val_acc: 0.7327\n",
      "Epoch 40/80\n",
      "29639/29639 [==============================] - 2s 71us/step - loss: 0.6723 - acc: 0.7761 - val_loss: 0.8090 - val_acc: 0.7312\n",
      "Epoch 41/80\n",
      "29639/29639 [==============================] - 2s 60us/step - loss: 0.6617 - acc: 0.7793 - val_loss: 0.8015 - val_acc: 0.7369\n",
      "Inner step: 4 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 3s 108us/step - loss: 1.3788 - acc: 0.3910 - val_loss: 1.2347 - val_acc: 0.4773\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 2s 69us/step - loss: 1.1766 - acc: 0.4876 - val_loss: 1.2174 - val_acc: 0.4940\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 2s 61us/step - loss: 1.1399 - acc: 0.5162 - val_loss: 1.1973 - val_acc: 0.4979\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 2s 65us/step - loss: 1.1160 - acc: 0.5312 - val_loss: 1.2128 - val_acc: 0.5116\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 2s 70us/step - loss: 1.0886 - acc: 0.5439 - val_loss: 1.0965 - val_acc: 0.5601\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 2s 70us/step - loss: 1.0350 - acc: 0.5684 - val_loss: 1.1000 - val_acc: 0.5657\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 2s 69us/step - loss: 1.0015 - acc: 0.5898 - val_loss: 1.0501 - val_acc: 0.5885\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 2s 64us/step - loss: 0.9791 - acc: 0.6038 - val_loss: 1.0654 - val_acc: 0.5857\n",
      "Epoch 9/80\n",
      "29639/29639 [==============================] - 2s 65us/step - loss: 0.9657 - acc: 0.6132 - val_loss: 1.0386 - val_acc: 0.6016\n",
      "Epoch 10/80\n",
      "29639/29639 [==============================] - 2s 66us/step - loss: 0.9529 - acc: 0.6189 - val_loss: 1.0437 - val_acc: 0.5985\n",
      "Epoch 11/80\n",
      "29639/29639 [==============================] - 2s 66us/step - loss: 0.9495 - acc: 0.6276 - val_loss: 1.0101 - val_acc: 0.6159\n",
      "Epoch 12/80\n",
      "29639/29639 [==============================] - 2s 66us/step - loss: 0.9324 - acc: 0.6345 - val_loss: 1.0029 - val_acc: 0.6235\n",
      "Epoch 13/80\n",
      "29639/29639 [==============================] - 2s 65us/step - loss: 0.9137 - acc: 0.6478 - val_loss: 1.0037 - val_acc: 0.6148\n",
      "Epoch 14/80\n",
      "29639/29639 [==============================] - 2s 68us/step - loss: 0.8957 - acc: 0.6563 - val_loss: 0.9896 - val_acc: 0.6401\n",
      "Epoch 15/80\n",
      "29639/29639 [==============================] - 2s 65us/step - loss: 0.8815 - acc: 0.6655 - val_loss: 0.9809 - val_acc: 0.6401\n",
      "Epoch 16/80\n",
      "29639/29639 [==============================] - 2s 62us/step - loss: 0.8695 - acc: 0.6727 - val_loss: 0.9546 - val_acc: 0.6518\n",
      "Epoch 17/80\n",
      "29639/29639 [==============================] - 2s 67us/step - loss: 0.8577 - acc: 0.6772 - val_loss: 0.9822 - val_acc: 0.6427\n",
      "Epoch 18/80\n",
      "29639/29639 [==============================] - 2s 63us/step - loss: 0.8505 - acc: 0.6825 - val_loss: 0.9591 - val_acc: 0.6560\n",
      "Epoch 19/80\n",
      "29639/29639 [==============================] - 2s 64us/step - loss: 0.8244 - acc: 0.7014 - val_loss: 0.9411 - val_acc: 0.6676\n",
      "Epoch 20/80\n",
      "29639/29639 [==============================] - 2s 65us/step - loss: 0.8067 - acc: 0.7159 - val_loss: 0.9121 - val_acc: 0.6823\n",
      "Epoch 21/80\n",
      "29639/29639 [==============================] - 2s 68us/step - loss: 0.7824 - acc: 0.7306 - val_loss: 0.8939 - val_acc: 0.6930\n",
      "Epoch 22/80\n",
      "29639/29639 [==============================] - 2s 65us/step - loss: 0.7604 - acc: 0.7412 - val_loss: 0.9093 - val_acc: 0.6875\n",
      "Epoch 23/80\n",
      "29639/29639 [==============================] - 2s 65us/step - loss: 0.7497 - acc: 0.7458 - val_loss: 0.8889 - val_acc: 0.6944\n",
      "Epoch 24/80\n",
      "29639/29639 [==============================] - 2s 63us/step - loss: 0.7370 - acc: 0.7479 - val_loss: 0.8639 - val_acc: 0.7014\n",
      "Epoch 25/80\n",
      "29639/29639 [==============================] - 2s 67us/step - loss: 0.7272 - acc: 0.7547 - val_loss: 0.8689 - val_acc: 0.6997\n",
      "Epoch 26/80\n",
      "29639/29639 [==============================] - 2s 67us/step - loss: 0.7194 - acc: 0.7546 - val_loss: 0.8678 - val_acc: 0.7056\n",
      "Epoch 27/80\n",
      "29639/29639 [==============================] - 2s 65us/step - loss: 0.7078 - acc: 0.7585 - val_loss: 0.8789 - val_acc: 0.7039\n",
      "Epoch 28/80\n",
      "29639/29639 [==============================] - 2s 65us/step - loss: 0.6984 - acc: 0.7636 - val_loss: 0.8320 - val_acc: 0.7161\n",
      "Epoch 29/80\n",
      "29639/29639 [==============================] - 2s 66us/step - loss: 0.6934 - acc: 0.7624 - val_loss: 0.8510 - val_acc: 0.7111\n",
      "Epoch 30/80\n",
      "29639/29639 [==============================] - 2s 65us/step - loss: 0.6799 - acc: 0.7684 - val_loss: 0.9190 - val_acc: 0.6777\n",
      "Epoch 31/80\n",
      "29639/29639 [==============================] - 2s 66us/step - loss: 0.6783 - acc: 0.7675 - val_loss: 0.8303 - val_acc: 0.7166\n",
      "Epoch 32/80\n",
      "29639/29639 [==============================] - 2s 65us/step - loss: 0.6659 - acc: 0.7714 - val_loss: 0.8920 - val_acc: 0.6926\n",
      "Epoch 33/80\n",
      "29639/29639 [==============================] - 2s 65us/step - loss: 0.6561 - acc: 0.7757 - val_loss: 0.8224 - val_acc: 0.7232\n",
      "Epoch 34/80\n",
      "29639/29639 [==============================] - 2s 63us/step - loss: 0.6559 - acc: 0.7737 - val_loss: 0.8077 - val_acc: 0.7250\n",
      "Epoch 35/80\n",
      "29639/29639 [==============================] - 2s 63us/step - loss: 0.6547 - acc: 0.7736 - val_loss: 0.8346 - val_acc: 0.7096\n",
      "Epoch 36/80\n",
      "29639/29639 [==============================] - 2s 64us/step - loss: 0.6402 - acc: 0.7806 - val_loss: 0.8397 - val_acc: 0.7137\n",
      "Epoch 37/80\n",
      "29639/29639 [==============================] - 2s 68us/step - loss: 0.6359 - acc: 0.7822 - val_loss: 0.8417 - val_acc: 0.7145\n",
      "Epoch 38/80\n",
      "29639/29639 [==============================] - 2s 62us/step - loss: 0.6319 - acc: 0.7828 - val_loss: 0.8300 - val_acc: 0.7203\n",
      "Epoch 39/80\n",
      "29639/29639 [==============================] - 2s 67us/step - loss: 0.6252 - acc: 0.7852 - val_loss: 0.8193 - val_acc: 0.7195\n",
      "Inner step: 5 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 3s 102us/step - loss: 1.5553 - acc: 0.2773 - val_loss: 1.5336 - val_acc: 0.3042\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 2s 60us/step - loss: 1.5069 - acc: 0.3035 - val_loss: 1.5420 - val_acc: 0.3036\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 2s 60us/step - loss: 1.5025 - acc: 0.3058 - val_loss: 1.5245 - val_acc: 0.3067\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 2s 74us/step - loss: 1.4999 - acc: 0.3078 - val_loss: 1.5172 - val_acc: 0.3059\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 2s 64us/step - loss: 1.4997 - acc: 0.3074 - val_loss: 1.5172 - val_acc: 0.3052\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 2s 63us/step - loss: 1.4986 - acc: 0.3072 - val_loss: 1.5199 - val_acc: 0.3051\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 2s 69us/step - loss: 1.4996 - acc: 0.3054 - val_loss: 1.5214 - val_acc: 0.3031\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 2s 61us/step - loss: 1.4997 - acc: 0.3063 - val_loss: 1.5308 - val_acc: 0.3066\n",
      "Epoch 9/80\n",
      "29639/29639 [==============================] - 2s 62us/step - loss: 1.4978 - acc: 0.3072 - val_loss: 1.5218 - val_acc: 0.3066\n",
      "Inner step: 6 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29639/29639 [==============================] - 3s 108us/step - loss: 1.5378 - acc: 0.2872 - val_loss: 1.5212 - val_acc: 0.3059\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 2s 68us/step - loss: 1.5054 - acc: 0.3043 - val_loss: 1.5363 - val_acc: 0.3049\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 2s 63us/step - loss: 1.5022 - acc: 0.3051 - val_loss: 1.5394 - val_acc: 0.3056\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 2s 67us/step - loss: 1.5012 - acc: 0.3073 - val_loss: 1.5376 - val_acc: 0.3062\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 2s 65us/step - loss: 1.5007 - acc: 0.3082 - val_loss: 1.5290 - val_acc: 0.3055\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 2s 68us/step - loss: 1.4993 - acc: 0.3082 - val_loss: 1.5502 - val_acc: 0.3037\n",
      "Inner step: 7 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 3s 108us/step - loss: 1.3981 - acc: 0.3779 - val_loss: 1.3368 - val_acc: 0.4169\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 2s 66us/step - loss: 1.2900 - acc: 0.4224 - val_loss: 1.3305 - val_acc: 0.4141\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 2s 64us/step - loss: 1.2725 - acc: 0.4276 - val_loss: 1.3190 - val_acc: 0.4092\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 2s 71us/step - loss: 1.2391 - acc: 0.4530 - val_loss: 1.2442 - val_acc: 0.4790\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 2s 68us/step - loss: 1.1576 - acc: 0.5086 - val_loss: 1.1772 - val_acc: 0.5171\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 2s 71us/step - loss: 1.1241 - acc: 0.5274 - val_loss: 1.1769 - val_acc: 0.5219\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 2s 65us/step - loss: 1.1129 - acc: 0.5348 - val_loss: 1.1698 - val_acc: 0.5425\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 2s 67us/step - loss: 1.1074 - acc: 0.5401 - val_loss: 1.1606 - val_acc: 0.5442\n",
      "Epoch 9/80\n",
      "29639/29639 [==============================] - 2s 65us/step - loss: 1.1019 - acc: 0.5484 - val_loss: 1.1568 - val_acc: 0.5280\n",
      "Epoch 10/80\n",
      "29639/29639 [==============================] - 2s 62us/step - loss: 1.0889 - acc: 0.5527 - val_loss: 1.1374 - val_acc: 0.5534\n",
      "Epoch 11/80\n",
      "29639/29639 [==============================] - 2s 68us/step - loss: 1.0691 - acc: 0.5674 - val_loss: 1.1360 - val_acc: 0.5547\n",
      "Epoch 12/80\n",
      "29639/29639 [==============================] - 2s 63us/step - loss: 1.0457 - acc: 0.5804 - val_loss: 1.1089 - val_acc: 0.5561\n",
      "Epoch 13/80\n",
      "29639/29639 [==============================] - 2s 64us/step - loss: 1.0125 - acc: 0.5989 - val_loss: 1.1054 - val_acc: 0.5830\n",
      "Epoch 14/80\n",
      "29639/29639 [==============================] - 2s 65us/step - loss: 0.9824 - acc: 0.6136 - val_loss: 1.0573 - val_acc: 0.5969\n",
      "Epoch 15/80\n",
      "29639/29639 [==============================] - 2s 69us/step - loss: 0.9742 - acc: 0.6185 - val_loss: 1.1149 - val_acc: 0.5827\n",
      "Epoch 16/80\n",
      "29639/29639 [==============================] - 2s 68us/step - loss: 0.9491 - acc: 0.6351 - val_loss: 1.0207 - val_acc: 0.6343\n",
      "Epoch 17/80\n",
      "29639/29639 [==============================] - 3s 85us/step - loss: 0.9037 - acc: 0.6669 - val_loss: 1.0209 - val_acc: 0.6389\n",
      "Epoch 18/80\n",
      "29639/29639 [==============================] - 2s 83us/step - loss: 0.8718 - acc: 0.6841 - val_loss: 1.0198 - val_acc: 0.6355\n",
      "Epoch 19/80\n",
      "29639/29639 [==============================] - 2s 76us/step - loss: 0.8481 - acc: 0.6967 - val_loss: 0.9537 - val_acc: 0.6617\n",
      "Epoch 20/80\n",
      "29639/29639 [==============================] - 2s 62us/step - loss: 0.8305 - acc: 0.7032 - val_loss: 0.9891 - val_acc: 0.6492\n",
      "Epoch 21/80\n",
      "29639/29639 [==============================] - 2s 67us/step - loss: 0.8179 - acc: 0.7118 - val_loss: 1.0157 - val_acc: 0.6526\n",
      "Epoch 22/80\n",
      "29639/29639 [==============================] - 2s 65us/step - loss: 0.8005 - acc: 0.7200 - val_loss: 0.9210 - val_acc: 0.6836\n",
      "Epoch 23/80\n",
      "29639/29639 [==============================] - 2s 63us/step - loss: 0.7859 - acc: 0.7279 - val_loss: 1.0155 - val_acc: 0.6378\n",
      "Epoch 24/80\n",
      "29639/29639 [==============================] - 2s 61us/step - loss: 0.7691 - acc: 0.7363 - val_loss: 0.9263 - val_acc: 0.6805\n",
      "Epoch 25/80\n",
      "29639/29639 [==============================] - 2s 61us/step - loss: 0.7599 - acc: 0.7407 - val_loss: 0.8782 - val_acc: 0.6974\n",
      "Epoch 26/80\n",
      "29639/29639 [==============================] - 2s 63us/step - loss: 0.7380 - acc: 0.7480 - val_loss: 0.8910 - val_acc: 0.6929\n",
      "Epoch 27/80\n",
      "29639/29639 [==============================] - 2s 61us/step - loss: 0.7228 - acc: 0.7546 - val_loss: 0.8618 - val_acc: 0.7048\n",
      "Epoch 28/80\n",
      "29639/29639 [==============================] - 2s 63us/step - loss: 0.7188 - acc: 0.7547 - val_loss: 0.9480 - val_acc: 0.6686\n",
      "Epoch 29/80\n",
      "29639/29639 [==============================] - 2s 64us/step - loss: 0.7009 - acc: 0.7608 - val_loss: 0.8745 - val_acc: 0.7060\n",
      "Epoch 30/80\n",
      "29639/29639 [==============================] - 2s 61us/step - loss: 0.7000 - acc: 0.7626 - val_loss: 0.8885 - val_acc: 0.7068\n",
      "Epoch 31/80\n",
      "29639/29639 [==============================] - 2s 61us/step - loss: 0.6943 - acc: 0.7619 - val_loss: 0.8570 - val_acc: 0.7046\n",
      "Epoch 32/80\n",
      "29639/29639 [==============================] - 2s 62us/step - loss: 0.6891 - acc: 0.7649 - val_loss: 0.8377 - val_acc: 0.7135\n",
      "Epoch 33/80\n",
      "29639/29639 [==============================] - 2s 61us/step - loss: 0.6851 - acc: 0.7670 - val_loss: 0.8182 - val_acc: 0.7210\n",
      "Epoch 34/80\n",
      "29639/29639 [==============================] - 2s 61us/step - loss: 0.6726 - acc: 0.7691 - val_loss: 0.8232 - val_acc: 0.7186\n",
      "Epoch 35/80\n",
      "29639/29639 [==============================] - 2s 61us/step - loss: 0.6662 - acc: 0.7731 - val_loss: 0.8010 - val_acc: 0.7222\n",
      "Epoch 36/80\n",
      "29639/29639 [==============================] - 2s 61us/step - loss: 0.6632 - acc: 0.7707 - val_loss: 0.8038 - val_acc: 0.7216\n",
      "Epoch 37/80\n",
      "29639/29639 [==============================] - 2s 61us/step - loss: 0.6567 - acc: 0.7760 - val_loss: 0.8065 - val_acc: 0.7211\n",
      "Epoch 38/80\n",
      "29639/29639 [==============================] - 2s 67us/step - loss: 0.6539 - acc: 0.7741 - val_loss: 0.8451 - val_acc: 0.7155\n",
      "Epoch 39/80\n",
      "29639/29639 [==============================] - 2s 66us/step - loss: 0.6609 - acc: 0.7748 - val_loss: 0.8183 - val_acc: 0.7164\n",
      "Epoch 40/80\n",
      "29639/29639 [==============================] - 2s 66us/step - loss: 0.6507 - acc: 0.7776 - val_loss: 0.8099 - val_acc: 0.7247\n",
      "Inner step: 8 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 3s 109us/step - loss: 1.5485 - acc: 0.2806 - val_loss: 1.5092 - val_acc: 0.3053\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 2s 67us/step - loss: 1.5068 - acc: 0.3051 - val_loss: 1.5078 - val_acc: 0.2960\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 2s 68us/step - loss: 1.4468 - acc: 0.3423 - val_loss: 1.3925 - val_acc: 0.3857\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 2s 75us/step - loss: 1.3548 - acc: 0.4208 - val_loss: 1.3070 - val_acc: 0.4917\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 2s 71us/step - loss: 1.2090 - acc: 0.5078 - val_loss: 1.2280 - val_acc: 0.5105\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 2s 64us/step - loss: 1.1645 - acc: 0.5254 - val_loss: 1.1665 - val_acc: 0.5276\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 2s 68us/step - loss: 1.1209 - acc: 0.5396 - val_loss: 1.1908 - val_acc: 0.5278\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 2s 66us/step - loss: 1.1080 - acc: 0.5426 - val_loss: 1.1651 - val_acc: 0.5344\n",
      "Epoch 9/80\n",
      "29639/29639 [==============================] - 2s 73us/step - loss: 1.0947 - acc: 0.5456 - val_loss: 1.1933 - val_acc: 0.5287\n",
      "Epoch 10/80\n",
      "29639/29639 [==============================] - 2s 75us/step - loss: 1.0875 - acc: 0.5497 - val_loss: 1.1781 - val_acc: 0.5337\n",
      "Epoch 11/80\n",
      "29639/29639 [==============================] - 2s 72us/step - loss: 1.0778 - acc: 0.5501 - val_loss: 1.2905 - val_acc: 0.5163\n",
      "Epoch 12/80\n",
      "29639/29639 [==============================] - 2s 63us/step - loss: 1.0743 - acc: 0.5502 - val_loss: 1.1497 - val_acc: 0.5449\n",
      "Epoch 13/80\n",
      "29639/29639 [==============================] - 2s 63us/step - loss: 1.0651 - acc: 0.5548 - val_loss: 1.1250 - val_acc: 0.5493\n",
      "Epoch 14/80\n",
      "29639/29639 [==============================] - 2s 64us/step - loss: 1.0547 - acc: 0.5627 - val_loss: 1.1062 - val_acc: 0.5559\n",
      "Epoch 15/80\n",
      "29639/29639 [==============================] - 2s 63us/step - loss: 1.0282 - acc: 0.5856 - val_loss: 1.0910 - val_acc: 0.5822\n",
      "Epoch 16/80\n",
      "29639/29639 [==============================] - 2s 63us/step - loss: 0.9936 - acc: 0.6084 - val_loss: 1.0515 - val_acc: 0.6018\n",
      "Epoch 17/80\n",
      "29639/29639 [==============================] - 2s 63us/step - loss: 0.9674 - acc: 0.6231 - val_loss: 1.0542 - val_acc: 0.6022\n",
      "Epoch 18/80\n",
      "29639/29639 [==============================] - 2s 63us/step - loss: 0.9549 - acc: 0.6315 - val_loss: 1.0474 - val_acc: 0.6109\n",
      "Epoch 19/80\n",
      "29639/29639 [==============================] - 2s 63us/step - loss: 0.9410 - acc: 0.6369 - val_loss: 1.0648 - val_acc: 0.6075\n",
      "Epoch 20/80\n",
      "29639/29639 [==============================] - 2s 66us/step - loss: 0.9295 - acc: 0.6438 - val_loss: 1.0824 - val_acc: 0.6019\n",
      "Epoch 21/80\n",
      "29639/29639 [==============================] - 2s 66us/step - loss: 0.9214 - acc: 0.6489 - val_loss: 1.0278 - val_acc: 0.6195\n",
      "Epoch 22/80\n",
      "29639/29639 [==============================] - 2s 68us/step - loss: 0.9126 - acc: 0.6545 - val_loss: 1.0588 - val_acc: 0.6141\n",
      "Epoch 23/80\n",
      "29639/29639 [==============================] - 2s 65us/step - loss: 0.9038 - acc: 0.6590 - val_loss: 1.0098 - val_acc: 0.6326\n",
      "Epoch 24/80\n",
      "29639/29639 [==============================] - 2s 66us/step - loss: 0.8942 - acc: 0.6678 - val_loss: 0.9985 - val_acc: 0.6393\n",
      "Epoch 25/80\n",
      "29639/29639 [==============================] - 2s 68us/step - loss: 0.8881 - acc: 0.6742 - val_loss: 1.0167 - val_acc: 0.6361\n",
      "Epoch 26/80\n",
      "29639/29639 [==============================] - 2s 69us/step - loss: 0.8731 - acc: 0.6837 - val_loss: 1.0204 - val_acc: 0.6433\n",
      "Epoch 27/80\n",
      "29639/29639 [==============================] - 2s 61us/step - loss: 0.8666 - acc: 0.6896 - val_loss: 0.9956 - val_acc: 0.6492\n",
      "Epoch 28/80\n",
      "29639/29639 [==============================] - 2s 66us/step - loss: 0.8364 - acc: 0.7029 - val_loss: 0.9360 - val_acc: 0.6731\n",
      "Epoch 29/80\n",
      "29639/29639 [==============================] - 2s 66us/step - loss: 0.8088 - acc: 0.7151 - val_loss: 0.9252 - val_acc: 0.6797\n",
      "Epoch 30/80\n",
      "29639/29639 [==============================] - 2s 66us/step - loss: 0.7837 - acc: 0.7279 - val_loss: 0.9340 - val_acc: 0.6801\n",
      "Epoch 31/80\n",
      "29639/29639 [==============================] - 2s 69us/step - loss: 0.7546 - acc: 0.7405 - val_loss: 0.8393 - val_acc: 0.7120\n",
      "Epoch 32/80\n",
      "29639/29639 [==============================] - 2s 64us/step - loss: 0.7247 - acc: 0.7529 - val_loss: 0.8243 - val_acc: 0.7238\n",
      "Epoch 33/80\n",
      "29639/29639 [==============================] - 2s 69us/step - loss: 0.7018 - acc: 0.7638 - val_loss: 0.8296 - val_acc: 0.7198\n",
      "Epoch 34/80\n",
      "29639/29639 [==============================] - 2s 64us/step - loss: 0.6851 - acc: 0.7699 - val_loss: 0.8215 - val_acc: 0.7345\n",
      "Epoch 35/80\n",
      "29639/29639 [==============================] - 2s 64us/step - loss: 0.6731 - acc: 0.7732 - val_loss: 0.8310 - val_acc: 0.7190\n",
      "Epoch 36/80\n",
      "29639/29639 [==============================] - 2s 72us/step - loss: 0.6621 - acc: 0.7791 - val_loss: 0.7599 - val_acc: 0.7488\n",
      "Epoch 37/80\n",
      "29639/29639 [==============================] - 2s 66us/step - loss: 0.6483 - acc: 0.7836 - val_loss: 0.8262 - val_acc: 0.7300\n",
      "Epoch 38/80\n",
      "29639/29639 [==============================] - 2s 63us/step - loss: 0.6392 - acc: 0.7880 - val_loss: 0.7756 - val_acc: 0.7465\n",
      "Epoch 39/80\n",
      "29639/29639 [==============================] - 2s 64us/step - loss: 0.6398 - acc: 0.7859 - val_loss: 0.8573 - val_acc: 0.7228\n",
      "Epoch 40/80\n",
      "29639/29639 [==============================] - 2s 62us/step - loss: 0.6323 - acc: 0.7909 - val_loss: 0.7978 - val_acc: 0.7458\n",
      "Epoch 41/80\n",
      "29639/29639 [==============================] - 2s 62us/step - loss: 0.6282 - acc: 0.7918 - val_loss: 0.7932 - val_acc: 0.7403\n",
      "Inner step: 9 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 3s 109us/step - loss: 1.5950 - acc: 0.2478 - val_loss: 1.5929 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 2s 67us/step - loss: 1.5949 - acc: 0.2499 - val_loss: 1.5937 - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 2s 66us/step - loss: 1.5951 - acc: 0.2456 - val_loss: 1.5936 - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 2s 66us/step - loss: 1.5527 - acc: 0.2734 - val_loss: 1.5473 - val_acc: 0.2998\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 2s 67us/step - loss: 1.5057 - acc: 0.3050 - val_loss: 1.5297 - val_acc: 0.3060\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 2s 67us/step - loss: 1.5018 - acc: 0.3057 - val_loss: 1.5277 - val_acc: 0.3039\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 2s 70us/step - loss: 1.4989 - acc: 0.3090 - val_loss: 1.5325 - val_acc: 0.3070\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 2s 65us/step - loss: 1.5002 - acc: 0.3071 - val_loss: 1.5217 - val_acc: 0.3049\n",
      "Epoch 9/80\n",
      "29639/29639 [==============================] - 2s 63us/step - loss: 1.4996 - acc: 0.3072 - val_loss: 1.5270 - val_acc: 0.3057\n",
      "Epoch 10/80\n",
      "29639/29639 [==============================] - 2s 63us/step - loss: 1.4979 - acc: 0.3081 - val_loss: 1.5220 - val_acc: 0.3048\n",
      "Epoch 11/80\n",
      "29639/29639 [==============================] - 2s 63us/step - loss: 1.4984 - acc: 0.3082 - val_loss: 1.5260 - val_acc: 0.3057\n",
      "Epoch 12/80\n",
      "29639/29639 [==============================] - 2s 65us/step - loss: 1.4984 - acc: 0.3073 - val_loss: 1.5212 - val_acc: 0.3049\n",
      "Epoch 13/80\n",
      "29639/29639 [==============================] - 2s 67us/step - loss: 1.4979 - acc: 0.3084 - val_loss: 1.5403 - val_acc: 0.3074\n",
      "Inner step: 10 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 3s 109us/step - loss: 1.5343 - acc: 0.2880 - val_loss: 1.5303 - val_acc: 0.3010\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 2s 67us/step - loss: 1.5031 - acc: 0.3075 - val_loss: 1.5919 - val_acc: 0.2938\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 2s 67us/step - loss: 1.5026 - acc: 0.3056 - val_loss: 1.5233 - val_acc: 0.3042\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 2s 64us/step - loss: 1.4997 - acc: 0.3077 - val_loss: 1.5343 - val_acc: 0.3068\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 2s 62us/step - loss: 1.4991 - acc: 0.3073 - val_loss: 1.5379 - val_acc: 0.3022\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 2s 62us/step - loss: 1.4982 - acc: 0.3085 - val_loss: 1.5296 - val_acc: 0.3075\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 2s 63us/step - loss: 1.4982 - acc: 0.3083 - val_loss: 1.5309 - val_acc: 0.3070\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 2s 64us/step - loss: 1.4885 - acc: 0.3201 - val_loss: 1.5254 - val_acc: 0.3307\n",
      "Inner step: 11 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 4s 132us/step - loss: 1.5385 - acc: 0.2828 - val_loss: 1.5414 - val_acc: 0.3013\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 2s 64us/step - loss: 1.5042 - acc: 0.3041 - val_loss: 1.5199 - val_acc: 0.3015\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 2s 70us/step - loss: 1.5042 - acc: 0.3058 - val_loss: 1.5252 - val_acc: 0.3067\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 2s 68us/step - loss: 1.5007 - acc: 0.3073 - val_loss: 1.5197 - val_acc: 0.3051\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 2s 65us/step - loss: 1.4998 - acc: 0.3058 - val_loss: 1.5254 - val_acc: 0.3057\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 2s 64us/step - loss: 1.4996 - acc: 0.3076 - val_loss: 1.5191 - val_acc: 0.3054\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 2s 73us/step - loss: 1.4995 - acc: 0.3081 - val_loss: 1.5298 - val_acc: 0.3056\n",
      "Inner step: 12 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29639/29639 [==============================] - 3s 114us/step - loss: 1.5366 - acc: 0.2887 - val_loss: 1.5956 - val_acc: 0.2812\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 2s 70us/step - loss: 1.4833 - acc: 0.3216 - val_loss: 1.4016 - val_acc: 0.3754\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 2s 65us/step - loss: 1.3560 - acc: 0.4038 - val_loss: 1.3879 - val_acc: 0.3878\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 2s 65us/step - loss: 1.3410 - acc: 0.4081 - val_loss: 1.3690 - val_acc: 0.4031\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 2s 65us/step - loss: 1.3391 - acc: 0.4066 - val_loss: 1.3633 - val_acc: 0.4029\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 2s 67us/step - loss: 1.3294 - acc: 0.4124 - val_loss: 1.3820 - val_acc: 0.3842\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 2s 64us/step - loss: 1.3312 - acc: 0.4085 - val_loss: 1.3837 - val_acc: 0.3912\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 2s 67us/step - loss: 1.3258 - acc: 0.4099 - val_loss: 1.3882 - val_acc: 0.3908\n",
      "Epoch 9/80\n",
      "29639/29639 [==============================] - 2s 64us/step - loss: 1.3263 - acc: 0.4093 - val_loss: 1.3817 - val_acc: 0.4042\n",
      "Epoch 10/80\n",
      "29639/29639 [==============================] - 2s 66us/step - loss: 1.3225 - acc: 0.4116 - val_loss: 1.3659 - val_acc: 0.4009\n",
      "Inner step: 13 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 3s 112us/step - loss: 1.5434 - acc: 0.2824 - val_loss: 1.5086 - val_acc: 0.3115\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 2s 67us/step - loss: 1.5058 - acc: 0.3053 - val_loss: 1.5040 - val_acc: 0.3083\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 2s 66us/step - loss: 1.5047 - acc: 0.3060 - val_loss: 1.5071 - val_acc: 0.3053\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 2s 66us/step - loss: 1.4597 - acc: 0.3415 - val_loss: 1.4005 - val_acc: 0.3906\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 2s 66us/step - loss: 1.3193 - acc: 0.4295 - val_loss: 1.2557 - val_acc: 0.4928\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 2s 67us/step - loss: 1.2162 - acc: 0.4998 - val_loss: 1.2617 - val_acc: 0.5115\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 2s 66us/step - loss: 1.1982 - acc: 0.5119 - val_loss: 1.2484 - val_acc: 0.4896\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 2s 66us/step - loss: 1.1669 - acc: 0.5249 - val_loss: 1.1826 - val_acc: 0.5276\n",
      "Epoch 9/80\n",
      "29639/29639 [==============================] - 2s 67us/step - loss: 1.1376 - acc: 0.5360 - val_loss: 1.1602 - val_acc: 0.5357\n",
      "Epoch 10/80\n",
      "29639/29639 [==============================] - 2s 68us/step - loss: 1.1090 - acc: 0.5431 - val_loss: 1.1458 - val_acc: 0.5392\n",
      "Epoch 11/80\n",
      "29639/29639 [==============================] - 2s 68us/step - loss: 1.0971 - acc: 0.5426 - val_loss: 1.1485 - val_acc: 0.5285\n",
      "Epoch 12/80\n",
      "29639/29639 [==============================] - 2s 68us/step - loss: 1.0887 - acc: 0.5469 - val_loss: 1.1399 - val_acc: 0.5421\n",
      "Epoch 13/80\n",
      "29639/29639 [==============================] - 2s 68us/step - loss: 1.0866 - acc: 0.5452 - val_loss: 1.1587 - val_acc: 0.5357\n",
      "Epoch 14/80\n",
      "29639/29639 [==============================] - 2s 69us/step - loss: 1.0795 - acc: 0.5517 - val_loss: 1.1361 - val_acc: 0.5390\n",
      "Epoch 15/80\n",
      "29639/29639 [==============================] - 2s 66us/step - loss: 1.0756 - acc: 0.5495 - val_loss: 1.1384 - val_acc: 0.5395\n",
      "Epoch 16/80\n",
      "29639/29639 [==============================] - 2s 67us/step - loss: 1.0702 - acc: 0.5529 - val_loss: 1.1803 - val_acc: 0.5335\n",
      "Epoch 17/80\n",
      "29639/29639 [==============================] - 2s 66us/step - loss: 1.0722 - acc: 0.5532 - val_loss: 1.1446 - val_acc: 0.5432\n",
      "Epoch 18/80\n",
      "29639/29639 [==============================] - 2s 66us/step - loss: 1.0649 - acc: 0.5562 - val_loss: 1.1502 - val_acc: 0.5366\n",
      "Epoch 19/80\n",
      "29639/29639 [==============================] - 2s 70us/step - loss: 1.0628 - acc: 0.5571 - val_loss: 1.1384 - val_acc: 0.5389\n",
      "Inner step: 14 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 3s 114us/step - loss: 1.4012 - acc: 0.3701 - val_loss: 1.3283 - val_acc: 0.4081\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 2s 66us/step - loss: 1.2788 - acc: 0.4365 - val_loss: 1.3150 - val_acc: 0.4614\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 2s 68us/step - loss: 1.1940 - acc: 0.4996 - val_loss: 1.2000 - val_acc: 0.5204\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 2s 68us/step - loss: 1.1346 - acc: 0.5278 - val_loss: 1.1727 - val_acc: 0.5324\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 2s 68us/step - loss: 1.1231 - acc: 0.5338 - val_loss: 1.1582 - val_acc: 0.5294\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 2s 68us/step - loss: 1.1187 - acc: 0.5391 - val_loss: 1.1564 - val_acc: 0.5326\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 2s 64us/step - loss: 1.1071 - acc: 0.5437 - val_loss: 1.1612 - val_acc: 0.5354\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 2s 65us/step - loss: 1.1076 - acc: 0.5434 - val_loss: 1.1625 - val_acc: 0.5329\n",
      "Epoch 9/80\n",
      "29639/29639 [==============================] - 2s 64us/step - loss: 1.0811 - acc: 0.5553 - val_loss: 1.1050 - val_acc: 0.5549\n",
      "Epoch 10/80\n",
      "29639/29639 [==============================] - 2s 64us/step - loss: 1.0257 - acc: 0.5790 - val_loss: 1.0941 - val_acc: 0.5608\n",
      "Epoch 11/80\n",
      "29639/29639 [==============================] - 2s 64us/step - loss: 0.9971 - acc: 0.5963 - val_loss: 1.1085 - val_acc: 0.5633\n",
      "Epoch 12/80\n",
      "29639/29639 [==============================] - 2s 66us/step - loss: 0.9825 - acc: 0.6061 - val_loss: 1.0609 - val_acc: 0.5942\n",
      "Epoch 13/80\n",
      "29639/29639 [==============================] - 2s 69us/step - loss: 0.9757 - acc: 0.6098 - val_loss: 1.0507 - val_acc: 0.5873\n",
      "Epoch 14/80\n",
      "29639/29639 [==============================] - 2s 64us/step - loss: 0.9580 - acc: 0.6204 - val_loss: 1.0626 - val_acc: 0.5885\n",
      "Epoch 15/80\n",
      "29639/29639 [==============================] - 2s 68us/step - loss: 0.9504 - acc: 0.6262 - val_loss: 1.0523 - val_acc: 0.6003\n",
      "Epoch 16/80\n",
      "29639/29639 [==============================] - 2s 68us/step - loss: 0.9361 - acc: 0.6311 - val_loss: 1.0795 - val_acc: 0.5888\n",
      "Epoch 17/80\n",
      "29639/29639 [==============================] - 2s 69us/step - loss: 0.9243 - acc: 0.6378 - val_loss: 1.0478 - val_acc: 0.6009\n",
      "Epoch 18/80\n",
      "29639/29639 [==============================] - 2s 66us/step - loss: 0.9034 - acc: 0.6527 - val_loss: 0.9872 - val_acc: 0.6301\n",
      "Epoch 19/80\n",
      "29639/29639 [==============================] - 2s 67us/step - loss: 0.8912 - acc: 0.6628 - val_loss: 1.0186 - val_acc: 0.6205\n",
      "Epoch 20/80\n",
      "29639/29639 [==============================] - 2s 67us/step - loss: 0.8703 - acc: 0.6746 - val_loss: 0.9876 - val_acc: 0.6364\n",
      "Epoch 21/80\n",
      "29639/29639 [==============================] - 2s 67us/step - loss: 0.8549 - acc: 0.6845 - val_loss: 0.9382 - val_acc: 0.6670\n",
      "Epoch 22/80\n",
      "29639/29639 [==============================] - 2s 64us/step - loss: 0.8493 - acc: 0.6926 - val_loss: 0.9391 - val_acc: 0.6659\n",
      "Epoch 23/80\n",
      "29639/29639 [==============================] - 2s 66us/step - loss: 0.8138 - acc: 0.7128 - val_loss: 0.9311 - val_acc: 0.6782\n",
      "Epoch 24/80\n",
      "29639/29639 [==============================] - 2s 65us/step - loss: 0.7921 - acc: 0.7253 - val_loss: 0.9460 - val_acc: 0.6701\n",
      "Epoch 25/80\n",
      "29639/29639 [==============================] - 2s 66us/step - loss: 0.7757 - acc: 0.7341 - val_loss: 0.8956 - val_acc: 0.6979\n",
      "Epoch 26/80\n",
      "29639/29639 [==============================] - 2s 67us/step - loss: 0.7622 - acc: 0.7410 - val_loss: 0.9037 - val_acc: 0.6935\n",
      "Epoch 27/80\n",
      "29639/29639 [==============================] - 2s 71us/step - loss: 0.7526 - acc: 0.7451 - val_loss: 0.8930 - val_acc: 0.6993\n",
      "Epoch 28/80\n",
      "29639/29639 [==============================] - 2s 69us/step - loss: 0.7417 - acc: 0.7488 - val_loss: 0.9162 - val_acc: 0.6931\n",
      "Epoch 29/80\n",
      "29639/29639 [==============================] - 2s 65us/step - loss: 0.7264 - acc: 0.7548 - val_loss: 0.8969 - val_acc: 0.7031\n",
      "Epoch 30/80\n",
      "29639/29639 [==============================] - 2s 65us/step - loss: 0.7246 - acc: 0.7560 - val_loss: 0.8568 - val_acc: 0.7117\n",
      "Epoch 31/80\n",
      "29639/29639 [==============================] - 2s 64us/step - loss: 0.7309 - acc: 0.7543 - val_loss: 0.9107 - val_acc: 0.6820\n",
      "Epoch 32/80\n",
      "29639/29639 [==============================] - 2s 68us/step - loss: 0.7075 - acc: 0.7622 - val_loss: 0.8488 - val_acc: 0.7152\n",
      "Epoch 33/80\n",
      "29639/29639 [==============================] - 2s 69us/step - loss: 0.7052 - acc: 0.7614 - val_loss: 0.8501 - val_acc: 0.7168\n",
      "Epoch 34/80\n",
      "29639/29639 [==============================] - 2s 70us/step - loss: 0.6951 - acc: 0.7668 - val_loss: 0.8515 - val_acc: 0.7259\n",
      "Epoch 35/80\n",
      "29639/29639 [==============================] - 2s 73us/step - loss: 0.6849 - acc: 0.7692 - val_loss: 0.8388 - val_acc: 0.7179\n",
      "Epoch 36/80\n",
      "29639/29639 [==============================] - 2s 68us/step - loss: 0.6850 - acc: 0.7685 - val_loss: 0.8543 - val_acc: 0.7128\n",
      "Epoch 37/80\n",
      "29639/29639 [==============================] - 2s 67us/step - loss: 0.6708 - acc: 0.7733 - val_loss: 0.8295 - val_acc: 0.7204\n",
      "Epoch 38/80\n",
      "29639/29639 [==============================] - 2s 66us/step - loss: 0.6682 - acc: 0.7767 - val_loss: 0.8722 - val_acc: 0.7105\n",
      "Epoch 39/80\n",
      "29639/29639 [==============================] - 2s 66us/step - loss: 0.6628 - acc: 0.7769 - val_loss: 0.8088 - val_acc: 0.7273\n",
      "Epoch 40/80\n",
      "29639/29639 [==============================] - 2s 64us/step - loss: 0.6589 - acc: 0.7764 - val_loss: 0.8321 - val_acc: 0.7107\n",
      "Epoch 41/80\n",
      "29639/29639 [==============================] - 2s 64us/step - loss: 0.6488 - acc: 0.7802 - val_loss: 0.7813 - val_acc: 0.7351\n",
      "Epoch 42/80\n",
      "29639/29639 [==============================] - 2s 63us/step - loss: 0.6391 - acc: 0.7838 - val_loss: 0.7835 - val_acc: 0.7403\n",
      "Epoch 43/80\n",
      "29639/29639 [==============================] - 2s 63us/step - loss: 0.6431 - acc: 0.7835 - val_loss: 0.8025 - val_acc: 0.7351\n",
      "Epoch 44/80\n",
      "29639/29639 [==============================] - 2s 63us/step - loss: 0.6390 - acc: 0.7858 - val_loss: 0.7978 - val_acc: 0.7373\n",
      "Epoch 45/80\n",
      "29639/29639 [==============================] - 2s 63us/step - loss: 0.6347 - acc: 0.7875 - val_loss: 0.7979 - val_acc: 0.7326\n",
      "Epoch 46/80\n",
      "29639/29639 [==============================] - 2s 63us/step - loss: 0.6290 - acc: 0.7890 - val_loss: 0.8432 - val_acc: 0.7213\n",
      "Inner step: 15 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 3s 116us/step - loss: 1.5716 - acc: 0.2620 - val_loss: 1.5099 - val_acc: 0.3048\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 2s 69us/step - loss: 1.4453 - acc: 0.3411 - val_loss: 1.4265 - val_acc: 0.3552\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 2s 69us/step - loss: 1.3808 - acc: 0.3869 - val_loss: 1.4279 - val_acc: 0.4058\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 2s 69us/step - loss: 1.3632 - acc: 0.4048 - val_loss: 1.3971 - val_acc: 0.3941\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 2s 68us/step - loss: 1.3525 - acc: 0.4078 - val_loss: 1.3909 - val_acc: 0.4046\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 2s 65us/step - loss: 1.2680 - acc: 0.4778 - val_loss: 1.3276 - val_acc: 0.4842\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 2s 65us/step - loss: 1.1900 - acc: 0.5133 - val_loss: 1.2398 - val_acc: 0.5092\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 2s 67us/step - loss: 1.1553 - acc: 0.5257 - val_loss: 1.1847 - val_acc: 0.5244\n",
      "Epoch 9/80\n",
      "29639/29639 [==============================] - 2s 66us/step - loss: 1.1143 - acc: 0.5378 - val_loss: 1.1524 - val_acc: 0.5293\n",
      "Epoch 10/80\n",
      "29639/29639 [==============================] - 2s 65us/step - loss: 1.0948 - acc: 0.5453 - val_loss: 1.1885 - val_acc: 0.5296\n",
      "Epoch 11/80\n",
      "29639/29639 [==============================] - 2s 67us/step - loss: 1.0743 - acc: 0.5496 - val_loss: 1.1078 - val_acc: 0.5497\n",
      "Epoch 12/80\n",
      "29639/29639 [==============================] - 2s 66us/step - loss: 1.0307 - acc: 0.5648 - val_loss: 1.0504 - val_acc: 0.5727\n",
      "Epoch 13/80\n",
      "29639/29639 [==============================] - 2s 65us/step - loss: 0.9876 - acc: 0.5856 - val_loss: 1.0618 - val_acc: 0.5720\n",
      "Epoch 14/80\n",
      "29639/29639 [==============================] - 2s 65us/step - loss: 0.9648 - acc: 0.6072 - val_loss: 1.0331 - val_acc: 0.5966\n",
      "Epoch 15/80\n",
      "29639/29639 [==============================] - 2s 65us/step - loss: 0.9302 - acc: 0.6312 - val_loss: 1.0451 - val_acc: 0.6118\n",
      "Epoch 16/80\n",
      "29639/29639 [==============================] - 2s 64us/step - loss: 0.8924 - acc: 0.6512 - val_loss: 0.9719 - val_acc: 0.6366\n",
      "Epoch 17/80\n",
      "29639/29639 [==============================] - 2s 65us/step - loss: 0.8609 - acc: 0.6665 - val_loss: 0.9308 - val_acc: 0.6500\n",
      "Epoch 18/80\n",
      "29639/29639 [==============================] - 2s 66us/step - loss: 0.8259 - acc: 0.6842 - val_loss: 0.9255 - val_acc: 0.6662\n",
      "Epoch 19/80\n",
      "29639/29639 [==============================] - 2s 68us/step - loss: 0.7942 - acc: 0.7049 - val_loss: 0.8905 - val_acc: 0.6808\n",
      "Epoch 20/80\n",
      "29639/29639 [==============================] - 2s 67us/step - loss: 0.7412 - acc: 0.7357 - val_loss: 0.8957 - val_acc: 0.6739\n",
      "Epoch 21/80\n",
      "29639/29639 [==============================] - 2s 65us/step - loss: 0.7129 - acc: 0.7466 - val_loss: 0.8255 - val_acc: 0.7119\n",
      "Epoch 22/80\n",
      "29639/29639 [==============================] - 2s 66us/step - loss: 0.6996 - acc: 0.7541 - val_loss: 0.8383 - val_acc: 0.7074\n",
      "Epoch 23/80\n",
      "29639/29639 [==============================] - 2s 66us/step - loss: 0.6905 - acc: 0.7581 - val_loss: 0.8278 - val_acc: 0.7231\n",
      "Epoch 24/80\n",
      "29639/29639 [==============================] - 2s 67us/step - loss: 0.6774 - acc: 0.7617 - val_loss: 0.8573 - val_acc: 0.7105\n",
      "Epoch 25/80\n",
      "29639/29639 [==============================] - 2s 69us/step - loss: 0.6766 - acc: 0.7654 - val_loss: 0.8488 - val_acc: 0.7126\n",
      "Epoch 26/80\n",
      "29639/29639 [==============================] - 2s 71us/step - loss: 0.6671 - acc: 0.7683 - val_loss: 0.8222 - val_acc: 0.7105\n",
      "Epoch 27/80\n",
      "29639/29639 [==============================] - 2s 66us/step - loss: 0.6528 - acc: 0.7719 - val_loss: 0.8031 - val_acc: 0.7273\n",
      "Epoch 28/80\n",
      "29639/29639 [==============================] - 2s 69us/step - loss: 0.6446 - acc: 0.7785 - val_loss: 0.7863 - val_acc: 0.7381\n",
      "Epoch 29/80\n",
      "29639/29639 [==============================] - 2s 69us/step - loss: 0.6309 - acc: 0.7820 - val_loss: 0.8574 - val_acc: 0.7112\n",
      "Epoch 30/80\n",
      "29639/29639 [==============================] - 2s 68us/step - loss: 0.6157 - acc: 0.7891 - val_loss: 0.7861 - val_acc: 0.7397\n",
      "Epoch 31/80\n",
      "29639/29639 [==============================] - 2s 68us/step - loss: 0.6087 - acc: 0.7910 - val_loss: 0.7575 - val_acc: 0.7483\n",
      "Epoch 32/80\n",
      "29639/29639 [==============================] - 2s 68us/step - loss: 0.6067 - acc: 0.7922 - val_loss: 0.8881 - val_acc: 0.7154\n",
      "Epoch 33/80\n",
      "29639/29639 [==============================] - 2s 67us/step - loss: 0.5963 - acc: 0.7941 - val_loss: 0.7955 - val_acc: 0.7356\n",
      "Epoch 34/80\n",
      "29639/29639 [==============================] - 2s 64us/step - loss: 0.5816 - acc: 0.8004 - val_loss: 0.7518 - val_acc: 0.7501\n",
      "Epoch 35/80\n",
      "29639/29639 [==============================] - 2s 65us/step - loss: 0.5754 - acc: 0.8037 - val_loss: 0.7404 - val_acc: 0.7549\n",
      "Epoch 36/80\n",
      "29639/29639 [==============================] - 2s 68us/step - loss: 0.5714 - acc: 0.8046 - val_loss: 0.7335 - val_acc: 0.7613\n",
      "Epoch 37/80\n",
      "29639/29639 [==============================] - 2s 69us/step - loss: 0.5677 - acc: 0.8069 - val_loss: 0.7963 - val_acc: 0.7363\n",
      "Epoch 38/80\n",
      "29639/29639 [==============================] - 2s 69us/step - loss: 0.5626 - acc: 0.8089 - val_loss: 0.7692 - val_acc: 0.7532\n",
      "Epoch 39/80\n",
      "29639/29639 [==============================] - 2s 68us/step - loss: 0.5615 - acc: 0.8078 - val_loss: 0.7233 - val_acc: 0.7652\n",
      "Epoch 40/80\n",
      "29639/29639 [==============================] - 2s 70us/step - loss: 0.5539 - acc: 0.8104 - val_loss: 0.7210 - val_acc: 0.7691\n",
      "Epoch 41/80\n",
      "29639/29639 [==============================] - 2s 64us/step - loss: 0.5482 - acc: 0.8136 - val_loss: 0.7278 - val_acc: 0.7627\n",
      "Epoch 42/80\n",
      "29639/29639 [==============================] - 2s 64us/step - loss: 0.5451 - acc: 0.8136 - val_loss: 0.6954 - val_acc: 0.7747\n",
      "Epoch 43/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29639/29639 [==============================] - 2s 66us/step - loss: 0.5417 - acc: 0.8171 - val_loss: 0.7177 - val_acc: 0.7678\n",
      "Epoch 44/80\n",
      "29639/29639 [==============================] - 2s 77us/step - loss: 0.5368 - acc: 0.8167 - val_loss: 0.7168 - val_acc: 0.7676\n",
      "Epoch 45/80\n",
      "29639/29639 [==============================] - 2s 81us/step - loss: 0.5329 - acc: 0.8189 - val_loss: 0.7214 - val_acc: 0.7669\n",
      "Epoch 46/80\n",
      "29639/29639 [==============================] - 2s 75us/step - loss: 0.5305 - acc: 0.8206 - val_loss: 0.7200 - val_acc: 0.7711\n",
      "Epoch 47/80\n",
      "29639/29639 [==============================] - 2s 69us/step - loss: 0.5272 - acc: 0.8200 - val_loss: 0.7164 - val_acc: 0.7661\n",
      "Inner step: 16 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 3s 116us/step - loss: 1.5030 - acc: 0.3004 - val_loss: 1.4782 - val_acc: 0.3305\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 2s 65us/step - loss: 1.4007 - acc: 0.3849 - val_loss: 1.3929 - val_acc: 0.4095\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 2s 66us/step - loss: 1.3591 - acc: 0.4255 - val_loss: 1.4241 - val_acc: 0.4141\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 2s 65us/step - loss: 1.3463 - acc: 0.4342 - val_loss: 1.3731 - val_acc: 0.4256\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 2s 65us/step - loss: 1.3017 - acc: 0.4523 - val_loss: 1.2954 - val_acc: 0.4602\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 2s 68us/step - loss: 1.2427 - acc: 0.4658 - val_loss: 1.3030 - val_acc: 0.4545\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 2s 67us/step - loss: 1.2012 - acc: 0.4998 - val_loss: 1.2451 - val_acc: 0.5012\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 2s 72us/step - loss: 1.1589 - acc: 0.5242 - val_loss: 1.1494 - val_acc: 0.5331\n",
      "Epoch 9/80\n",
      "29639/29639 [==============================] - 2s 68us/step - loss: 1.1051 - acc: 0.5429 - val_loss: 1.2004 - val_acc: 0.5192\n",
      "Epoch 10/80\n",
      "29639/29639 [==============================] - 2s 65us/step - loss: 1.0885 - acc: 0.5462 - val_loss: 1.1500 - val_acc: 0.5302\n",
      "Epoch 11/80\n",
      "29639/29639 [==============================] - 2s 68us/step - loss: 1.0849 - acc: 0.5486 - val_loss: 1.2133 - val_acc: 0.5255\n",
      "Epoch 12/80\n",
      "29639/29639 [==============================] - 2s 65us/step - loss: 1.0723 - acc: 0.5568 - val_loss: 1.1331 - val_acc: 0.5434\n",
      "Epoch 13/80\n",
      "29639/29639 [==============================] - 2s 64us/step - loss: 1.0603 - acc: 0.5646 - val_loss: 1.1277 - val_acc: 0.5483\n",
      "Epoch 14/80\n",
      "29639/29639 [==============================] - 2s 64us/step - loss: 1.0441 - acc: 0.5836 - val_loss: 1.1091 - val_acc: 0.5643\n",
      "Epoch 15/80\n",
      "29639/29639 [==============================] - 2s 65us/step - loss: 1.0182 - acc: 0.6015 - val_loss: 1.0738 - val_acc: 0.5894\n",
      "Epoch 16/80\n",
      "29639/29639 [==============================] - 2s 71us/step - loss: 0.9637 - acc: 0.6405 - val_loss: 1.0539 - val_acc: 0.6112\n",
      "Epoch 17/80\n",
      "29639/29639 [==============================] - 2s 73us/step - loss: 0.9149 - acc: 0.6634 - val_loss: 1.0267 - val_acc: 0.6258\n",
      "Epoch 18/80\n",
      "29639/29639 [==============================] - 2s 72us/step - loss: 0.8876 - acc: 0.6799 - val_loss: 1.0011 - val_acc: 0.6408\n",
      "Epoch 19/80\n",
      "29639/29639 [==============================] - 2s 68us/step - loss: 0.8689 - acc: 0.6903 - val_loss: 0.9953 - val_acc: 0.6442\n",
      "Epoch 20/80\n",
      "29639/29639 [==============================] - 2s 64us/step - loss: 0.8569 - acc: 0.6966 - val_loss: 0.9750 - val_acc: 0.6538\n",
      "Epoch 21/80\n",
      "29639/29639 [==============================] - 2s 63us/step - loss: 0.8426 - acc: 0.7065 - val_loss: 0.9787 - val_acc: 0.6627\n",
      "Epoch 22/80\n",
      "29639/29639 [==============================] - 2s 70us/step - loss: 0.8361 - acc: 0.7098 - val_loss: 0.9794 - val_acc: 0.6573\n",
      "Epoch 23/80\n",
      "29639/29639 [==============================] - 2s 72us/step - loss: 0.8193 - acc: 0.7158 - val_loss: 0.9712 - val_acc: 0.6641\n",
      "Epoch 24/80\n",
      "29639/29639 [==============================] - 2s 70us/step - loss: 0.8114 - acc: 0.7194 - val_loss: 0.9489 - val_acc: 0.6659\n",
      "Epoch 25/80\n",
      "29639/29639 [==============================] - 2s 69us/step - loss: 0.7983 - acc: 0.7248 - val_loss: 0.9876 - val_acc: 0.6658\n",
      "Epoch 26/80\n",
      "29639/29639 [==============================] - 2s 71us/step - loss: 0.7911 - acc: 0.7287 - val_loss: 0.9229 - val_acc: 0.6798\n",
      "Epoch 27/80\n",
      "29639/29639 [==============================] - 2s 65us/step - loss: 0.7823 - acc: 0.7328 - val_loss: 0.9043 - val_acc: 0.6891\n",
      "Epoch 28/80\n",
      "29639/29639 [==============================] - 2s 62us/step - loss: 0.7712 - acc: 0.7347 - val_loss: 0.8864 - val_acc: 0.6974\n",
      "Epoch 29/80\n",
      "29639/29639 [==============================] - 2s 63us/step - loss: 0.7643 - acc: 0.7376 - val_loss: 0.9030 - val_acc: 0.6990\n",
      "Epoch 30/80\n",
      "29639/29639 [==============================] - 2s 63us/step - loss: 0.7512 - acc: 0.7422 - val_loss: 0.8788 - val_acc: 0.7017\n",
      "Epoch 31/80\n",
      "29639/29639 [==============================] - 2s 64us/step - loss: 0.7375 - acc: 0.7467 - val_loss: 0.8776 - val_acc: 0.6955\n",
      "Epoch 32/80\n",
      "29639/29639 [==============================] - 2s 74us/step - loss: 0.7187 - acc: 0.7558 - val_loss: 0.8919 - val_acc: 0.6976\n",
      "Epoch 33/80\n",
      "29639/29639 [==============================] - 2s 63us/step - loss: 0.7012 - acc: 0.7600 - val_loss: 0.8870 - val_acc: 0.6886\n",
      "Epoch 34/80\n",
      "29639/29639 [==============================] - 2s 63us/step - loss: 0.6818 - acc: 0.7700 - val_loss: 0.8384 - val_acc: 0.7097\n",
      "Epoch 35/80\n",
      "29639/29639 [==============================] - 2s 63us/step - loss: 0.6670 - acc: 0.7758 - val_loss: 0.8265 - val_acc: 0.7284\n",
      "Epoch 36/80\n",
      "29639/29639 [==============================] - 2s 62us/step - loss: 0.6545 - acc: 0.7800 - val_loss: 0.8345 - val_acc: 0.7246\n",
      "Epoch 37/80\n",
      "29639/29639 [==============================] - 2s 63us/step - loss: 0.6491 - acc: 0.7811 - val_loss: 0.8177 - val_acc: 0.7361\n",
      "Epoch 38/80\n",
      "29639/29639 [==============================] - 2s 64us/step - loss: 0.6440 - acc: 0.7848 - val_loss: 0.8014 - val_acc: 0.7384\n",
      "Epoch 39/80\n",
      "29639/29639 [==============================] - 2s 63us/step - loss: 0.6330 - acc: 0.7874 - val_loss: 0.8067 - val_acc: 0.7321\n",
      "Epoch 40/80\n",
      "29639/29639 [==============================] - 2s 63us/step - loss: 0.6283 - acc: 0.7916 - val_loss: 0.8125 - val_acc: 0.7288\n",
      "Epoch 41/80\n",
      "29639/29639 [==============================] - 2s 75us/step - loss: 0.6191 - acc: 0.7948 - val_loss: 0.8897 - val_acc: 0.7099\n",
      "Epoch 42/80\n",
      "29639/29639 [==============================] - 2s 69us/step - loss: 0.6104 - acc: 0.7977 - val_loss: 0.7753 - val_acc: 0.7441\n",
      "Epoch 43/80\n",
      "29639/29639 [==============================] - 2s 65us/step - loss: 0.6125 - acc: 0.7970 - val_loss: 0.8974 - val_acc: 0.7168\n",
      "Epoch 44/80\n",
      "29639/29639 [==============================] - 2s 63us/step - loss: 0.6060 - acc: 0.7986 - val_loss: 0.8344 - val_acc: 0.7317\n",
      "Epoch 45/80\n",
      "29639/29639 [==============================] - 2s 71us/step - loss: 0.6081 - acc: 0.7972 - val_loss: 0.7707 - val_acc: 0.7506\n",
      "Epoch 46/80\n",
      "29639/29639 [==============================] - 2s 69us/step - loss: 0.6006 - acc: 0.8017 - val_loss: 0.7657 - val_acc: 0.7509\n",
      "Epoch 47/80\n",
      "29639/29639 [==============================] - 2s 72us/step - loss: 0.5955 - acc: 0.8024 - val_loss: 0.8005 - val_acc: 0.7324\n",
      "Epoch 48/80\n",
      "29639/29639 [==============================] - 2s 69us/step - loss: 0.5933 - acc: 0.8044 - val_loss: 0.7813 - val_acc: 0.7467\n",
      "Epoch 49/80\n",
      "29639/29639 [==============================] - 2s 63us/step - loss: 0.5887 - acc: 0.8061 - val_loss: 0.7829 - val_acc: 0.7489\n",
      "Epoch 50/80\n",
      "29639/29639 [==============================] - 2s 66us/step - loss: 0.5841 - acc: 0.8078 - val_loss: 0.7605 - val_acc: 0.7535\n",
      "Epoch 51/80\n",
      "29639/29639 [==============================] - 2s 71us/step - loss: 0.5839 - acc: 0.8054 - val_loss: 0.8148 - val_acc: 0.7319\n",
      "Epoch 52/80\n",
      "29639/29639 [==============================] - 2s 67us/step - loss: 0.5839 - acc: 0.8068 - val_loss: 0.7641 - val_acc: 0.7520\n",
      "Epoch 53/80\n",
      "29639/29639 [==============================] - 2s 66us/step - loss: 0.5761 - acc: 0.8090 - val_loss: 0.7806 - val_acc: 0.7486\n",
      "Epoch 54/80\n",
      "29639/29639 [==============================] - 2s 63us/step - loss: 0.5741 - acc: 0.8101 - val_loss: 0.7497 - val_acc: 0.7626\n",
      "Epoch 55/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29639/29639 [==============================] - 2s 65us/step - loss: 0.5668 - acc: 0.8126 - val_loss: 0.8209 - val_acc: 0.7479\n",
      "Epoch 56/80\n",
      "29639/29639 [==============================] - 2s 63us/step - loss: 0.5651 - acc: 0.8137 - val_loss: 0.7611 - val_acc: 0.7622\n",
      "Epoch 57/80\n",
      "29639/29639 [==============================] - 2s 66us/step - loss: 0.5662 - acc: 0.8119 - val_loss: 0.7480 - val_acc: 0.7657\n",
      "Epoch 58/80\n",
      "29639/29639 [==============================] - 2s 66us/step - loss: 0.5646 - acc: 0.8146 - val_loss: 0.8783 - val_acc: 0.7373\n",
      "Epoch 59/80\n",
      "29639/29639 [==============================] - 2s 64us/step - loss: 0.5614 - acc: 0.8159 - val_loss: 0.7862 - val_acc: 0.7464\n",
      "Epoch 60/80\n",
      "29639/29639 [==============================] - 2s 63us/step - loss: 0.5546 - acc: 0.8166 - val_loss: 0.7591 - val_acc: 0.7640\n",
      "Epoch 61/80\n",
      "29639/29639 [==============================] - 2s 64us/step - loss: 0.5572 - acc: 0.8165 - val_loss: 0.8127 - val_acc: 0.7449\n",
      "Epoch 62/80\n",
      "29639/29639 [==============================] - 2s 63us/step - loss: 0.5504 - acc: 0.8179 - val_loss: 0.8065 - val_acc: 0.7435\n",
      "Inner step: 17 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 3s 117us/step - loss: 1.5435 - acc: 0.2834 - val_loss: 1.5168 - val_acc: 0.2880\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 2s 72us/step - loss: 1.5063 - acc: 0.3045 - val_loss: 1.5100 - val_acc: 0.3081\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 2s 70us/step - loss: 1.5050 - acc: 0.3061 - val_loss: 1.5111 - val_acc: 0.3097\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 2s 67us/step - loss: 1.5031 - acc: 0.3068 - val_loss: 1.5077 - val_acc: 0.3109\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 2s 73us/step - loss: 1.4628 - acc: 0.3441 - val_loss: 1.3729 - val_acc: 0.3936\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 2s 70us/step - loss: 1.2392 - acc: 0.4556 - val_loss: 1.2174 - val_acc: 0.4740\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 2s 67us/step - loss: 1.1738 - acc: 0.4783 - val_loss: 1.1999 - val_acc: 0.4767\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 2s 70us/step - loss: 1.1393 - acc: 0.5093 - val_loss: 1.1929 - val_acc: 0.5261\n",
      "Epoch 9/80\n",
      "29639/29639 [==============================] - 2s 68us/step - loss: 1.0750 - acc: 0.5508 - val_loss: 1.1148 - val_acc: 0.5472\n",
      "Epoch 10/80\n",
      "29639/29639 [==============================] - 2s 66us/step - loss: 1.0296 - acc: 0.5680 - val_loss: 1.0831 - val_acc: 0.5673\n",
      "Epoch 11/80\n",
      "29639/29639 [==============================] - 2s 65us/step - loss: 0.9887 - acc: 0.5860 - val_loss: 1.0797 - val_acc: 0.5736\n",
      "Epoch 12/80\n",
      "29639/29639 [==============================] - 2s 75us/step - loss: 0.9672 - acc: 0.6001 - val_loss: 1.1238 - val_acc: 0.5807\n",
      "Epoch 13/80\n",
      "29639/29639 [==============================] - 2s 69us/step - loss: 0.9462 - acc: 0.6165 - val_loss: 1.0309 - val_acc: 0.6011\n",
      "Epoch 14/80\n",
      "29639/29639 [==============================] - 2s 68us/step - loss: 0.9187 - acc: 0.6375 - val_loss: 1.0224 - val_acc: 0.6164\n",
      "Epoch 15/80\n",
      "29639/29639 [==============================] - 2s 66us/step - loss: 0.8744 - acc: 0.6686 - val_loss: 0.9606 - val_acc: 0.6498\n",
      "Epoch 16/80\n",
      "29639/29639 [==============================] - 2s 68us/step - loss: 0.8246 - acc: 0.7057 - val_loss: 0.9073 - val_acc: 0.6890\n",
      "Epoch 17/80\n",
      "29639/29639 [==============================] - 2s 67us/step - loss: 0.7892 - acc: 0.7265 - val_loss: 0.9003 - val_acc: 0.6900\n",
      "Epoch 18/80\n",
      "29639/29639 [==============================] - 2s 67us/step - loss: 0.7547 - acc: 0.7437 - val_loss: 0.8779 - val_acc: 0.6989\n",
      "Epoch 19/80\n",
      "29639/29639 [==============================] - 2s 66us/step - loss: 0.7368 - acc: 0.7500 - val_loss: 0.8773 - val_acc: 0.7083\n",
      "Epoch 20/80\n",
      "29639/29639 [==============================] - 2s 65us/step - loss: 0.7258 - acc: 0.7534 - val_loss: 0.9125 - val_acc: 0.6949\n",
      "Epoch 21/80\n",
      "29639/29639 [==============================] - 2s 66us/step - loss: 0.7104 - acc: 0.7612 - val_loss: 0.8511 - val_acc: 0.7135\n",
      "Epoch 22/80\n",
      "29639/29639 [==============================] - 2s 67us/step - loss: 0.7029 - acc: 0.7624 - val_loss: 0.9503 - val_acc: 0.6681\n",
      "Epoch 23/80\n",
      "29639/29639 [==============================] - 2s 66us/step - loss: 0.6899 - acc: 0.7666 - val_loss: 0.8355 - val_acc: 0.7202\n",
      "Epoch 24/80\n",
      "29639/29639 [==============================] - 2s 66us/step - loss: 0.6889 - acc: 0.7679 - val_loss: 0.8560 - val_acc: 0.7231\n",
      "Epoch 25/80\n",
      "29639/29639 [==============================] - 2s 68us/step - loss: 0.6686 - acc: 0.7739 - val_loss: 0.8119 - val_acc: 0.7262\n",
      "Epoch 26/80\n",
      "29639/29639 [==============================] - 2s 72us/step - loss: 0.6666 - acc: 0.7731 - val_loss: 0.7997 - val_acc: 0.7285\n",
      "Epoch 27/80\n",
      "29639/29639 [==============================] - 2s 67us/step - loss: 0.6576 - acc: 0.7768 - val_loss: 0.8259 - val_acc: 0.7174\n",
      "Epoch 28/80\n",
      "29639/29639 [==============================] - 2s 68us/step - loss: 0.6518 - acc: 0.7774 - val_loss: 0.8226 - val_acc: 0.7194\n",
      "Epoch 29/80\n",
      "29639/29639 [==============================] - 2s 65us/step - loss: 0.6458 - acc: 0.7802 - val_loss: 0.8574 - val_acc: 0.7151\n",
      "Epoch 30/80\n",
      "29639/29639 [==============================] - 2s 64us/step - loss: 0.6430 - acc: 0.7815 - val_loss: 0.8281 - val_acc: 0.7244\n",
      "Epoch 31/80\n",
      "29639/29639 [==============================] - 2s 64us/step - loss: 0.6334 - acc: 0.7844 - val_loss: 0.7813 - val_acc: 0.7371\n",
      "Epoch 32/80\n",
      "29639/29639 [==============================] - 2s 64us/step - loss: 0.6319 - acc: 0.7840 - val_loss: 0.7842 - val_acc: 0.7353\n",
      "Epoch 33/80\n",
      "29639/29639 [==============================] - 2s 64us/step - loss: 0.6240 - acc: 0.7872 - val_loss: 0.8041 - val_acc: 0.7207\n",
      "Epoch 34/80\n",
      "29639/29639 [==============================] - 2s 73us/step - loss: 0.6217 - acc: 0.7889 - val_loss: 0.7793 - val_acc: 0.7474\n",
      "Epoch 35/80\n",
      "29639/29639 [==============================] - 2s 75us/step - loss: 0.6279 - acc: 0.7845 - val_loss: 0.8272 - val_acc: 0.7124\n",
      "Epoch 36/80\n",
      "29639/29639 [==============================] - 2s 67us/step - loss: 0.6175 - acc: 0.7899 - val_loss: 0.7759 - val_acc: 0.7368\n",
      "Epoch 37/80\n",
      "29639/29639 [==============================] - 2s 67us/step - loss: 0.6120 - acc: 0.7908 - val_loss: 0.7724 - val_acc: 0.7461\n",
      "Epoch 38/80\n",
      "29639/29639 [==============================] - 2s 67us/step - loss: 0.6112 - acc: 0.7910 - val_loss: 0.8243 - val_acc: 0.7196\n",
      "Epoch 39/80\n",
      "29639/29639 [==============================] - 2s 65us/step - loss: 0.6042 - acc: 0.7954 - val_loss: 0.7771 - val_acc: 0.7343\n",
      "Epoch 40/80\n",
      "29639/29639 [==============================] - 2s 64us/step - loss: 0.6008 - acc: 0.7955 - val_loss: 0.7526 - val_acc: 0.7500\n",
      "Epoch 41/80\n",
      "29639/29639 [==============================] - 2s 64us/step - loss: 0.5979 - acc: 0.7946 - val_loss: 0.7854 - val_acc: 0.7390\n",
      "Epoch 42/80\n",
      "29639/29639 [==============================] - 2s 65us/step - loss: 0.5965 - acc: 0.7963 - val_loss: 0.7486 - val_acc: 0.7538\n",
      "Epoch 43/80\n",
      "29639/29639 [==============================] - 2s 71us/step - loss: 0.5908 - acc: 0.7995 - val_loss: 0.8045 - val_acc: 0.7386\n",
      "Epoch 44/80\n",
      "29639/29639 [==============================] - 2s 69us/step - loss: 0.5894 - acc: 0.7985 - val_loss: 0.7693 - val_acc: 0.7479\n",
      "Epoch 45/80\n",
      "29639/29639 [==============================] - 2s 69us/step - loss: 0.5836 - acc: 0.8023 - val_loss: 0.7632 - val_acc: 0.7558\n",
      "Epoch 46/80\n",
      "29639/29639 [==============================] - 2s 65us/step - loss: 0.5796 - acc: 0.8036 - val_loss: 0.7540 - val_acc: 0.7464\n",
      "Epoch 47/80\n",
      "29639/29639 [==============================] - 2s 67us/step - loss: 0.5805 - acc: 0.8013 - val_loss: 0.7413 - val_acc: 0.7547\n",
      "Epoch 48/80\n",
      "29639/29639 [==============================] - 2s 69us/step - loss: 0.5751 - acc: 0.8042 - val_loss: 0.8117 - val_acc: 0.7362\n",
      "Epoch 49/80\n",
      "29639/29639 [==============================] - 2s 68us/step - loss: 0.5718 - acc: 0.8025 - val_loss: 0.7663 - val_acc: 0.7502\n",
      "Epoch 50/80\n",
      "29639/29639 [==============================] - 2s 80us/step - loss: 0.5686 - acc: 0.8060 - val_loss: 0.7765 - val_acc: 0.7467\n",
      "Epoch 51/80\n",
      "29639/29639 [==============================] - 2s 70us/step - loss: 0.5661 - acc: 0.8079 - val_loss: 0.7566 - val_acc: 0.7543\n",
      "Epoch 52/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29639/29639 [==============================] - 2s 68us/step - loss: 0.5635 - acc: 0.8067 - val_loss: 0.7795 - val_acc: 0.7444\n",
      "Inner step: 18 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 4s 128us/step - loss: 1.5344 - acc: 0.2850 - val_loss: 1.5095 - val_acc: 0.3112\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 2s 67us/step - loss: 1.5061 - acc: 0.3046 - val_loss: 1.5062 - val_acc: 0.3073\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 2s 64us/step - loss: 1.5038 - acc: 0.3040 - val_loss: 1.5127 - val_acc: 0.3292\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 2s 65us/step - loss: 1.3984 - acc: 0.3807 - val_loss: 1.3546 - val_acc: 0.4022\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 2s 66us/step - loss: 1.2907 - acc: 0.4385 - val_loss: 1.2657 - val_acc: 0.4612\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 2s 68us/step - loss: 1.1817 - acc: 0.4966 - val_loss: 1.2050 - val_acc: 0.5091\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 2s 67us/step - loss: 1.0933 - acc: 0.5414 - val_loss: 1.1721 - val_acc: 0.5306\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 2s 67us/step - loss: 1.0545 - acc: 0.5567 - val_loss: 1.1069 - val_acc: 0.5499\n",
      "Epoch 9/80\n",
      "29639/29639 [==============================] - 2s 67us/step - loss: 1.0366 - acc: 0.5638 - val_loss: 1.1110 - val_acc: 0.5477\n",
      "Epoch 10/80\n",
      "29639/29639 [==============================] - 2s 69us/step - loss: 1.0162 - acc: 0.5762 - val_loss: 1.0811 - val_acc: 0.5674\n",
      "Epoch 11/80\n",
      "29639/29639 [==============================] - 2s 67us/step - loss: 0.9941 - acc: 0.5930 - val_loss: 1.0605 - val_acc: 0.5763\n",
      "Epoch 12/80\n",
      "29639/29639 [==============================] - 2s 70us/step - loss: 0.9761 - acc: 0.6064 - val_loss: 1.0617 - val_acc: 0.5948\n",
      "Epoch 13/80\n",
      "29639/29639 [==============================] - 2s 66us/step - loss: 0.9488 - acc: 0.6295 - val_loss: 1.0466 - val_acc: 0.6054\n",
      "Epoch 14/80\n",
      "29639/29639 [==============================] - 2s 66us/step - loss: 0.9024 - acc: 0.6583 - val_loss: 0.9563 - val_acc: 0.6453\n",
      "Epoch 15/80\n",
      "29639/29639 [==============================] - 2s 67us/step - loss: 0.8689 - acc: 0.6794 - val_loss: 0.9816 - val_acc: 0.6404\n",
      "Epoch 16/80\n",
      "29639/29639 [==============================] - 2s 68us/step - loss: 0.8470 - acc: 0.6933 - val_loss: 0.9404 - val_acc: 0.6601\n",
      "Epoch 17/80\n",
      "29639/29639 [==============================] - 2s 71us/step - loss: 0.8116 - acc: 0.7107 - val_loss: 0.8831 - val_acc: 0.6867\n",
      "Epoch 18/80\n",
      "29639/29639 [==============================] - 2s 69us/step - loss: 0.7982 - acc: 0.7200 - val_loss: 0.9840 - val_acc: 0.6538\n",
      "Epoch 19/80\n",
      "29639/29639 [==============================] - 2s 69us/step - loss: 0.7833 - acc: 0.7294 - val_loss: 0.9077 - val_acc: 0.6920\n",
      "Epoch 20/80\n",
      "29639/29639 [==============================] - 2s 69us/step - loss: 0.7623 - acc: 0.7366 - val_loss: 0.8715 - val_acc: 0.6979\n",
      "Epoch 21/80\n",
      "29639/29639 [==============================] - 2s 70us/step - loss: 0.7363 - acc: 0.7502 - val_loss: 0.8515 - val_acc: 0.7016\n",
      "Epoch 22/80\n",
      "29639/29639 [==============================] - 2s 68us/step - loss: 0.7216 - acc: 0.7558 - val_loss: 0.8441 - val_acc: 0.7080\n",
      "Epoch 23/80\n",
      "29639/29639 [==============================] - 2s 66us/step - loss: 0.7115 - acc: 0.7586 - val_loss: 0.8161 - val_acc: 0.7200\n",
      "Epoch 24/80\n",
      "29639/29639 [==============================] - 2s 65us/step - loss: 0.7035 - acc: 0.7631 - val_loss: 0.8993 - val_acc: 0.6949\n",
      "Epoch 25/80\n",
      "29639/29639 [==============================] - 2s 65us/step - loss: 0.6931 - acc: 0.7646 - val_loss: 0.8569 - val_acc: 0.7034\n",
      "Epoch 26/80\n",
      "29639/29639 [==============================] - 2s 70us/step - loss: 0.6856 - acc: 0.7682 - val_loss: 0.8300 - val_acc: 0.7169\n",
      "Epoch 27/80\n",
      "29639/29639 [==============================] - 2s 67us/step - loss: 0.6857 - acc: 0.7665 - val_loss: 0.8132 - val_acc: 0.7228\n",
      "Epoch 28/80\n",
      "29639/29639 [==============================] - 2s 67us/step - loss: 0.6682 - acc: 0.7737 - val_loss: 0.8159 - val_acc: 0.7191\n",
      "Epoch 29/80\n",
      "29639/29639 [==============================] - 2s 71us/step - loss: 0.6717 - acc: 0.7760 - val_loss: 0.8270 - val_acc: 0.7195\n",
      "Epoch 30/80\n",
      "29639/29639 [==============================] - 2s 72us/step - loss: 0.6649 - acc: 0.7768 - val_loss: 0.8255 - val_acc: 0.7163\n",
      "Epoch 31/80\n",
      "29639/29639 [==============================] - 2s 70us/step - loss: 0.6593 - acc: 0.7774 - val_loss: 0.7965 - val_acc: 0.7288\n",
      "Epoch 32/80\n",
      "29639/29639 [==============================] - 2s 66us/step - loss: 0.6544 - acc: 0.7798 - val_loss: 0.8153 - val_acc: 0.7234\n",
      "Epoch 33/80\n",
      "29639/29639 [==============================] - 2s 66us/step - loss: 0.6442 - acc: 0.7813 - val_loss: 0.8207 - val_acc: 0.7259\n",
      "Epoch 34/80\n",
      "29639/29639 [==============================] - 2s 67us/step - loss: 0.6379 - acc: 0.7833 - val_loss: 0.9571 - val_acc: 0.6764\n",
      "Epoch 35/80\n",
      "29639/29639 [==============================] - 2s 70us/step - loss: 0.6257 - acc: 0.7867 - val_loss: 0.8277 - val_acc: 0.7226\n",
      "Epoch 36/80\n",
      "29639/29639 [==============================] - 2s 72us/step - loss: 0.6319 - acc: 0.7845 - val_loss: 0.8054 - val_acc: 0.7138\n",
      "Inner step: 19 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 4s 124us/step - loss: 1.5382 - acc: 0.2872 - val_loss: 1.5094 - val_acc: 0.3054\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 2s 73us/step - loss: 1.5048 - acc: 0.3040 - val_loss: 1.5383 - val_acc: 0.3088\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 2s 70us/step - loss: 1.5047 - acc: 0.3058 - val_loss: 1.5092 - val_acc: 0.3052\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 2s 70us/step - loss: 1.4973 - acc: 0.3133 - val_loss: 1.4820 - val_acc: 0.3314\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 2s 69us/step - loss: 1.4262 - acc: 0.3755 - val_loss: 1.4213 - val_acc: 0.3948\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 2s 68us/step - loss: 1.3752 - acc: 0.4168 - val_loss: 1.3947 - val_acc: 0.4113\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 2s 65us/step - loss: 1.3445 - acc: 0.4350 - val_loss: 1.3811 - val_acc: 0.4195\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 2s 70us/step - loss: 1.3084 - acc: 0.4508 - val_loss: 1.3120 - val_acc: 0.4570\n",
      "Epoch 9/80\n",
      "29639/29639 [==============================] - 2s 65us/step - loss: 1.2101 - acc: 0.4975 - val_loss: 1.1762 - val_acc: 0.5387\n",
      "Epoch 10/80\n",
      "29639/29639 [==============================] - 2s 70us/step - loss: 1.0853 - acc: 0.5651 - val_loss: 1.1461 - val_acc: 0.5534\n",
      "Epoch 11/80\n",
      "29639/29639 [==============================] - 2s 69us/step - loss: 1.0386 - acc: 0.5888 - val_loss: 1.1784 - val_acc: 0.5508\n",
      "Epoch 12/80\n",
      "29639/29639 [==============================] - 2s 70us/step - loss: 1.0106 - acc: 0.6020 - val_loss: 1.1573 - val_acc: 0.5470\n",
      "Epoch 13/80\n",
      "29639/29639 [==============================] - 2s 66us/step - loss: 0.9943 - acc: 0.6090 - val_loss: 1.0871 - val_acc: 0.5798\n",
      "Epoch 14/80\n",
      "29639/29639 [==============================] - 2s 67us/step - loss: 0.9818 - acc: 0.6179 - val_loss: 1.0457 - val_acc: 0.5983\n",
      "Epoch 15/80\n",
      "29639/29639 [==============================] - 2s 68us/step - loss: 0.9807 - acc: 0.6165 - val_loss: 1.0784 - val_acc: 0.5821\n",
      "Epoch 16/80\n",
      "29639/29639 [==============================] - 2s 71us/step - loss: 0.9614 - acc: 0.6290 - val_loss: 1.0607 - val_acc: 0.5953\n",
      "Epoch 17/80\n",
      "29639/29639 [==============================] - 2s 65us/step - loss: 0.9419 - acc: 0.6412 - val_loss: 1.0021 - val_acc: 0.6153\n",
      "Epoch 18/80\n",
      "29639/29639 [==============================] - 2s 70us/step - loss: 0.9093 - acc: 0.6625 - val_loss: 1.0007 - val_acc: 0.6307\n",
      "Epoch 19/80\n",
      "29639/29639 [==============================] - 2s 66us/step - loss: 0.8787 - acc: 0.6850 - val_loss: 0.9571 - val_acc: 0.6655\n",
      "Epoch 20/80\n",
      "29639/29639 [==============================] - 2s 65us/step - loss: 0.8609 - acc: 0.7032 - val_loss: 0.9443 - val_acc: 0.6688\n",
      "Epoch 21/80\n",
      "29639/29639 [==============================] - 2s 66us/step - loss: 0.8294 - acc: 0.7159 - val_loss: 0.9061 - val_acc: 0.6974\n",
      "Epoch 22/80\n",
      "29639/29639 [==============================] - 2s 72us/step - loss: 0.8120 - acc: 0.7246 - val_loss: 0.9158 - val_acc: 0.6909\n",
      "Epoch 23/80\n",
      "29639/29639 [==============================] - 2s 73us/step - loss: 0.7996 - acc: 0.7266 - val_loss: 0.8887 - val_acc: 0.7019\n",
      "Epoch 24/80\n",
      "29639/29639 [==============================] - 2s 73us/step - loss: 0.7912 - acc: 0.7302 - val_loss: 0.9185 - val_acc: 0.6914\n",
      "Epoch 25/80\n",
      "29639/29639 [==============================] - 2s 71us/step - loss: 0.7734 - acc: 0.7371 - val_loss: 0.9958 - val_acc: 0.6628\n",
      "Epoch 26/80\n",
      "29639/29639 [==============================] - 2s 68us/step - loss: 0.7728 - acc: 0.7368 - val_loss: 0.8542 - val_acc: 0.7129\n",
      "Epoch 27/80\n",
      "29639/29639 [==============================] - 2s 70us/step - loss: 0.7511 - acc: 0.7443 - val_loss: 0.8315 - val_acc: 0.7179\n",
      "Epoch 28/80\n",
      "29639/29639 [==============================] - 2s 69us/step - loss: 0.7360 - acc: 0.7471 - val_loss: 0.8907 - val_acc: 0.6915\n",
      "Epoch 29/80\n",
      "29639/29639 [==============================] - 2s 69us/step - loss: 0.7248 - acc: 0.7524 - val_loss: 0.8263 - val_acc: 0.7185\n",
      "Epoch 30/80\n",
      "29639/29639 [==============================] - 2s 70us/step - loss: 0.7132 - acc: 0.7569 - val_loss: 0.8861 - val_acc: 0.6933\n",
      "Epoch 31/80\n",
      "29639/29639 [==============================] - 2s 69us/step - loss: 0.6982 - acc: 0.7612 - val_loss: 0.8224 - val_acc: 0.7171\n",
      "Epoch 32/80\n",
      "29639/29639 [==============================] - 2s 69us/step - loss: 0.6939 - acc: 0.7622 - val_loss: 0.8283 - val_acc: 0.7138\n",
      "Epoch 33/80\n",
      "29639/29639 [==============================] - 2s 69us/step - loss: 0.6829 - acc: 0.7652 - val_loss: 0.8069 - val_acc: 0.7219\n",
      "Epoch 34/80\n",
      "29639/29639 [==============================] - 2s 66us/step - loss: 0.6813 - acc: 0.7662 - val_loss: 0.7961 - val_acc: 0.7275\n",
      "Epoch 35/80\n",
      "29639/29639 [==============================] - 2s 71us/step - loss: 0.6630 - acc: 0.7729 - val_loss: 0.8755 - val_acc: 0.6939\n",
      "Epoch 36/80\n",
      "29639/29639 [==============================] - 2s 74us/step - loss: 0.6683 - acc: 0.7683 - val_loss: 0.8564 - val_acc: 0.7085\n",
      "Epoch 37/80\n",
      "29639/29639 [==============================] - 2s 72us/step - loss: 0.6550 - acc: 0.7734 - val_loss: 0.8728 - val_acc: 0.7025\n",
      "Epoch 38/80\n",
      "29639/29639 [==============================] - 2s 67us/step - loss: 0.6541 - acc: 0.7731 - val_loss: 0.8276 - val_acc: 0.7132\n",
      "Epoch 39/80\n",
      "29639/29639 [==============================] - 2s 69us/step - loss: 0.6461 - acc: 0.7767 - val_loss: 0.8106 - val_acc: 0.7218\n",
      "Inner step: 20 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 4s 127us/step - loss: 1.4744 - acc: 0.3334 - val_loss: 1.4856 - val_acc: 0.3810\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 2s 73us/step - loss: 1.3516 - acc: 0.4048 - val_loss: 1.3583 - val_acc: 0.4064\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 2s 70us/step - loss: 1.3401 - acc: 0.4097 - val_loss: 1.3635 - val_acc: 0.4062\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 2s 71us/step - loss: 1.3357 - acc: 0.4101 - val_loss: 1.3768 - val_acc: 0.4052\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 2s 71us/step - loss: 1.3337 - acc: 0.4096 - val_loss: 1.4094 - val_acc: 0.3899\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 2s 73us/step - loss: 1.3301 - acc: 0.4106 - val_loss: 1.3574 - val_acc: 0.4073\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 2s 76us/step - loss: 1.3282 - acc: 0.4107 - val_loss: 1.3552 - val_acc: 0.4053\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 2s 72us/step - loss: 1.3249 - acc: 0.4115 - val_loss: 1.3557 - val_acc: 0.4010\n",
      "Epoch 9/80\n",
      "29639/29639 [==============================] - 2s 76us/step - loss: 1.3277 - acc: 0.4108 - val_loss: 1.3616 - val_acc: 0.4010\n",
      "Epoch 10/80\n",
      "29639/29639 [==============================] - 2s 72us/step - loss: 1.3232 - acc: 0.4127 - val_loss: 1.4381 - val_acc: 0.3914\n",
      "Epoch 11/80\n",
      "29639/29639 [==============================] - 2s 70us/step - loss: 1.2737 - acc: 0.4459 - val_loss: 1.2601 - val_acc: 0.4617\n",
      "Epoch 12/80\n",
      "29639/29639 [==============================] - 2s 74us/step - loss: 1.2229 - acc: 0.4693 - val_loss: 1.2491 - val_acc: 0.4607\n",
      "Epoch 13/80\n",
      "29639/29639 [==============================] - 2s 71us/step - loss: 1.2005 - acc: 0.4938 - val_loss: 1.2223 - val_acc: 0.4998\n",
      "Epoch 14/80\n",
      "29639/29639 [==============================] - 2s 70us/step - loss: 1.1525 - acc: 0.5284 - val_loss: 1.1846 - val_acc: 0.5192\n",
      "Epoch 15/80\n",
      "29639/29639 [==============================] - 2s 73us/step - loss: 1.1133 - acc: 0.5383 - val_loss: 1.1542 - val_acc: 0.5259\n",
      "Epoch 16/80\n",
      "29639/29639 [==============================] - 2s 66us/step - loss: 1.0927 - acc: 0.5453 - val_loss: 1.1442 - val_acc: 0.5331\n",
      "Epoch 17/80\n",
      "29639/29639 [==============================] - 2s 72us/step - loss: 1.0839 - acc: 0.5509 - val_loss: 1.1390 - val_acc: 0.5387\n",
      "Epoch 18/80\n",
      "29639/29639 [==============================] - 2s 66us/step - loss: 1.0626 - acc: 0.5678 - val_loss: 1.1275 - val_acc: 0.5445\n",
      "Epoch 19/80\n",
      "29639/29639 [==============================] - 2s 70us/step - loss: 1.0412 - acc: 0.5793 - val_loss: 1.1578 - val_acc: 0.5416\n",
      "Epoch 20/80\n",
      "29639/29639 [==============================] - 2s 69us/step - loss: 1.0292 - acc: 0.5883 - val_loss: 1.0949 - val_acc: 0.5694\n",
      "Epoch 21/80\n",
      "29639/29639 [==============================] - 2s 72us/step - loss: 0.9908 - acc: 0.6190 - val_loss: 1.0624 - val_acc: 0.5950\n",
      "Epoch 22/80\n",
      "29639/29639 [==============================] - 2s 69us/step - loss: 0.9545 - acc: 0.6436 - val_loss: 1.0580 - val_acc: 0.6024\n",
      "Epoch 23/80\n",
      "29639/29639 [==============================] - 2s 71us/step - loss: 0.9205 - acc: 0.6695 - val_loss: 1.0505 - val_acc: 0.6204\n",
      "Epoch 24/80\n",
      "29639/29639 [==============================] - 2s 74us/step - loss: 0.8970 - acc: 0.6801 - val_loss: 1.0305 - val_acc: 0.6253\n",
      "Epoch 25/80\n",
      "29639/29639 [==============================] - 2s 73us/step - loss: 0.8717 - acc: 0.6933 - val_loss: 0.9478 - val_acc: 0.6639\n",
      "Epoch 26/80\n",
      "29639/29639 [==============================] - 2s 68us/step - loss: 0.8430 - acc: 0.7028 - val_loss: 0.9182 - val_acc: 0.6860\n",
      "Epoch 27/80\n",
      "29639/29639 [==============================] - 2s 66us/step - loss: 0.7978 - acc: 0.7242 - val_loss: 0.9192 - val_acc: 0.6877\n",
      "Epoch 28/80\n",
      "29639/29639 [==============================] - 2s 67us/step - loss: 0.7618 - acc: 0.7410 - val_loss: 0.8997 - val_acc: 0.7031\n",
      "Epoch 29/80\n",
      "29639/29639 [==============================] - 2s 66us/step - loss: 0.7389 - acc: 0.7488 - val_loss: 0.8429 - val_acc: 0.7209\n",
      "Epoch 30/80\n",
      "29639/29639 [==============================] - 2s 66us/step - loss: 0.7223 - acc: 0.7566 - val_loss: 0.8590 - val_acc: 0.7084\n",
      "Epoch 31/80\n",
      "29639/29639 [==============================] - 2s 66us/step - loss: 0.7064 - acc: 0.7620 - val_loss: 0.8639 - val_acc: 0.7056\n",
      "Epoch 32/80\n",
      "29639/29639 [==============================] - 2s 65us/step - loss: 0.7034 - acc: 0.7597 - val_loss: 0.8345 - val_acc: 0.7161\n",
      "Epoch 33/80\n",
      "29639/29639 [==============================] - 2s 65us/step - loss: 0.6784 - acc: 0.7714 - val_loss: 0.8871 - val_acc: 0.7023\n",
      "Epoch 34/80\n",
      "29639/29639 [==============================] - 2s 66us/step - loss: 0.6769 - acc: 0.7714 - val_loss: 0.8485 - val_acc: 0.7221\n",
      "Epoch 35/80\n",
      "29639/29639 [==============================] - 2s 68us/step - loss: 0.6630 - acc: 0.7752 - val_loss: 0.8782 - val_acc: 0.7153\n",
      "Epoch 36/80\n",
      "29639/29639 [==============================] - 2s 70us/step - loss: 0.6614 - acc: 0.7763 - val_loss: 0.8527 - val_acc: 0.7096\n",
      "Epoch 37/80\n",
      "29639/29639 [==============================] - 2s 68us/step - loss: 0.6566 - acc: 0.7793 - val_loss: 0.8122 - val_acc: 0.7183\n",
      "Epoch 38/80\n",
      "29639/29639 [==============================] - 2s 69us/step - loss: 0.6492 - acc: 0.7794 - val_loss: 0.8042 - val_acc: 0.7375\n",
      "Epoch 39/80\n",
      "29639/29639 [==============================] - 2s 74us/step - loss: 0.6426 - acc: 0.7791 - val_loss: 0.8081 - val_acc: 0.7295\n",
      "Epoch 40/80\n",
      "29639/29639 [==============================] - 2s 68us/step - loss: 0.6351 - acc: 0.7859 - val_loss: 0.8011 - val_acc: 0.7313\n",
      "Epoch 41/80\n",
      "29639/29639 [==============================] - 2s 73us/step - loss: 0.6401 - acc: 0.7830 - val_loss: 0.8377 - val_acc: 0.7253\n",
      "Epoch 42/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29639/29639 [==============================] - 2s 67us/step - loss: 0.6253 - acc: 0.7874 - val_loss: 0.8696 - val_acc: 0.7185\n",
      "Epoch 43/80\n",
      "29639/29639 [==============================] - 2s 65us/step - loss: 0.6272 - acc: 0.7890 - val_loss: 0.8104 - val_acc: 0.7244\n",
      "Epoch 44/80\n",
      "29639/29639 [==============================] - 2s 65us/step - loss: 0.6257 - acc: 0.7896 - val_loss: 0.7969 - val_acc: 0.7352\n",
      "Epoch 45/80\n",
      "29639/29639 [==============================] - 2s 66us/step - loss: 0.6232 - acc: 0.7890 - val_loss: 0.7824 - val_acc: 0.7345\n",
      "Epoch 46/80\n",
      "29639/29639 [==============================] - 2s 65us/step - loss: 0.6149 - acc: 0.7926 - val_loss: 0.7894 - val_acc: 0.7395\n",
      "Epoch 47/80\n",
      "29639/29639 [==============================] - 2s 66us/step - loss: 0.6148 - acc: 0.7932 - val_loss: 0.8503 - val_acc: 0.7311\n",
      "Epoch 48/80\n",
      "29639/29639 [==============================] - 2s 65us/step - loss: 0.6011 - acc: 0.7982 - val_loss: 0.8820 - val_acc: 0.7118\n",
      "Epoch 49/80\n",
      "29639/29639 [==============================] - 2s 65us/step - loss: 0.6034 - acc: 0.7968 - val_loss: 0.7826 - val_acc: 0.7426\n",
      "Epoch 50/80\n",
      "29639/29639 [==============================] - 2s 65us/step - loss: 0.5972 - acc: 0.7984 - val_loss: 0.8498 - val_acc: 0.7275\n",
      "Inner step: 21 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 4s 127us/step - loss: 1.5343 - acc: 0.2910 - val_loss: 1.5160 - val_acc: 0.3025\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 2s 69us/step - loss: 1.5050 - acc: 0.3027 - val_loss: 1.5044 - val_acc: 0.2936\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 2s 66us/step - loss: 1.5047 - acc: 0.3048 - val_loss: 1.5071 - val_acc: 0.2937\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 2s 66us/step - loss: 1.4930 - acc: 0.3194 - val_loss: 1.4376 - val_acc: 0.3792\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 2s 66us/step - loss: 1.4195 - acc: 0.3808 - val_loss: 1.4027 - val_acc: 0.3821\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 2s 66us/step - loss: 1.3364 - acc: 0.3976 - val_loss: 1.3510 - val_acc: 0.3935\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 2s 66us/step - loss: 1.2880 - acc: 0.4460 - val_loss: 1.2563 - val_acc: 0.4973\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 2s 67us/step - loss: 1.1695 - acc: 0.5259 - val_loss: 1.1571 - val_acc: 0.5491\n",
      "Epoch 9/80\n",
      "29639/29639 [==============================] - 2s 65us/step - loss: 1.0707 - acc: 0.5654 - val_loss: 1.1610 - val_acc: 0.5575\n",
      "Epoch 10/80\n",
      "29639/29639 [==============================] - 2s 65us/step - loss: 1.0282 - acc: 0.5849 - val_loss: 1.0982 - val_acc: 0.5733\n",
      "Epoch 11/80\n",
      "29639/29639 [==============================] - 2s 68us/step - loss: 0.9939 - acc: 0.6023 - val_loss: 1.0638 - val_acc: 0.5758\n",
      "Epoch 12/80\n",
      "29639/29639 [==============================] - 2s 65us/step - loss: 0.9685 - acc: 0.6175 - val_loss: 1.0937 - val_acc: 0.5829\n",
      "Epoch 13/80\n",
      "29639/29639 [==============================] - 2s 67us/step - loss: 0.9400 - acc: 0.6342 - val_loss: 1.0191 - val_acc: 0.6137\n",
      "Epoch 14/80\n",
      "29639/29639 [==============================] - 2s 70us/step - loss: 0.9216 - acc: 0.6464 - val_loss: 0.9930 - val_acc: 0.6244\n",
      "Epoch 15/80\n",
      "29639/29639 [==============================] - 2s 70us/step - loss: 0.9081 - acc: 0.6549 - val_loss: 1.0092 - val_acc: 0.6238\n",
      "Epoch 16/80\n",
      "29639/29639 [==============================] - 2s 69us/step - loss: 0.8761 - acc: 0.6738 - val_loss: 1.0032 - val_acc: 0.6433\n",
      "Epoch 17/80\n",
      "29639/29639 [==============================] - 2s 71us/step - loss: 0.8579 - acc: 0.6865 - val_loss: 0.9364 - val_acc: 0.6614\n",
      "Epoch 18/80\n",
      "29639/29639 [==============================] - 2s 80us/step - loss: 0.8315 - acc: 0.7005 - val_loss: 0.9568 - val_acc: 0.6539\n",
      "Epoch 19/80\n",
      "29639/29639 [==============================] - 2s 67us/step - loss: 0.8065 - acc: 0.7152 - val_loss: 0.9128 - val_acc: 0.6768\n",
      "Epoch 20/80\n",
      "29639/29639 [==============================] - 2s 72us/step - loss: 0.7703 - acc: 0.7338 - val_loss: 0.8700 - val_acc: 0.6946\n",
      "Epoch 21/80\n",
      "29639/29639 [==============================] - 2s 70us/step - loss: 0.7481 - acc: 0.7453 - val_loss: 0.8730 - val_acc: 0.6957\n",
      "Epoch 22/80\n",
      "29639/29639 [==============================] - 2s 72us/step - loss: 0.7371 - acc: 0.7503 - val_loss: 0.8602 - val_acc: 0.7098\n",
      "Epoch 23/80\n",
      "29639/29639 [==============================] - 2s 68us/step - loss: 0.7192 - acc: 0.7578 - val_loss: 0.8220 - val_acc: 0.7170\n",
      "Epoch 24/80\n",
      "29639/29639 [==============================] - 2s 66us/step - loss: 0.7094 - acc: 0.7606 - val_loss: 0.8295 - val_acc: 0.7222\n",
      "Epoch 25/80\n",
      "29639/29639 [==============================] - 2s 65us/step - loss: 0.6977 - acc: 0.7652 - val_loss: 0.8078 - val_acc: 0.7277\n",
      "Epoch 26/80\n",
      "29639/29639 [==============================] - 2s 71us/step - loss: 0.6829 - acc: 0.7698 - val_loss: 0.8487 - val_acc: 0.7229\n",
      "Epoch 27/80\n",
      "29639/29639 [==============================] - 2s 68us/step - loss: 0.6757 - acc: 0.7726 - val_loss: 0.8491 - val_acc: 0.7178\n",
      "Epoch 28/80\n",
      "29639/29639 [==============================] - 2s 65us/step - loss: 0.6695 - acc: 0.7742 - val_loss: 0.8787 - val_acc: 0.7080\n",
      "Epoch 29/80\n",
      "29639/29639 [==============================] - 2s 67us/step - loss: 0.6680 - acc: 0.7729 - val_loss: 0.7921 - val_acc: 0.7351\n",
      "Epoch 30/80\n",
      "29639/29639 [==============================] - 2s 65us/step - loss: 0.6572 - acc: 0.7769 - val_loss: 0.8167 - val_acc: 0.7280\n",
      "Epoch 31/80\n",
      "29639/29639 [==============================] - 2s 74us/step - loss: 0.6572 - acc: 0.7766 - val_loss: 0.8844 - val_acc: 0.7065\n",
      "Epoch 32/80\n",
      "29639/29639 [==============================] - 2s 68us/step - loss: 0.6513 - acc: 0.7804 - val_loss: 0.8165 - val_acc: 0.7164\n",
      "Epoch 33/80\n",
      "29639/29639 [==============================] - 2s 66us/step - loss: 0.6436 - acc: 0.7818 - val_loss: 0.8203 - val_acc: 0.7275\n",
      "Epoch 34/80\n",
      "29639/29639 [==============================] - 2s 65us/step - loss: 0.6389 - acc: 0.7807 - val_loss: 0.8096 - val_acc: 0.7265\n",
      "Inner step: 22 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 4s 126us/step - loss: 1.4825 - acc: 0.3268 - val_loss: 1.4329 - val_acc: 0.3634\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 2s 72us/step - loss: 1.4034 - acc: 0.3653 - val_loss: 1.4215 - val_acc: 0.3752\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 2s 73us/step - loss: 1.3846 - acc: 0.3817 - val_loss: 1.4014 - val_acc: 0.3833\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 2s 71us/step - loss: 1.3684 - acc: 0.3965 - val_loss: 1.4319 - val_acc: 0.4072\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 2s 66us/step - loss: 1.3625 - acc: 0.4032 - val_loss: 1.3837 - val_acc: 0.4074\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 2s 66us/step - loss: 1.3520 - acc: 0.4131 - val_loss: 1.3902 - val_acc: 0.4414\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 2s 71us/step - loss: 1.2544 - acc: 0.4822 - val_loss: 1.2732 - val_acc: 0.4927\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 2s 75us/step - loss: 1.1649 - acc: 0.5217 - val_loss: 1.2363 - val_acc: 0.5090\n",
      "Epoch 9/80\n",
      "29639/29639 [==============================] - 2s 71us/step - loss: 1.1284 - acc: 0.5298 - val_loss: 1.1877 - val_acc: 0.5192\n",
      "Epoch 10/80\n",
      "29639/29639 [==============================] - 2s 69us/step - loss: 1.1066 - acc: 0.5378 - val_loss: 1.1555 - val_acc: 0.5169\n",
      "Epoch 11/80\n",
      "29639/29639 [==============================] - 2s 69us/step - loss: 1.0850 - acc: 0.5458 - val_loss: 1.2219 - val_acc: 0.4957\n",
      "Epoch 12/80\n",
      "29639/29639 [==============================] - 2s 71us/step - loss: 1.0718 - acc: 0.5511 - val_loss: 1.1413 - val_acc: 0.5329\n",
      "Epoch 13/80\n",
      "29639/29639 [==============================] - 2s 71us/step - loss: 1.0668 - acc: 0.5562 - val_loss: 1.1621 - val_acc: 0.5308\n",
      "Epoch 14/80\n",
      "29639/29639 [==============================] - 2s 68us/step - loss: 1.0452 - acc: 0.5819 - val_loss: 1.1326 - val_acc: 0.5643\n",
      "Epoch 15/80\n",
      "29639/29639 [==============================] - 2s 68us/step - loss: 1.0031 - acc: 0.6112 - val_loss: 1.0628 - val_acc: 0.6101\n",
      "Epoch 16/80\n",
      "29639/29639 [==============================] - 2s 68us/step - loss: 0.9613 - acc: 0.6381 - val_loss: 1.0521 - val_acc: 0.5999\n",
      "Epoch 17/80\n",
      "29639/29639 [==============================] - 2s 69us/step - loss: 0.9276 - acc: 0.6576 - val_loss: 0.9925 - val_acc: 0.6326\n",
      "Epoch 18/80\n",
      "29639/29639 [==============================] - 2s 77us/step - loss: 0.8825 - acc: 0.6808 - val_loss: 1.0259 - val_acc: 0.6309\n",
      "Epoch 19/80\n",
      "29639/29639 [==============================] - 2s 75us/step - loss: 0.8437 - acc: 0.7002 - val_loss: 0.9190 - val_acc: 0.6749\n",
      "Epoch 20/80\n",
      "29639/29639 [==============================] - 2s 75us/step - loss: 0.8032 - acc: 0.7187 - val_loss: 0.9965 - val_acc: 0.6706\n",
      "Epoch 21/80\n",
      "29639/29639 [==============================] - 2s 72us/step - loss: 0.7632 - acc: 0.7350 - val_loss: 0.8499 - val_acc: 0.7070\n",
      "Epoch 22/80\n",
      "29639/29639 [==============================] - 2s 71us/step - loss: 0.7312 - acc: 0.7515 - val_loss: 0.8338 - val_acc: 0.7182\n",
      "Epoch 23/80\n",
      "29639/29639 [==============================] - 2s 73us/step - loss: 0.7152 - acc: 0.7588 - val_loss: 0.8875 - val_acc: 0.7061\n",
      "Epoch 24/80\n",
      "29639/29639 [==============================] - 2s 73us/step - loss: 0.6918 - acc: 0.7687 - val_loss: 0.8591 - val_acc: 0.7202\n",
      "Epoch 25/80\n",
      "29639/29639 [==============================] - 2s 78us/step - loss: 0.6814 - acc: 0.7741 - val_loss: 0.8379 - val_acc: 0.7332\n",
      "Epoch 26/80\n",
      "29639/29639 [==============================] - 2s 68us/step - loss: 0.6594 - acc: 0.7805 - val_loss: 0.8500 - val_acc: 0.7160\n",
      "Epoch 27/80\n",
      "29639/29639 [==============================] - 2s 74us/step - loss: 0.6633 - acc: 0.7773 - val_loss: 0.8257 - val_acc: 0.7256\n",
      "Epoch 28/80\n",
      "29639/29639 [==============================] - 2s 77us/step - loss: 0.6502 - acc: 0.7831 - val_loss: 0.8343 - val_acc: 0.7193\n",
      "Epoch 29/80\n",
      "29639/29639 [==============================] - 2s 70us/step - loss: 0.6451 - acc: 0.7850 - val_loss: 0.8946 - val_acc: 0.7091\n",
      "Epoch 30/80\n",
      "29639/29639 [==============================] - 2s 75us/step - loss: 0.6386 - acc: 0.7841 - val_loss: 0.8574 - val_acc: 0.7266\n",
      "Epoch 31/80\n",
      "29639/29639 [==============================] - 2s 70us/step - loss: 0.6322 - acc: 0.7872 - val_loss: 0.8717 - val_acc: 0.7149\n",
      "Epoch 32/80\n",
      "29639/29639 [==============================] - 2s 73us/step - loss: 0.6278 - acc: 0.7895 - val_loss: 0.8639 - val_acc: 0.7153\n",
      "Inner step: 23 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 4s 127us/step - loss: 1.5410 - acc: 0.2835 - val_loss: 1.5113 - val_acc: 0.3024\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 2s 70us/step - loss: 1.4505 - acc: 0.3503 - val_loss: 1.3952 - val_acc: 0.3891\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 2s 68us/step - loss: 1.3875 - acc: 0.3891 - val_loss: 1.3467 - val_acc: 0.4223\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 2s 69us/step - loss: 1.2715 - acc: 0.4525 - val_loss: 1.1992 - val_acc: 0.5038\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 2s 74us/step - loss: 1.1370 - acc: 0.5242 - val_loss: 1.1750 - val_acc: 0.5296\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 2s 73us/step - loss: 1.0957 - acc: 0.5428 - val_loss: 1.1260 - val_acc: 0.5409\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 2s 67us/step - loss: 1.0245 - acc: 0.5741 - val_loss: 1.0980 - val_acc: 0.5534\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 2s 73us/step - loss: 0.9895 - acc: 0.5941 - val_loss: 1.0833 - val_acc: 0.5710\n",
      "Epoch 9/80\n",
      "29639/29639 [==============================] - 2s 76us/step - loss: 0.9685 - acc: 0.6080 - val_loss: 1.0385 - val_acc: 0.5877\n",
      "Epoch 10/80\n",
      "29639/29639 [==============================] - 2s 74us/step - loss: 0.9484 - acc: 0.6205 - val_loss: 1.0246 - val_acc: 0.6078\n",
      "Epoch 11/80\n",
      "29639/29639 [==============================] - 2s 73us/step - loss: 0.9239 - acc: 0.6420 - val_loss: 1.0060 - val_acc: 0.6217\n",
      "Epoch 12/80\n",
      "29639/29639 [==============================] - 2s 77us/step - loss: 0.8792 - acc: 0.6770 - val_loss: 0.9332 - val_acc: 0.6731\n",
      "Epoch 13/80\n",
      "29639/29639 [==============================] - 2s 73us/step - loss: 0.8525 - acc: 0.6946 - val_loss: 0.9480 - val_acc: 0.6754\n",
      "Epoch 14/80\n",
      "29639/29639 [==============================] - 2s 72us/step - loss: 0.8243 - acc: 0.7124 - val_loss: 0.8933 - val_acc: 0.6810\n",
      "Epoch 15/80\n",
      "29639/29639 [==============================] - 2s 73us/step - loss: 0.8059 - acc: 0.7243 - val_loss: 0.9110 - val_acc: 0.6874\n",
      "Epoch 16/80\n",
      "29639/29639 [==============================] - 2s 72us/step - loss: 0.7678 - acc: 0.7397 - val_loss: 0.8907 - val_acc: 0.7031\n",
      "Epoch 17/80\n",
      "29639/29639 [==============================] - 2s 71us/step - loss: 0.7502 - acc: 0.7480 - val_loss: 0.8518 - val_acc: 0.7113\n",
      "Epoch 18/80\n",
      "29639/29639 [==============================] - 2s 76us/step - loss: 0.7316 - acc: 0.7555 - val_loss: 0.8907 - val_acc: 0.6912\n",
      "Epoch 19/80\n",
      "29639/29639 [==============================] - 2s 77us/step - loss: 0.7220 - acc: 0.7631 - val_loss: 0.8562 - val_acc: 0.7087\n",
      "Epoch 20/80\n",
      "29639/29639 [==============================] - 2s 68us/step - loss: 0.7142 - acc: 0.7634 - val_loss: 0.8505 - val_acc: 0.7066\n",
      "Epoch 21/80\n",
      "29639/29639 [==============================] - 2s 69us/step - loss: 0.7101 - acc: 0.7632 - val_loss: 0.8698 - val_acc: 0.7060\n",
      "Epoch 22/80\n",
      "29639/29639 [==============================] - 2s 68us/step - loss: 0.7050 - acc: 0.7667 - val_loss: 0.8384 - val_acc: 0.7158\n",
      "Epoch 23/80\n",
      "29639/29639 [==============================] - 2s 69us/step - loss: 0.6924 - acc: 0.7694 - val_loss: 0.8332 - val_acc: 0.7170\n",
      "Epoch 24/80\n",
      "29639/29639 [==============================] - 2s 69us/step - loss: 0.6887 - acc: 0.7700 - val_loss: 0.8163 - val_acc: 0.7268\n",
      "Epoch 25/80\n",
      "29639/29639 [==============================] - 2s 68us/step - loss: 0.6935 - acc: 0.7683 - val_loss: 0.8231 - val_acc: 0.7193\n",
      "Epoch 26/80\n",
      "29639/29639 [==============================] - 2s 68us/step - loss: 0.6845 - acc: 0.7707 - val_loss: 0.8481 - val_acc: 0.7161\n",
      "Epoch 27/80\n",
      "29639/29639 [==============================] - 2s 69us/step - loss: 0.6793 - acc: 0.7718 - val_loss: 0.8571 - val_acc: 0.7074\n",
      "Epoch 28/80\n",
      "29639/29639 [==============================] - 2s 75us/step - loss: 0.6751 - acc: 0.7713 - val_loss: 0.8151 - val_acc: 0.7297\n",
      "Epoch 29/80\n",
      "29639/29639 [==============================] - 2s 76us/step - loss: 0.6646 - acc: 0.7767 - val_loss: 0.8386 - val_acc: 0.7247\n",
      "Epoch 30/80\n",
      "29639/29639 [==============================] - 2s 70us/step - loss: 0.6596 - acc: 0.7781 - val_loss: 0.8214 - val_acc: 0.7278\n",
      "Epoch 31/80\n",
      "29639/29639 [==============================] - 2s 74us/step - loss: 0.6576 - acc: 0.7781 - val_loss: 0.8699 - val_acc: 0.7089\n",
      "Epoch 32/80\n",
      "29639/29639 [==============================] - 2s 75us/step - loss: 0.6491 - acc: 0.7814 - val_loss: 0.8055 - val_acc: 0.7327\n",
      "Epoch 33/80\n",
      "29639/29639 [==============================] - 2s 74us/step - loss: 0.6500 - acc: 0.7802 - val_loss: 0.8105 - val_acc: 0.7281\n",
      "Epoch 34/80\n",
      "29639/29639 [==============================] - 2s 68us/step - loss: 0.6464 - acc: 0.7830 - val_loss: 0.8182 - val_acc: 0.7214\n",
      "Epoch 35/80\n",
      "29639/29639 [==============================] - 2s 68us/step - loss: 0.6368 - acc: 0.7860 - val_loss: 0.7990 - val_acc: 0.7271\n",
      "Epoch 36/80\n",
      "29639/29639 [==============================] - 2s 72us/step - loss: 0.6356 - acc: 0.7848 - val_loss: 0.8442 - val_acc: 0.7191\n",
      "Epoch 37/80\n",
      "29639/29639 [==============================] - 2s 73us/step - loss: 0.6378 - acc: 0.7844 - val_loss: 0.7878 - val_acc: 0.7379\n",
      "Epoch 38/80\n",
      "29639/29639 [==============================] - 2s 74us/step - loss: 0.6332 - acc: 0.7861 - val_loss: 0.7978 - val_acc: 0.7325\n",
      "Epoch 39/80\n",
      "29639/29639 [==============================] - 2s 80us/step - loss: 0.6284 - acc: 0.7854 - val_loss: 0.7680 - val_acc: 0.7407\n",
      "Epoch 40/80\n",
      "29639/29639 [==============================] - 2s 68us/step - loss: 0.6291 - acc: 0.7872 - val_loss: 0.8007 - val_acc: 0.7275\n",
      "Epoch 41/80\n",
      "29639/29639 [==============================] - 2s 69us/step - loss: 0.6226 - acc: 0.7888 - val_loss: 0.8192 - val_acc: 0.7302\n",
      "Epoch 42/80\n",
      "29639/29639 [==============================] - 2s 71us/step - loss: 0.6182 - acc: 0.7897 - val_loss: 0.7942 - val_acc: 0.7356\n",
      "Epoch 43/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29639/29639 [==============================] - 2s 75us/step - loss: 0.6194 - acc: 0.7895 - val_loss: 0.7824 - val_acc: 0.7376\n",
      "Epoch 44/80\n",
      "29639/29639 [==============================] - 2s 69us/step - loss: 0.6167 - acc: 0.7914 - val_loss: 0.7854 - val_acc: 0.7364\n",
      "Inner step: 24 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 4s 131us/step - loss: 1.5360 - acc: 0.2879 - val_loss: 1.5152 - val_acc: 0.3038\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 2s 69us/step - loss: 1.4982 - acc: 0.3115 - val_loss: 1.4328 - val_acc: 0.3685\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 2s 68us/step - loss: 1.3318 - acc: 0.4316 - val_loss: 1.2150 - val_acc: 0.4945\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 2s 71us/step - loss: 1.1421 - acc: 0.5361 - val_loss: 1.1420 - val_acc: 0.5370\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 2s 71us/step - loss: 1.1055 - acc: 0.5516 - val_loss: 1.1128 - val_acc: 0.5418\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 2s 70us/step - loss: 1.0790 - acc: 0.5625 - val_loss: 1.0952 - val_acc: 0.5667\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 2s 67us/step - loss: 1.0361 - acc: 0.5798 - val_loss: 1.0597 - val_acc: 0.5665\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 2s 73us/step - loss: 0.9970 - acc: 0.5980 - val_loss: 1.0715 - val_acc: 0.5812\n",
      "Epoch 9/80\n",
      "29639/29639 [==============================] - 2s 70us/step - loss: 0.9686 - acc: 0.6144 - val_loss: 1.0328 - val_acc: 0.5949\n",
      "Epoch 10/80\n",
      "29639/29639 [==============================] - 2s 67us/step - loss: 0.9450 - acc: 0.6285 - val_loss: 0.9923 - val_acc: 0.6234\n",
      "Epoch 11/80\n",
      "29639/29639 [==============================] - 2s 66us/step - loss: 0.9261 - acc: 0.6401 - val_loss: 0.9991 - val_acc: 0.6149\n",
      "Epoch 12/80\n",
      "29639/29639 [==============================] - 2s 65us/step - loss: 0.9007 - acc: 0.6496 - val_loss: 0.9770 - val_acc: 0.6331\n",
      "Epoch 13/80\n",
      "29639/29639 [==============================] - 2s 69us/step - loss: 0.8731 - acc: 0.6703 - val_loss: 0.9674 - val_acc: 0.6348\n",
      "Epoch 14/80\n",
      "29639/29639 [==============================] - 2s 68us/step - loss: 0.8439 - acc: 0.6883 - val_loss: 0.9234 - val_acc: 0.6606\n",
      "Epoch 15/80\n",
      "29639/29639 [==============================] - 2s 69us/step - loss: 0.8043 - acc: 0.7101 - val_loss: 0.9462 - val_acc: 0.6626\n",
      "Epoch 16/80\n",
      "29639/29639 [==============================] - 2s 69us/step - loss: 0.7780 - acc: 0.7226 - val_loss: 0.8974 - val_acc: 0.6767\n",
      "Epoch 17/80\n",
      "29639/29639 [==============================] - 2s 68us/step - loss: 0.7576 - acc: 0.7368 - val_loss: 0.8762 - val_acc: 0.6923\n",
      "Epoch 18/80\n",
      "29639/29639 [==============================] - 2s 67us/step - loss: 0.7459 - acc: 0.7421 - val_loss: 0.8469 - val_acc: 0.7114\n",
      "Epoch 19/80\n",
      "29639/29639 [==============================] - 2s 66us/step - loss: 0.7358 - acc: 0.7463 - val_loss: 0.8835 - val_acc: 0.6986\n",
      "Epoch 20/80\n",
      "29639/29639 [==============================] - 2s 66us/step - loss: 0.7264 - acc: 0.7505 - val_loss: 0.9469 - val_acc: 0.6713\n",
      "Epoch 21/80\n",
      "29639/29639 [==============================] - 2s 68us/step - loss: 0.7125 - acc: 0.7557 - val_loss: 0.8364 - val_acc: 0.7240\n",
      "Epoch 22/80\n",
      "29639/29639 [==============================] - 2s 71us/step - loss: 0.7045 - acc: 0.7596 - val_loss: 0.8225 - val_acc: 0.7292\n",
      "Epoch 23/80\n",
      "29639/29639 [==============================] - 2s 70us/step - loss: 0.7009 - acc: 0.7609 - val_loss: 0.8270 - val_acc: 0.7217\n",
      "Epoch 24/80\n",
      "29639/29639 [==============================] - 2s 71us/step - loss: 0.6827 - acc: 0.7675 - val_loss: 0.8221 - val_acc: 0.7204\n",
      "Epoch 25/80\n",
      "29639/29639 [==============================] - 2s 66us/step - loss: 0.6876 - acc: 0.7647 - val_loss: 0.8130 - val_acc: 0.7309\n",
      "Epoch 26/80\n",
      "29639/29639 [==============================] - 2s 69us/step - loss: 0.6798 - acc: 0.7707 - val_loss: 0.8451 - val_acc: 0.7167\n",
      "Epoch 27/80\n",
      "29639/29639 [==============================] - 2s 66us/step - loss: 0.6719 - acc: 0.7724 - val_loss: 0.7977 - val_acc: 0.7330\n",
      "Epoch 28/80\n",
      "29639/29639 [==============================] - 2s 66us/step - loss: 0.6689 - acc: 0.7726 - val_loss: 0.7850 - val_acc: 0.7377\n",
      "Epoch 29/80\n",
      "29639/29639 [==============================] - 2s 66us/step - loss: 0.6696 - acc: 0.7734 - val_loss: 0.8080 - val_acc: 0.7282\n",
      "Epoch 30/80\n",
      "29639/29639 [==============================] - 2s 69us/step - loss: 0.6571 - acc: 0.7766 - val_loss: 0.8218 - val_acc: 0.7273\n",
      "Epoch 31/80\n",
      "29639/29639 [==============================] - 2s 68us/step - loss: 0.6546 - acc: 0.7798 - val_loss: 0.8292 - val_acc: 0.7331\n",
      "Epoch 32/80\n",
      "29639/29639 [==============================] - 2s 67us/step - loss: 0.6485 - acc: 0.7795 - val_loss: 0.8126 - val_acc: 0.7317\n",
      "Epoch 33/80\n",
      "29639/29639 [==============================] - 2s 69us/step - loss: 0.6487 - acc: 0.7804 - val_loss: 0.8177 - val_acc: 0.7352\n",
      "Inner step: 25 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 4s 134us/step - loss: 1.4367 - acc: 0.3624 - val_loss: 1.3965 - val_acc: 0.3870\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 2s 70us/step - loss: 1.3533 - acc: 0.4040 - val_loss: 1.3727 - val_acc: 0.4032\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 2s 68us/step - loss: 1.3435 - acc: 0.4094 - val_loss: 1.4002 - val_acc: 0.3977\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 2s 69us/step - loss: 1.3358 - acc: 0.4104 - val_loss: 1.3829 - val_acc: 0.3981\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 2s 73us/step - loss: 1.3343 - acc: 0.4120 - val_loss: 1.3677 - val_acc: 0.4052\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 2s 71us/step - loss: 1.3312 - acc: 0.4105 - val_loss: 1.3613 - val_acc: 0.4050\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 2s 71us/step - loss: 1.3296 - acc: 0.4101 - val_loss: 1.3499 - val_acc: 0.4033\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 2s 72us/step - loss: 1.3306 - acc: 0.4117 - val_loss: 1.3697 - val_acc: 0.4043\n",
      "Epoch 9/80\n",
      "29639/29639 [==============================] - 2s 69us/step - loss: 1.3297 - acc: 0.4112 - val_loss: 1.3562 - val_acc: 0.4048\n",
      "Epoch 10/80\n",
      "29639/29639 [==============================] - 2s 73us/step - loss: 1.3268 - acc: 0.4111 - val_loss: 1.4574 - val_acc: 0.3858\n",
      "Epoch 11/80\n",
      "29639/29639 [==============================] - 2s 74us/step - loss: 1.3249 - acc: 0.4119 - val_loss: 1.3597 - val_acc: 0.4031\n",
      "Epoch 12/80\n",
      "29639/29639 [==============================] - 2s 72us/step - loss: 1.3259 - acc: 0.4124 - val_loss: 1.3577 - val_acc: 0.4050\n",
      "Inner step: 26 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 4s 133us/step - loss: 1.4475 - acc: 0.3522 - val_loss: 1.3478 - val_acc: 0.4077\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 2s 71us/step - loss: 1.2928 - acc: 0.4375 - val_loss: 1.2994 - val_acc: 0.4430\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 2s 81us/step - loss: 1.2487 - acc: 0.4621 - val_loss: 1.2414 - val_acc: 0.4616\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 2s 84us/step - loss: 1.2223 - acc: 0.4865 - val_loss: 1.2482 - val_acc: 0.4689\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 2s 73us/step - loss: 1.1885 - acc: 0.5134 - val_loss: 1.2126 - val_acc: 0.5156\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 2s 70us/step - loss: 1.1609 - acc: 0.5252 - val_loss: 1.2389 - val_acc: 0.5166\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 2s 68us/step - loss: 1.1101 - acc: 0.5432 - val_loss: 1.1393 - val_acc: 0.5360\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 2s 66us/step - loss: 1.0900 - acc: 0.5512 - val_loss: 1.1485 - val_acc: 0.5369\n",
      "Epoch 9/80\n",
      "29639/29639 [==============================] - 2s 72us/step - loss: 1.0796 - acc: 0.5601 - val_loss: 1.1784 - val_acc: 0.5339\n",
      "Epoch 10/80\n",
      "29639/29639 [==============================] - 2s 71us/step - loss: 1.0699 - acc: 0.5688 - val_loss: 1.1390 - val_acc: 0.5529\n",
      "Epoch 11/80\n",
      "29639/29639 [==============================] - 2s 66us/step - loss: 1.0492 - acc: 0.5782 - val_loss: 1.1011 - val_acc: 0.5641\n",
      "Epoch 12/80\n",
      "29639/29639 [==============================] - 2s 67us/step - loss: 1.0349 - acc: 0.5850 - val_loss: 1.2222 - val_acc: 0.5404\n",
      "Epoch 13/80\n",
      "29639/29639 [==============================] - 2s 66us/step - loss: 1.0229 - acc: 0.5901 - val_loss: 1.1376 - val_acc: 0.5515\n",
      "Epoch 14/80\n",
      "29639/29639 [==============================] - 2s 66us/step - loss: 1.0129 - acc: 0.5932 - val_loss: 1.1102 - val_acc: 0.5622\n",
      "Epoch 15/80\n",
      "29639/29639 [==============================] - 2s 68us/step - loss: 0.9957 - acc: 0.6094 - val_loss: 1.1212 - val_acc: 0.5768\n",
      "Epoch 16/80\n",
      "29639/29639 [==============================] - 2s 70us/step - loss: 0.9577 - acc: 0.6380 - val_loss: 1.0045 - val_acc: 0.6330\n",
      "Epoch 17/80\n",
      "29639/29639 [==============================] - 2s 69us/step - loss: 0.9102 - acc: 0.6642 - val_loss: 0.9832 - val_acc: 0.6363\n",
      "Epoch 18/80\n",
      "29639/29639 [==============================] - 2s 69us/step - loss: 0.8776 - acc: 0.6825 - val_loss: 1.0035 - val_acc: 0.6205\n",
      "Epoch 19/80\n",
      "29639/29639 [==============================] - 2s 71us/step - loss: 0.8521 - acc: 0.6929 - val_loss: 0.9551 - val_acc: 0.6577\n",
      "Epoch 20/80\n",
      "29639/29639 [==============================] - 2s 68us/step - loss: 0.8081 - acc: 0.7168 - val_loss: 0.9207 - val_acc: 0.6894\n",
      "Epoch 21/80\n",
      "29639/29639 [==============================] - 2s 68us/step - loss: 0.7695 - acc: 0.7414 - val_loss: 0.8485 - val_acc: 0.7254\n",
      "Epoch 22/80\n",
      "29639/29639 [==============================] - 2s 75us/step - loss: 0.7280 - acc: 0.7617 - val_loss: 0.8868 - val_acc: 0.7145\n",
      "Epoch 23/80\n",
      "29639/29639 [==============================] - 2s 67us/step - loss: 0.7047 - acc: 0.7689 - val_loss: 0.8027 - val_acc: 0.7460\n",
      "Epoch 24/80\n",
      "29639/29639 [==============================] - 2s 66us/step - loss: 0.6822 - acc: 0.7794 - val_loss: 0.8637 - val_acc: 0.7276\n",
      "Epoch 25/80\n",
      "29639/29639 [==============================] - 2s 67us/step - loss: 0.6686 - acc: 0.7850 - val_loss: 0.8122 - val_acc: 0.7345\n",
      "Epoch 26/80\n",
      "29639/29639 [==============================] - 2s 69us/step - loss: 0.6565 - acc: 0.7890 - val_loss: 0.7834 - val_acc: 0.7495\n",
      "Epoch 27/80\n",
      "29639/29639 [==============================] - 2s 69us/step - loss: 0.6463 - acc: 0.7920 - val_loss: 0.7889 - val_acc: 0.7462\n",
      "Epoch 28/80\n",
      "29639/29639 [==============================] - 2s 67us/step - loss: 0.6391 - acc: 0.7939 - val_loss: 0.8169 - val_acc: 0.7308\n",
      "Epoch 29/80\n",
      "29639/29639 [==============================] - 2s 67us/step - loss: 0.6304 - acc: 0.7966 - val_loss: 0.7950 - val_acc: 0.7388\n",
      "Epoch 30/80\n",
      "29639/29639 [==============================] - 2s 67us/step - loss: 0.6324 - acc: 0.7948 - val_loss: 0.8046 - val_acc: 0.7389\n",
      "Epoch 31/80\n",
      "29639/29639 [==============================] - 2s 67us/step - loss: 0.6184 - acc: 0.8008 - val_loss: 0.7583 - val_acc: 0.7539\n",
      "Epoch 32/80\n",
      "29639/29639 [==============================] - 2s 68us/step - loss: 0.6141 - acc: 0.8009 - val_loss: 0.8230 - val_acc: 0.7307\n",
      "Epoch 33/80\n",
      "29639/29639 [==============================] - 2s 68us/step - loss: 0.6133 - acc: 0.8009 - val_loss: 0.7463 - val_acc: 0.7585\n",
      "Epoch 34/80\n",
      "29639/29639 [==============================] - 2s 67us/step - loss: 0.6024 - acc: 0.8025 - val_loss: 0.8081 - val_acc: 0.7424\n",
      "Epoch 35/80\n",
      "29639/29639 [==============================] - 2s 68us/step - loss: 0.6000 - acc: 0.8046 - val_loss: 0.7357 - val_acc: 0.7586\n",
      "Epoch 36/80\n",
      "29639/29639 [==============================] - 2s 69us/step - loss: 0.5962 - acc: 0.8034 - val_loss: 0.7737 - val_acc: 0.7510\n",
      "Epoch 37/80\n",
      "29639/29639 [==============================] - 2s 68us/step - loss: 0.5985 - acc: 0.8051 - val_loss: 0.8015 - val_acc: 0.7320\n",
      "Epoch 38/80\n",
      "29639/29639 [==============================] - 2s 69us/step - loss: 0.5875 - acc: 0.8076 - val_loss: 0.7450 - val_acc: 0.7571\n",
      "Epoch 39/80\n",
      "29639/29639 [==============================] - 2s 71us/step - loss: 0.5854 - acc: 0.8068 - val_loss: 0.7935 - val_acc: 0.7431\n",
      "Epoch 40/80\n",
      "29639/29639 [==============================] - 2s 67us/step - loss: 0.5839 - acc: 0.8085 - val_loss: 0.7437 - val_acc: 0.7604\n",
      "Inner step: 27 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 4s 129us/step - loss: 1.5428 - acc: 0.2806 - val_loss: 1.5060 - val_acc: 0.3087\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 2s 68us/step - loss: 1.4829 - acc: 0.3197 - val_loss: 1.4424 - val_acc: 0.3548\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 2s 68us/step - loss: 1.4224 - acc: 0.3795 - val_loss: 1.4134 - val_acc: 0.3803\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 2s 74us/step - loss: 1.3909 - acc: 0.4053 - val_loss: 1.4058 - val_acc: 0.4096\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 2s 71us/step - loss: 1.3761 - acc: 0.4222 - val_loss: 1.3758 - val_acc: 0.4312\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 2s 69us/step - loss: 1.3303 - acc: 0.4588 - val_loss: 1.3605 - val_acc: 0.4448\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 2s 74us/step - loss: 1.2898 - acc: 0.4788 - val_loss: 1.3169 - val_acc: 0.4759\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 2s 67us/step - loss: 1.2639 - acc: 0.4895 - val_loss: 1.3072 - val_acc: 0.4719\n",
      "Epoch 9/80\n",
      "29639/29639 [==============================] - 2s 70us/step - loss: 1.2477 - acc: 0.4974 - val_loss: 1.2912 - val_acc: 0.4796\n",
      "Epoch 10/80\n",
      "29639/29639 [==============================] - 2s 81us/step - loss: 1.2330 - acc: 0.5086 - val_loss: 1.2456 - val_acc: 0.5085\n",
      "Epoch 11/80\n",
      "29639/29639 [==============================] - 3s 90us/step - loss: 1.1745 - acc: 0.5395 - val_loss: 1.2095 - val_acc: 0.5293\n",
      "Epoch 12/80\n",
      "29639/29639 [==============================] - 2s 69us/step - loss: 1.1307 - acc: 0.5602 - val_loss: 1.2349 - val_acc: 0.5209\n",
      "Epoch 13/80\n",
      "29639/29639 [==============================] - 3s 95us/step - loss: 1.0836 - acc: 0.5840 - val_loss: 1.1628 - val_acc: 0.5546\n",
      "Epoch 14/80\n",
      "29639/29639 [==============================] - 3s 92us/step - loss: 1.0505 - acc: 0.6039 - val_loss: 1.1047 - val_acc: 0.5887\n",
      "Epoch 15/80\n",
      "29639/29639 [==============================] - 2s 69us/step - loss: 1.0260 - acc: 0.6170 - val_loss: 1.1747 - val_acc: 0.5587\n",
      "Epoch 16/80\n",
      "29639/29639 [==============================] - 2s 70us/step - loss: 1.0144 - acc: 0.6235 - val_loss: 1.1275 - val_acc: 0.5851\n",
      "Epoch 17/80\n",
      "29639/29639 [==============================] - 2s 73us/step - loss: 0.9726 - acc: 0.6401 - val_loss: 1.0138 - val_acc: 0.6351\n",
      "Epoch 18/80\n",
      "29639/29639 [==============================] - 2s 71us/step - loss: 0.8850 - acc: 0.6828 - val_loss: 1.0084 - val_acc: 0.6428\n",
      "Epoch 19/80\n",
      "29639/29639 [==============================] - 2s 74us/step - loss: 0.8389 - acc: 0.7051 - val_loss: 0.9816 - val_acc: 0.6671\n",
      "Epoch 20/80\n",
      "29639/29639 [==============================] - 2s 75us/step - loss: 0.8075 - acc: 0.7190 - val_loss: 0.9444 - val_acc: 0.6791\n",
      "Epoch 21/80\n",
      "29639/29639 [==============================] - 2s 73us/step - loss: 0.7871 - acc: 0.7279 - val_loss: 0.8903 - val_acc: 0.7012\n",
      "Epoch 22/80\n",
      "29639/29639 [==============================] - 2s 72us/step - loss: 0.7815 - acc: 0.7296 - val_loss: 0.9079 - val_acc: 0.6948\n",
      "Epoch 23/80\n",
      "29639/29639 [==============================] - 2s 69us/step - loss: 0.7619 - acc: 0.7376 - val_loss: 0.8747 - val_acc: 0.7090\n",
      "Epoch 24/80\n",
      "29639/29639 [==============================] - 2s 74us/step - loss: 0.7527 - acc: 0.7427 - val_loss: 0.8850 - val_acc: 0.6983\n",
      "Epoch 25/80\n",
      "29639/29639 [==============================] - 2s 73us/step - loss: 0.7446 - acc: 0.7432 - val_loss: 0.9309 - val_acc: 0.6907\n",
      "Epoch 26/80\n",
      "29639/29639 [==============================] - 2s 68us/step - loss: 0.7454 - acc: 0.7442 - val_loss: 0.9423 - val_acc: 0.6823\n",
      "Epoch 27/80\n",
      "29639/29639 [==============================] - 2s 75us/step - loss: 0.7306 - acc: 0.7485 - val_loss: 0.8524 - val_acc: 0.7112\n",
      "Epoch 28/80\n",
      "29639/29639 [==============================] - 2s 74us/step - loss: 0.7224 - acc: 0.7532 - val_loss: 0.8603 - val_acc: 0.7082\n",
      "Epoch 29/80\n",
      "29639/29639 [==============================] - 2s 70us/step - loss: 0.7116 - acc: 0.7558 - val_loss: 0.9218 - val_acc: 0.6942\n",
      "Epoch 30/80\n",
      "29639/29639 [==============================] - 2s 68us/step - loss: 0.7033 - acc: 0.7581 - val_loss: 0.8485 - val_acc: 0.7170\n",
      "Epoch 31/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29639/29639 [==============================] - 2s 71us/step - loss: 0.7000 - acc: 0.7586 - val_loss: 0.8599 - val_acc: 0.7048\n",
      "Epoch 32/80\n",
      "29639/29639 [==============================] - 2s 74us/step - loss: 0.6917 - acc: 0.7626 - val_loss: 0.7991 - val_acc: 0.7329\n",
      "Epoch 33/80\n",
      "29639/29639 [==============================] - 2s 72us/step - loss: 0.6817 - acc: 0.7654 - val_loss: 0.8228 - val_acc: 0.7209\n",
      "Epoch 34/80\n",
      "29639/29639 [==============================] - 2s 72us/step - loss: 0.6763 - acc: 0.7670 - val_loss: 0.8677 - val_acc: 0.7095\n",
      "Epoch 35/80\n",
      "29639/29639 [==============================] - 2s 69us/step - loss: 0.6775 - acc: 0.7668 - val_loss: 0.9018 - val_acc: 0.6908\n",
      "Epoch 36/80\n",
      "29639/29639 [==============================] - 2s 71us/step - loss: 0.6552 - acc: 0.7752 - val_loss: 0.8005 - val_acc: 0.7382\n",
      "Epoch 37/80\n",
      "29639/29639 [==============================] - 2s 72us/step - loss: 0.6557 - acc: 0.7746 - val_loss: 0.8178 - val_acc: 0.7324\n",
      "Inner step: 28 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 4s 135us/step - loss: 1.4893 - acc: 0.3197 - val_loss: 1.4458 - val_acc: 0.3594\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 2s 73us/step - loss: 1.4104 - acc: 0.3625 - val_loss: 1.4295 - val_acc: 0.3552\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 2s 71us/step - loss: 1.3937 - acc: 0.3766 - val_loss: 1.4216 - val_acc: 0.3856\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 2s 67us/step - loss: 1.3731 - acc: 0.4077 - val_loss: 1.3921 - val_acc: 0.4147\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 2s 69us/step - loss: 1.3046 - acc: 0.4479 - val_loss: 1.3356 - val_acc: 0.4482\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 2s 75us/step - loss: 1.2675 - acc: 0.4639 - val_loss: 1.4643 - val_acc: 0.4257\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 2s 71us/step - loss: 1.2073 - acc: 0.5041 - val_loss: 1.3131 - val_acc: 0.4835\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 2s 69us/step - loss: 1.1584 - acc: 0.5277 - val_loss: 1.2340 - val_acc: 0.5110\n",
      "Epoch 9/80\n",
      "29639/29639 [==============================] - 2s 69us/step - loss: 1.1403 - acc: 0.5335 - val_loss: 1.2426 - val_acc: 0.5078\n",
      "Epoch 10/80\n",
      "29639/29639 [==============================] - 2s 69us/step - loss: 1.1278 - acc: 0.5378 - val_loss: 1.1824 - val_acc: 0.5249\n",
      "Epoch 11/80\n",
      "29639/29639 [==============================] - 2s 71us/step - loss: 1.1260 - acc: 0.5370 - val_loss: 1.1691 - val_acc: 0.5286\n",
      "Epoch 12/80\n",
      "29639/29639 [==============================] - 2s 73us/step - loss: 1.1171 - acc: 0.5373 - val_loss: 1.1843 - val_acc: 0.5231\n",
      "Epoch 13/80\n",
      "29639/29639 [==============================] - 2s 72us/step - loss: 1.1101 - acc: 0.5413 - val_loss: 1.2082 - val_acc: 0.5216\n",
      "Epoch 14/80\n",
      "29639/29639 [==============================] - 2s 68us/step - loss: 1.1086 - acc: 0.5421 - val_loss: 1.1980 - val_acc: 0.5190\n",
      "Epoch 15/80\n",
      "29639/29639 [==============================] - 2s 69us/step - loss: 1.1081 - acc: 0.5403 - val_loss: 1.2322 - val_acc: 0.5189\n",
      "Epoch 16/80\n",
      "29639/29639 [==============================] - 2s 68us/step - loss: 1.1032 - acc: 0.5438 - val_loss: 1.1906 - val_acc: 0.5145\n",
      "Inner step: 29 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 4s 131us/step - loss: 1.5468 - acc: 0.2819 - val_loss: 1.5085 - val_acc: 0.2929\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 2s 74us/step - loss: 1.5075 - acc: 0.3020 - val_loss: 1.5149 - val_acc: 0.2873\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 2s 73us/step - loss: 1.5058 - acc: 0.3034 - val_loss: 1.5092 - val_acc: 0.3097\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 2s 69us/step - loss: 1.5034 - acc: 0.3038 - val_loss: 1.5079 - val_acc: 0.3112\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 2s 72us/step - loss: 1.4491 - acc: 0.3474 - val_loss: 1.3496 - val_acc: 0.4352\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 2s 70us/step - loss: 1.2374 - acc: 0.4869 - val_loss: 1.2181 - val_acc: 0.5118\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 2s 69us/step - loss: 1.1988 - acc: 0.5130 - val_loss: 1.2193 - val_acc: 0.5116\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 2s 72us/step - loss: 1.1697 - acc: 0.5284 - val_loss: 1.2451 - val_acc: 0.4937\n",
      "Epoch 9/80\n",
      "29639/29639 [==============================] - 2s 74us/step - loss: 1.1353 - acc: 0.5396 - val_loss: 1.1793 - val_acc: 0.5224\n",
      "Epoch 10/80\n",
      "29639/29639 [==============================] - 2s 77us/step - loss: 1.1112 - acc: 0.5467 - val_loss: 1.2227 - val_acc: 0.5222\n",
      "Epoch 11/80\n",
      "29639/29639 [==============================] - 2s 71us/step - loss: 1.0983 - acc: 0.5497 - val_loss: 1.1426 - val_acc: 0.5343\n",
      "Epoch 12/80\n",
      "29639/29639 [==============================] - 2s 69us/step - loss: 1.0869 - acc: 0.5517 - val_loss: 1.1734 - val_acc: 0.5311\n",
      "Epoch 13/80\n",
      "29639/29639 [==============================] - 2s 69us/step - loss: 1.0813 - acc: 0.5566 - val_loss: 1.1320 - val_acc: 0.5376\n",
      "Epoch 14/80\n",
      "29639/29639 [==============================] - 2s 67us/step - loss: 1.0787 - acc: 0.5564 - val_loss: 1.1440 - val_acc: 0.5305\n",
      "Epoch 15/80\n",
      "29639/29639 [==============================] - 2s 74us/step - loss: 1.0714 - acc: 0.5569 - val_loss: 1.2310 - val_acc: 0.5193\n",
      "Epoch 16/80\n",
      "29639/29639 [==============================] - 2s 69us/step - loss: 1.0712 - acc: 0.5605 - val_loss: 1.1403 - val_acc: 0.5412\n",
      "Epoch 17/80\n",
      "29639/29639 [==============================] - 2s 68us/step - loss: 1.0666 - acc: 0.5620 - val_loss: 1.1499 - val_acc: 0.5439\n",
      "Epoch 18/80\n",
      "29639/29639 [==============================] - 2s 73us/step - loss: 1.0526 - acc: 0.5757 - val_loss: 1.1202 - val_acc: 0.5543\n",
      "Epoch 19/80\n",
      "29639/29639 [==============================] - 2s 68us/step - loss: 1.0373 - acc: 0.5803 - val_loss: 1.1257 - val_acc: 0.5573\n",
      "Epoch 20/80\n",
      "29639/29639 [==============================] - 2s 70us/step - loss: 1.0239 - acc: 0.5875 - val_loss: 1.1110 - val_acc: 0.5565\n",
      "Epoch 21/80\n",
      "29639/29639 [==============================] - 2s 68us/step - loss: 1.0154 - acc: 0.5939 - val_loss: 1.0854 - val_acc: 0.5625\n",
      "Epoch 22/80\n",
      "29639/29639 [==============================] - 2s 79us/step - loss: 1.0070 - acc: 0.5955 - val_loss: 1.1337 - val_acc: 0.5541\n",
      "Epoch 23/80\n",
      "29639/29639 [==============================] - 2s 70us/step - loss: 1.0037 - acc: 0.6005 - val_loss: 1.0770 - val_acc: 0.5842\n",
      "Epoch 24/80\n",
      "29639/29639 [==============================] - 2s 73us/step - loss: 0.9608 - acc: 0.6300 - val_loss: 1.0422 - val_acc: 0.5997\n",
      "Epoch 25/80\n",
      "29639/29639 [==============================] - 2s 70us/step - loss: 0.9240 - acc: 0.6589 - val_loss: 1.0043 - val_acc: 0.6477\n",
      "Epoch 26/80\n",
      "29639/29639 [==============================] - 2s 69us/step - loss: 0.8821 - acc: 0.6858 - val_loss: 1.0110 - val_acc: 0.6536\n",
      "Epoch 27/80\n",
      "29639/29639 [==============================] - 2s 68us/step - loss: 0.8591 - acc: 0.7001 - val_loss: 0.9331 - val_acc: 0.6753\n",
      "Epoch 28/80\n",
      "29639/29639 [==============================] - 2s 67us/step - loss: 0.8413 - acc: 0.7084 - val_loss: 0.9411 - val_acc: 0.6776\n",
      "Epoch 29/80\n",
      "29639/29639 [==============================] - 2s 68us/step - loss: 0.8192 - acc: 0.7173 - val_loss: 0.9525 - val_acc: 0.6784\n",
      "Epoch 30/80\n",
      "29639/29639 [==============================] - 2s 68us/step - loss: 0.8128 - acc: 0.7196 - val_loss: 0.9353 - val_acc: 0.6778\n",
      "Epoch 31/80\n",
      "29639/29639 [==============================] - 2s 68us/step - loss: 0.8065 - acc: 0.7208 - val_loss: 0.9283 - val_acc: 0.6823\n",
      "Epoch 32/80\n",
      "29639/29639 [==============================] - 2s 72us/step - loss: 0.7955 - acc: 0.7288 - val_loss: 0.9151 - val_acc: 0.6886\n",
      "Epoch 33/80\n",
      "29639/29639 [==============================] - 2s 70us/step - loss: 0.7947 - acc: 0.7284 - val_loss: 0.9670 - val_acc: 0.6771\n",
      "Epoch 34/80\n",
      "29639/29639 [==============================] - 2s 68us/step - loss: 0.7836 - acc: 0.7318 - val_loss: 0.9317 - val_acc: 0.6920\n",
      "Epoch 35/80\n",
      "29639/29639 [==============================] - 2s 67us/step - loss: 0.7729 - acc: 0.7345 - val_loss: 0.9458 - val_acc: 0.6743\n",
      "Epoch 36/80\n",
      "29639/29639 [==============================] - 2s 67us/step - loss: 0.7634 - acc: 0.7395 - val_loss: 0.8836 - val_acc: 0.7069\n",
      "Epoch 37/80\n",
      "29639/29639 [==============================] - 2s 67us/step - loss: 0.7582 - acc: 0.7388 - val_loss: 0.9399 - val_acc: 0.6855\n",
      "Epoch 38/80\n",
      "29639/29639 [==============================] - 2s 68us/step - loss: 0.7503 - acc: 0.7447 - val_loss: 0.9225 - val_acc: 0.6860\n",
      "Epoch 39/80\n",
      "29639/29639 [==============================] - 2s 68us/step - loss: 0.7433 - acc: 0.7463 - val_loss: 0.8842 - val_acc: 0.7063\n",
      "Epoch 40/80\n",
      "29639/29639 [==============================] - 2s 69us/step - loss: 0.7435 - acc: 0.7462 - val_loss: 0.8779 - val_acc: 0.7018\n",
      "Epoch 41/80\n",
      "29639/29639 [==============================] - 2s 73us/step - loss: 0.7368 - acc: 0.7475 - val_loss: 0.8683 - val_acc: 0.7108\n",
      "Epoch 42/80\n",
      "29639/29639 [==============================] - 2s 69us/step - loss: 0.7347 - acc: 0.7510 - val_loss: 0.8670 - val_acc: 0.7123\n",
      "Epoch 43/80\n",
      "29639/29639 [==============================] - 2s 69us/step - loss: 0.7255 - acc: 0.7557 - val_loss: 0.8922 - val_acc: 0.7020\n",
      "Epoch 44/80\n",
      "29639/29639 [==============================] - 2s 73us/step - loss: 0.7188 - acc: 0.7564 - val_loss: 0.8668 - val_acc: 0.7173\n",
      "Epoch 45/80\n",
      "29639/29639 [==============================] - 2s 74us/step - loss: 0.7114 - acc: 0.7617 - val_loss: 0.8902 - val_acc: 0.7014\n",
      "Epoch 46/80\n",
      "29639/29639 [==============================] - 2s 69us/step - loss: 0.6962 - acc: 0.7668 - val_loss: 0.8508 - val_acc: 0.7159\n",
      "Epoch 47/80\n",
      "29639/29639 [==============================] - 2s 71us/step - loss: 0.6848 - acc: 0.7711 - val_loss: 0.8217 - val_acc: 0.7333\n",
      "Epoch 48/80\n",
      "29639/29639 [==============================] - 2s 72us/step - loss: 0.6718 - acc: 0.7769 - val_loss: 0.8394 - val_acc: 0.7310\n",
      "Epoch 49/80\n",
      "29639/29639 [==============================] - 2s 68us/step - loss: 0.6627 - acc: 0.7811 - val_loss: 0.7981 - val_acc: 0.7332\n",
      "Epoch 50/80\n",
      "29639/29639 [==============================] - 2s 67us/step - loss: 0.6522 - acc: 0.7837 - val_loss: 0.7908 - val_acc: 0.7411\n",
      "Epoch 51/80\n",
      "29639/29639 [==============================] - 2s 69us/step - loss: 0.6460 - acc: 0.7836 - val_loss: 0.8047 - val_acc: 0.7320\n",
      "Epoch 52/80\n",
      "29639/29639 [==============================] - 2s 75us/step - loss: 0.6353 - acc: 0.7878 - val_loss: 0.7951 - val_acc: 0.7402\n",
      "Epoch 53/80\n",
      "29639/29639 [==============================] - 2s 72us/step - loss: 0.6328 - acc: 0.7897 - val_loss: 0.8059 - val_acc: 0.7322\n",
      "Epoch 54/80\n",
      "29639/29639 [==============================] - 2s 70us/step - loss: 0.6188 - acc: 0.7934 - val_loss: 0.8259 - val_acc: 0.7379\n",
      "Epoch 55/80\n",
      "29639/29639 [==============================] - 2s 69us/step - loss: 0.6166 - acc: 0.7922 - val_loss: 0.8206 - val_acc: 0.7353\n",
      "Inner step: 30 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 4s 146us/step - loss: 1.4330 - acc: 0.3577 - val_loss: 1.3402 - val_acc: 0.4146\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 2s 84us/step - loss: 1.2907 - acc: 0.4333 - val_loss: 1.2845 - val_acc: 0.4445\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 3s 89us/step - loss: 1.2468 - acc: 0.4616 - val_loss: 1.2625 - val_acc: 0.4929\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 2s 80us/step - loss: 1.2149 - acc: 0.4939 - val_loss: 1.2377 - val_acc: 0.4956\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 2s 75us/step - loss: 1.1930 - acc: 0.5103 - val_loss: 1.2094 - val_acc: 0.5178\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 2s 77us/step - loss: 1.1749 - acc: 0.5232 - val_loss: 1.1880 - val_acc: 0.5166\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 2s 75us/step - loss: 1.1415 - acc: 0.5359 - val_loss: 1.1967 - val_acc: 0.5266\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 2s 73us/step - loss: 1.1105 - acc: 0.5429 - val_loss: 1.1311 - val_acc: 0.5415\n",
      "Epoch 9/80\n",
      "29639/29639 [==============================] - 2s 73us/step - loss: 1.1000 - acc: 0.5460 - val_loss: 1.1482 - val_acc: 0.5301\n",
      "Epoch 10/80\n",
      "29639/29639 [==============================] - 2s 71us/step - loss: 1.0926 - acc: 0.5450 - val_loss: 1.1269 - val_acc: 0.5409\n",
      "Epoch 11/80\n",
      "29639/29639 [==============================] - 2s 76us/step - loss: 1.0842 - acc: 0.5473 - val_loss: 1.1583 - val_acc: 0.5382\n",
      "Epoch 12/80\n",
      "29639/29639 [==============================] - 2s 71us/step - loss: 1.0779 - acc: 0.5504 - val_loss: 1.1430 - val_acc: 0.5366\n",
      "Epoch 13/80\n",
      "29639/29639 [==============================] - 2s 75us/step - loss: 1.0755 - acc: 0.5514 - val_loss: 1.1422 - val_acc: 0.5311\n",
      "Epoch 14/80\n",
      "29639/29639 [==============================] - 2s 74us/step - loss: 1.0662 - acc: 0.5552 - val_loss: 1.1348 - val_acc: 0.5384\n",
      "Epoch 15/80\n",
      "29639/29639 [==============================] - 2s 71us/step - loss: 1.0605 - acc: 0.5587 - val_loss: 1.1393 - val_acc: 0.5415\n",
      "Inner step: 31 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 4s 133us/step - loss: 1.5380 - acc: 0.2878 - val_loss: 1.5308 - val_acc: 0.3093\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 2s 72us/step - loss: 1.5058 - acc: 0.3028 - val_loss: 1.5067 - val_acc: 0.3067\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 2s 77us/step - loss: 1.5034 - acc: 0.3043 - val_loss: 1.5064 - val_acc: 0.3121\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 2s 68us/step - loss: 1.4945 - acc: 0.3034 - val_loss: 1.4468 - val_acc: 0.3133\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 2s 68us/step - loss: 1.4176 - acc: 0.3625 - val_loss: 1.4601 - val_acc: 0.3887\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 2s 70us/step - loss: 1.3308 - acc: 0.4303 - val_loss: 1.2863 - val_acc: 0.4527\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 2s 70us/step - loss: 1.2024 - acc: 0.5097 - val_loss: 1.2219 - val_acc: 0.5188\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 2s 69us/step - loss: 1.1859 - acc: 0.5148 - val_loss: 1.2367 - val_acc: 0.5141\n",
      "Epoch 9/80\n",
      "29639/29639 [==============================] - 2s 73us/step - loss: 1.1749 - acc: 0.5212 - val_loss: 1.2123 - val_acc: 0.5158\n",
      "Epoch 10/80\n",
      "29639/29639 [==============================] - 2s 72us/step - loss: 1.1626 - acc: 0.5271 - val_loss: 1.2506 - val_acc: 0.5017\n",
      "Epoch 11/80\n",
      "29639/29639 [==============================] - 2s 72us/step - loss: 1.1498 - acc: 0.5323 - val_loss: 1.1912 - val_acc: 0.5191\n",
      "Epoch 12/80\n",
      "29639/29639 [==============================] - 2s 68us/step - loss: 1.1249 - acc: 0.5394 - val_loss: 1.1828 - val_acc: 0.5185\n",
      "Epoch 13/80\n",
      "29639/29639 [==============================] - 2s 72us/step - loss: 1.1058 - acc: 0.5457 - val_loss: 1.2061 - val_acc: 0.5182\n",
      "Epoch 14/80\n",
      "29639/29639 [==============================] - 2s 77us/step - loss: 1.0931 - acc: 0.5502 - val_loss: 1.1490 - val_acc: 0.5340\n",
      "Epoch 15/80\n",
      "29639/29639 [==============================] - 2s 76us/step - loss: 1.0856 - acc: 0.5503 - val_loss: 1.1639 - val_acc: 0.5389\n",
      "Epoch 16/80\n",
      "29639/29639 [==============================] - 2s 71us/step - loss: 1.0800 - acc: 0.5493 - val_loss: 1.1414 - val_acc: 0.5368\n",
      "Epoch 17/80\n",
      "29639/29639 [==============================] - 2s 70us/step - loss: 1.0759 - acc: 0.5535 - val_loss: 1.2122 - val_acc: 0.5284\n",
      "Epoch 18/80\n",
      "29639/29639 [==============================] - 2s 77us/step - loss: 1.0727 - acc: 0.5538 - val_loss: 1.1318 - val_acc: 0.5407\n",
      "Epoch 19/80\n",
      "29639/29639 [==============================] - 2s 75us/step - loss: 1.0712 - acc: 0.5551 - val_loss: 1.1312 - val_acc: 0.5414\n",
      "Epoch 20/80\n",
      "29639/29639 [==============================] - 2s 68us/step - loss: 1.0651 - acc: 0.5575 - val_loss: 1.1771 - val_acc: 0.5342\n",
      "Epoch 21/80\n",
      "29639/29639 [==============================] - 2s 71us/step - loss: 1.0642 - acc: 0.5579 - val_loss: 1.1435 - val_acc: 0.5468\n",
      "Epoch 22/80\n",
      "29639/29639 [==============================] - 2s 74us/step - loss: 1.0591 - acc: 0.5600 - val_loss: 1.1378 - val_acc: 0.5430\n",
      "Epoch 23/80\n",
      "29639/29639 [==============================] - 2s 71us/step - loss: 1.0569 - acc: 0.5586 - val_loss: 1.1511 - val_acc: 0.5328\n",
      "Inner step: 32 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29639/29639 [==============================] - 4s 135us/step - loss: 1.4923 - acc: 0.3216 - val_loss: 1.3383 - val_acc: 0.4050\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 2s 71us/step - loss: 1.3089 - acc: 0.4157 - val_loss: 1.3139 - val_acc: 0.4171\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 2s 70us/step - loss: 1.2826 - acc: 0.4210 - val_loss: 1.2924 - val_acc: 0.4191\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 2s 70us/step - loss: 1.2669 - acc: 0.4301 - val_loss: 1.2852 - val_acc: 0.4437\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 2s 69us/step - loss: 1.2468 - acc: 0.4574 - val_loss: 1.2557 - val_acc: 0.4951\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 2s 69us/step - loss: 1.1661 - acc: 0.5102 - val_loss: 1.1688 - val_acc: 0.5297\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 2s 69us/step - loss: 1.1193 - acc: 0.5312 - val_loss: 1.1567 - val_acc: 0.5181\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 2s 69us/step - loss: 1.0730 - acc: 0.5578 - val_loss: 1.1302 - val_acc: 0.5386\n",
      "Epoch 9/80\n",
      "29639/29639 [==============================] - 2s 69us/step - loss: 1.0340 - acc: 0.5784 - val_loss: 1.1034 - val_acc: 0.5523\n",
      "Epoch 10/80\n",
      "29639/29639 [==============================] - 2s 69us/step - loss: 1.0162 - acc: 0.5883 - val_loss: 1.1119 - val_acc: 0.5464\n",
      "Epoch 11/80\n",
      "29639/29639 [==============================] - 2s 69us/step - loss: 1.0002 - acc: 0.5994 - val_loss: 1.0607 - val_acc: 0.5818\n",
      "Epoch 12/80\n",
      "29639/29639 [==============================] - 2s 69us/step - loss: 0.9809 - acc: 0.6142 - val_loss: 1.0861 - val_acc: 0.5782\n",
      "Epoch 13/80\n",
      "29639/29639 [==============================] - 2s 69us/step - loss: 0.9694 - acc: 0.6211 - val_loss: 1.1825 - val_acc: 0.5391\n",
      "Epoch 14/80\n",
      "29639/29639 [==============================] - 2s 69us/step - loss: 0.9604 - acc: 0.6265 - val_loss: 1.2136 - val_acc: 0.5552\n",
      "Epoch 15/80\n",
      "29639/29639 [==============================] - 2s 75us/step - loss: 0.9537 - acc: 0.6313 - val_loss: 1.0863 - val_acc: 0.6040\n",
      "Epoch 16/80\n",
      "29639/29639 [==============================] - 2s 70us/step - loss: 0.9395 - acc: 0.6401 - val_loss: 1.0546 - val_acc: 0.6132\n",
      "Epoch 17/80\n",
      "29639/29639 [==============================] - 2s 69us/step - loss: 0.9339 - acc: 0.6454 - val_loss: 1.0329 - val_acc: 0.6148\n",
      "Epoch 18/80\n",
      "29639/29639 [==============================] - 2s 76us/step - loss: 0.9262 - acc: 0.6472 - val_loss: 1.0394 - val_acc: 0.6165\n",
      "Epoch 19/80\n",
      "29639/29639 [==============================] - 2s 80us/step - loss: 0.9150 - acc: 0.6573 - val_loss: 1.0196 - val_acc: 0.6201\n",
      "Epoch 20/80\n",
      "29639/29639 [==============================] - 4s 120us/step - loss: 0.9090 - acc: 0.6575 - val_loss: 1.0039 - val_acc: 0.6267\n",
      "Epoch 21/80\n",
      "29639/29639 [==============================] - 3s 109us/step - loss: 0.8965 - acc: 0.6677 - val_loss: 0.9852 - val_acc: 0.6341\n",
      "Epoch 22/80\n",
      "29639/29639 [==============================] - 4s 129us/step - loss: 0.8801 - acc: 0.6751 - val_loss: 1.0229 - val_acc: 0.6238\n",
      "Epoch 23/80\n",
      "29639/29639 [==============================] - 7s 246us/step - loss: 0.8680 - acc: 0.6835 - val_loss: 0.9488 - val_acc: 0.6628\n",
      "Epoch 24/80\n",
      "29639/29639 [==============================] - 5s 161us/step - loss: 0.8512 - acc: 0.6930 - val_loss: 0.9378 - val_acc: 0.6644\n",
      "Epoch 25/80\n",
      "29639/29639 [==============================] - 3s 94us/step - loss: 0.8407 - acc: 0.6957 - val_loss: 0.9392 - val_acc: 0.6677\n",
      "Epoch 26/80\n",
      "29639/29639 [==============================] - 3s 100us/step - loss: 0.8296 - acc: 0.7024 - val_loss: 0.9377 - val_acc: 0.6664\n",
      "Epoch 27/80\n",
      "29639/29639 [==============================] - 3s 95us/step - loss: 0.8327 - acc: 0.7023 - val_loss: 1.0754 - val_acc: 0.6341\n",
      "Epoch 28/80\n",
      "29639/29639 [==============================] - 3s 85us/step - loss: 0.8280 - acc: 0.7087 - val_loss: 0.9684 - val_acc: 0.6612\n",
      "Epoch 29/80\n",
      "29639/29639 [==============================] - 3s 88us/step - loss: 0.8120 - acc: 0.7144 - val_loss: 0.9482 - val_acc: 0.6724\n",
      "Inner step: 33 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 4s 152us/step - loss: 1.5376 - acc: 0.2866 - val_loss: 1.5156 - val_acc: 0.2962\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 2s 83us/step - loss: 1.5054 - acc: 0.3054 - val_loss: 1.5238 - val_acc: 0.3118\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 2s 84us/step - loss: 1.5040 - acc: 0.3051 - val_loss: 1.5031 - val_acc: 0.3111\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 2s 83us/step - loss: 1.5029 - acc: 0.3055 - val_loss: 1.5070 - val_acc: 0.3088\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 2s 83us/step - loss: 1.4562 - acc: 0.3305 - val_loss: 1.4354 - val_acc: 0.3657\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 2s 82us/step - loss: 1.4077 - acc: 0.3667 - val_loss: 1.4130 - val_acc: 0.3649\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 2s 82us/step - loss: 1.3390 - acc: 0.4245 - val_loss: 1.2715 - val_acc: 0.4825\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 2s 82us/step - loss: 1.1553 - acc: 0.5255 - val_loss: 1.1426 - val_acc: 0.5403\n",
      "Epoch 9/80\n",
      "29639/29639 [==============================] - 2s 82us/step - loss: 1.1067 - acc: 0.5520 - val_loss: 1.1286 - val_acc: 0.5457\n",
      "Epoch 10/80\n",
      "29639/29639 [==============================] - 2s 83us/step - loss: 1.0761 - acc: 0.5634 - val_loss: 1.1197 - val_acc: 0.5546\n",
      "Epoch 11/80\n",
      "29639/29639 [==============================] - 2s 83us/step - loss: 1.0353 - acc: 0.5791 - val_loss: 1.1245 - val_acc: 0.5494\n",
      "Epoch 12/80\n",
      "29639/29639 [==============================] - 3s 89us/step - loss: 1.0047 - acc: 0.5922 - val_loss: 1.0753 - val_acc: 0.5766\n",
      "Epoch 13/80\n",
      "29639/29639 [==============================] - 2s 84us/step - loss: 0.9830 - acc: 0.5975 - val_loss: 1.0746 - val_acc: 0.5820\n",
      "Epoch 14/80\n",
      "29639/29639 [==============================] - 3s 87us/step - loss: 0.9640 - acc: 0.6109 - val_loss: 1.0209 - val_acc: 0.5955\n",
      "Epoch 15/80\n",
      "29639/29639 [==============================] - 3s 85us/step - loss: 0.9476 - acc: 0.6235 - val_loss: 1.0398 - val_acc: 0.6031\n",
      "Epoch 16/80\n",
      "29639/29639 [==============================] - 2s 84us/step - loss: 0.9050 - acc: 0.6586 - val_loss: 1.0369 - val_acc: 0.6141\n",
      "Epoch 17/80\n",
      "29639/29639 [==============================] - 3s 98us/step - loss: 0.8580 - acc: 0.6985 - val_loss: 0.9683 - val_acc: 0.6576\n",
      "Epoch 18/80\n",
      "29639/29639 [==============================] - 3s 97us/step - loss: 0.8165 - acc: 0.7147 - val_loss: 0.9076 - val_acc: 0.6796\n",
      "Epoch 19/80\n",
      "29639/29639 [==============================] - 3s 87us/step - loss: 0.7880 - acc: 0.7251 - val_loss: 0.9104 - val_acc: 0.6889\n",
      "Epoch 20/80\n",
      "29639/29639 [==============================] - 3s 86us/step - loss: 0.7732 - acc: 0.7332 - val_loss: 0.8765 - val_acc: 0.7036\n",
      "Epoch 21/80\n",
      "29639/29639 [==============================] - 3s 90us/step - loss: 0.7554 - acc: 0.7400 - val_loss: 0.8625 - val_acc: 0.7095\n",
      "Epoch 22/80\n",
      "29639/29639 [==============================] - 3s 91us/step - loss: 0.7427 - acc: 0.7458 - val_loss: 0.8682 - val_acc: 0.7098\n",
      "Epoch 23/80\n",
      "29639/29639 [==============================] - 2s 84us/step - loss: 0.7301 - acc: 0.7535 - val_loss: 0.8323 - val_acc: 0.7217\n",
      "Epoch 24/80\n",
      "29639/29639 [==============================] - 3s 90us/step - loss: 0.7125 - acc: 0.7579 - val_loss: 0.8775 - val_acc: 0.7060\n",
      "Epoch 25/80\n",
      "29639/29639 [==============================] - 3s 93us/step - loss: 0.7078 - acc: 0.7610 - val_loss: 0.9082 - val_acc: 0.6916\n",
      "Epoch 26/80\n",
      "29639/29639 [==============================] - 3s 91us/step - loss: 0.6957 - acc: 0.7641 - val_loss: 0.8161 - val_acc: 0.7343\n",
      "Epoch 27/80\n",
      "29639/29639 [==============================] - 3s 87us/step - loss: 0.6852 - acc: 0.7686 - val_loss: 0.7982 - val_acc: 0.7314\n",
      "Epoch 28/80\n",
      "29639/29639 [==============================] - 3s 89us/step - loss: 0.6802 - acc: 0.7698 - val_loss: 0.8283 - val_acc: 0.7292\n",
      "Epoch 29/80\n",
      "29639/29639 [==============================] - 3s 87us/step - loss: 0.6661 - acc: 0.7748 - val_loss: 0.8203 - val_acc: 0.7251\n",
      "Epoch 30/80\n",
      "29639/29639 [==============================] - 3s 92us/step - loss: 0.6609 - acc: 0.7755 - val_loss: 0.8775 - val_acc: 0.7093\n",
      "Epoch 31/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29639/29639 [==============================] - 3s 90us/step - loss: 0.6600 - acc: 0.7768 - val_loss: 0.8051 - val_acc: 0.7349\n",
      "Epoch 32/80\n",
      "29639/29639 [==============================] - 3s 87us/step - loss: 0.6559 - acc: 0.7761 - val_loss: 0.7761 - val_acc: 0.7445\n",
      "Epoch 33/80\n",
      "29639/29639 [==============================] - 3s 87us/step - loss: 0.6518 - acc: 0.7808 - val_loss: 0.8715 - val_acc: 0.7123\n",
      "Epoch 34/80\n",
      "29639/29639 [==============================] - 3s 90us/step - loss: 0.6527 - acc: 0.7774 - val_loss: 0.7940 - val_acc: 0.7283\n",
      "Epoch 35/80\n",
      "29639/29639 [==============================] - 3s 92us/step - loss: 0.6363 - acc: 0.7828 - val_loss: 0.8107 - val_acc: 0.7299\n",
      "Epoch 36/80\n",
      "29639/29639 [==============================] - 3s 97us/step - loss: 0.6421 - acc: 0.7810 - val_loss: 0.8084 - val_acc: 0.7242\n",
      "Epoch 37/80\n",
      "29639/29639 [==============================] - 3s 85us/step - loss: 0.6335 - acc: 0.7852 - val_loss: 0.7924 - val_acc: 0.7381\n",
      "Inner step: 34 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 5s 164us/step - loss: 1.5335 - acc: 0.2904 - val_loss: 1.5460 - val_acc: 0.3022\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 3s 96us/step - loss: 1.5047 - acc: 0.3050 - val_loss: 1.5228 - val_acc: 0.3028\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 3s 91us/step - loss: 1.5029 - acc: 0.3058 - val_loss: 1.5313 - val_acc: 0.3053\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 3s 90us/step - loss: 1.5004 - acc: 0.3077 - val_loss: 1.5231 - val_acc: 0.3069\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 3s 91us/step - loss: 1.5012 - acc: 0.3072 - val_loss: 1.5201 - val_acc: 0.3055\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 3s 89us/step - loss: 1.4980 - acc: 0.3076 - val_loss: 1.5366 - val_acc: 0.3067\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 3s 94us/step - loss: 1.4980 - acc: 0.3074 - val_loss: 1.5209 - val_acc: 0.3053\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 3s 88us/step - loss: 1.4956 - acc: 0.3087 - val_loss: 1.5179 - val_acc: 0.3071\n",
      "Epoch 9/80\n",
      "29639/29639 [==============================] - 3s 85us/step - loss: 1.4707 - acc: 0.3417 - val_loss: 1.4471 - val_acc: 0.3666\n",
      "Epoch 10/80\n",
      "29639/29639 [==============================] - 3s 89us/step - loss: 1.3810 - acc: 0.3973 - val_loss: 1.4409 - val_acc: 0.3657\n",
      "Epoch 11/80\n",
      "29639/29639 [==============================] - 3s 91us/step - loss: 1.3508 - acc: 0.4063 - val_loss: 1.4028 - val_acc: 0.3869\n",
      "Epoch 12/80\n",
      "29639/29639 [==============================] - 3s 87us/step - loss: 1.3420 - acc: 0.4072 - val_loss: 1.3574 - val_acc: 0.3997\n",
      "Epoch 13/80\n",
      "29639/29639 [==============================] - 3s 89us/step - loss: 1.3332 - acc: 0.4085 - val_loss: 1.3560 - val_acc: 0.4017\n",
      "Epoch 14/80\n",
      "29639/29639 [==============================] - 3s 90us/step - loss: 1.3308 - acc: 0.4095 - val_loss: 1.3834 - val_acc: 0.3962\n",
      "Epoch 15/80\n",
      "29639/29639 [==============================] - 3s 98us/step - loss: 1.3300 - acc: 0.4092 - val_loss: 1.3462 - val_acc: 0.4032\n",
      "Epoch 16/80\n",
      "29639/29639 [==============================] - 3s 88us/step - loss: 1.3258 - acc: 0.4108 - val_loss: 1.3475 - val_acc: 0.4046\n",
      "Epoch 17/80\n",
      "29639/29639 [==============================] - 3s 90us/step - loss: 1.3243 - acc: 0.4112 - val_loss: 1.3554 - val_acc: 0.4058\n",
      "Epoch 18/80\n",
      "29639/29639 [==============================] - 3s 101us/step - loss: 1.3237 - acc: 0.4117 - val_loss: 1.3523 - val_acc: 0.4031\n",
      "Epoch 19/80\n",
      "29639/29639 [==============================] - 3s 89us/step - loss: 1.3220 - acc: 0.4131 - val_loss: 1.3684 - val_acc: 0.4026\n",
      "Epoch 20/80\n",
      "29639/29639 [==============================] - 3s 93us/step - loss: 1.3246 - acc: 0.4119 - val_loss: 1.3529 - val_acc: 0.4009\n",
      "Inner step: 35 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 5s 168us/step - loss: 1.5372 - acc: 0.2882 - val_loss: 1.6509 - val_acc: 0.2888\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 3s 94us/step - loss: 1.5055 - acc: 0.3044 - val_loss: 1.5683 - val_acc: 0.3049\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 3s 96us/step - loss: 1.5005 - acc: 0.3145 - val_loss: 1.5031 - val_acc: 0.3427\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 3s 90us/step - loss: 1.4374 - acc: 0.3780 - val_loss: 1.4008 - val_acc: 0.3940\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 3s 85us/step - loss: 1.3306 - acc: 0.4342 - val_loss: 1.2363 - val_acc: 0.4927\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 3s 86us/step - loss: 1.1823 - acc: 0.5062 - val_loss: 1.1708 - val_acc: 0.5145\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 3s 97us/step - loss: 1.0751 - acc: 0.5533 - val_loss: 1.0775 - val_acc: 0.5686\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 3s 96us/step - loss: 1.0087 - acc: 0.5853 - val_loss: 1.0850 - val_acc: 0.5763\n",
      "Epoch 9/80\n",
      "29639/29639 [==============================] - 3s 95us/step - loss: 0.9793 - acc: 0.6065 - val_loss: 1.0302 - val_acc: 0.6027\n",
      "Epoch 10/80\n",
      "29639/29639 [==============================] - 3s 98us/step - loss: 0.9489 - acc: 0.6316 - val_loss: 1.0637 - val_acc: 0.6039\n",
      "Epoch 11/80\n",
      "29639/29639 [==============================] - 3s 99us/step - loss: 0.9021 - acc: 0.6549 - val_loss: 0.9716 - val_acc: 0.6340\n",
      "Epoch 12/80\n",
      "29639/29639 [==============================] - 3s 94us/step - loss: 0.8881 - acc: 0.6604 - val_loss: 0.9433 - val_acc: 0.6439\n",
      "Epoch 13/80\n",
      "29639/29639 [==============================] - 3s 95us/step - loss: 0.8676 - acc: 0.6708 - val_loss: 0.9749 - val_acc: 0.6409\n",
      "Epoch 14/80\n",
      "29639/29639 [==============================] - 4s 120us/step - loss: 0.8572 - acc: 0.6739 - val_loss: 1.0001 - val_acc: 0.6197\n",
      "Epoch 15/80\n",
      "29639/29639 [==============================] - 3s 91us/step - loss: 0.8520 - acc: 0.6818 - val_loss: 1.0024 - val_acc: 0.6340\n",
      "Epoch 16/80\n",
      "29639/29639 [==============================] - 3s 88us/step - loss: 0.8410 - acc: 0.6866 - val_loss: 1.0370 - val_acc: 0.6175\n",
      "Epoch 17/80\n",
      "29639/29639 [==============================] - 3s 85us/step - loss: 0.8347 - acc: 0.6905 - val_loss: 1.0099 - val_acc: 0.6353\n",
      "Inner step: 36 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 5s 161us/step - loss: 1.4243 - acc: 0.3649 - val_loss: 1.3610 - val_acc: 0.4013\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 3s 92us/step - loss: 1.2920 - acc: 0.4246 - val_loss: 1.3298 - val_acc: 0.4159\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 3s 87us/step - loss: 1.2809 - acc: 0.4253 - val_loss: 1.3478 - val_acc: 0.4057\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 3s 108us/step - loss: 1.2685 - acc: 0.4327 - val_loss: 1.3013 - val_acc: 0.4284\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 4s 121us/step - loss: 1.2635 - acc: 0.4452 - val_loss: 1.3118 - val_acc: 0.4239\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 3s 88us/step - loss: 1.2474 - acc: 0.4606 - val_loss: 1.3076 - val_acc: 0.4412\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 3s 85us/step - loss: 1.2251 - acc: 0.4736 - val_loss: 1.2745 - val_acc: 0.4717\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 2s 84us/step - loss: 1.1952 - acc: 0.5008 - val_loss: 1.2743 - val_acc: 0.4828\n",
      "Epoch 9/80\n",
      "29639/29639 [==============================] - 3s 87us/step - loss: 1.1767 - acc: 0.5195 - val_loss: 1.2275 - val_acc: 0.5236\n",
      "Epoch 10/80\n",
      "29639/29639 [==============================] - 3s 98us/step - loss: 1.1075 - acc: 0.5563 - val_loss: 1.1701 - val_acc: 0.5550\n",
      "Epoch 11/80\n",
      "29639/29639 [==============================] - 3s 98us/step - loss: 1.0692 - acc: 0.5709 - val_loss: 1.1335 - val_acc: 0.5710\n",
      "Epoch 12/80\n",
      "29639/29639 [==============================] - 3s 96us/step - loss: 1.0467 - acc: 0.5814 - val_loss: 1.1345 - val_acc: 0.5602\n",
      "Epoch 13/80\n",
      "29639/29639 [==============================] - 3s 87us/step - loss: 1.0359 - acc: 0.5864 - val_loss: 1.2140 - val_acc: 0.5435\n",
      "Epoch 14/80\n",
      "29639/29639 [==============================] - 2s 82us/step - loss: 1.0137 - acc: 0.6007 - val_loss: 1.0757 - val_acc: 0.5871\n",
      "Epoch 15/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29639/29639 [==============================] - 3s 88us/step - loss: 0.9997 - acc: 0.6006 - val_loss: 1.0762 - val_acc: 0.5818\n",
      "Epoch 16/80\n",
      "29639/29639 [==============================] - 3s 89us/step - loss: 0.9882 - acc: 0.6112 - val_loss: 1.0866 - val_acc: 0.5795\n",
      "Epoch 17/80\n",
      "29639/29639 [==============================] - 2s 83us/step - loss: 0.9725 - acc: 0.6174 - val_loss: 1.0331 - val_acc: 0.6023\n",
      "Epoch 18/80\n",
      "29639/29639 [==============================] - 3s 90us/step - loss: 0.9542 - acc: 0.6304 - val_loss: 1.0579 - val_acc: 0.6015\n",
      "Epoch 19/80\n",
      "29639/29639 [==============================] - 3s 91us/step - loss: 0.9457 - acc: 0.6390 - val_loss: 1.0235 - val_acc: 0.6269\n",
      "Epoch 20/80\n",
      "29639/29639 [==============================] - 3s 85us/step - loss: 0.9205 - acc: 0.6540 - val_loss: 0.9794 - val_acc: 0.6422\n",
      "Epoch 21/80\n",
      "29639/29639 [==============================] - 3s 98us/step - loss: 0.8827 - acc: 0.6783 - val_loss: 0.9545 - val_acc: 0.6597\n",
      "Epoch 22/80\n",
      "29639/29639 [==============================] - 3s 101us/step - loss: 0.8424 - acc: 0.7031 - val_loss: 0.9714 - val_acc: 0.6554\n",
      "Epoch 23/80\n",
      "29639/29639 [==============================] - 3s 99us/step - loss: 0.8140 - acc: 0.7141 - val_loss: 0.9815 - val_acc: 0.6359\n",
      "Epoch 24/80\n",
      "29639/29639 [==============================] - 3s 104us/step - loss: 0.7909 - acc: 0.7265 - val_loss: 0.8943 - val_acc: 0.6968\n",
      "Epoch 25/80\n",
      "29639/29639 [==============================] - 3s 89us/step - loss: 0.7736 - acc: 0.7323 - val_loss: 0.9050 - val_acc: 0.6818\n",
      "Epoch 26/80\n",
      "29639/29639 [==============================] - 2s 84us/step - loss: 0.7607 - acc: 0.7391 - val_loss: 0.8655 - val_acc: 0.7014\n",
      "Epoch 27/80\n",
      "29639/29639 [==============================] - 3s 90us/step - loss: 0.7382 - acc: 0.7481 - val_loss: 0.8868 - val_acc: 0.7044\n",
      "Epoch 28/80\n",
      "29639/29639 [==============================] - 3s 86us/step - loss: 0.7172 - acc: 0.7580 - val_loss: 0.9055 - val_acc: 0.7013\n",
      "Epoch 29/80\n",
      "29639/29639 [==============================] - 3s 86us/step - loss: 0.7037 - acc: 0.7626 - val_loss: 0.8221 - val_acc: 0.7340\n",
      "Epoch 30/80\n",
      "29639/29639 [==============================] - 2s 83us/step - loss: 0.6827 - acc: 0.7710 - val_loss: 0.8561 - val_acc: 0.7236\n",
      "Epoch 31/80\n",
      "29639/29639 [==============================] - 3s 85us/step - loss: 0.6886 - acc: 0.7692 - val_loss: 0.8274 - val_acc: 0.7295\n",
      "Epoch 32/80\n",
      "29639/29639 [==============================] - 3s 87us/step - loss: 0.6674 - acc: 0.7778 - val_loss: 0.8231 - val_acc: 0.7290\n",
      "Epoch 33/80\n",
      "29639/29639 [==============================] - 3s 88us/step - loss: 0.6632 - acc: 0.7779 - val_loss: 0.8536 - val_acc: 0.7086\n",
      "Epoch 34/80\n",
      "29639/29639 [==============================] - 3s 104us/step - loss: 0.6613 - acc: 0.7806 - val_loss: 0.8140 - val_acc: 0.7374\n",
      "Epoch 35/80\n",
      "29639/29639 [==============================] - 3s 104us/step - loss: 0.6501 - acc: 0.7829 - val_loss: 0.8201 - val_acc: 0.7354\n",
      "Epoch 36/80\n",
      "29639/29639 [==============================] - 3s 91us/step - loss: 0.6510 - acc: 0.7796 - val_loss: 0.7898 - val_acc: 0.7402\n",
      "Epoch 37/80\n",
      "29639/29639 [==============================] - 3s 86us/step - loss: 0.6381 - acc: 0.7872 - val_loss: 0.7896 - val_acc: 0.7456\n",
      "Epoch 38/80\n",
      "29639/29639 [==============================] - 3s 95us/step - loss: 0.6402 - acc: 0.7860 - val_loss: 0.8043 - val_acc: 0.7415\n",
      "Epoch 39/80\n",
      "29639/29639 [==============================] - 3s 88us/step - loss: 0.6256 - acc: 0.7910 - val_loss: 0.7929 - val_acc: 0.7396\n",
      "Epoch 40/80\n",
      "29639/29639 [==============================] - 3s 98us/step - loss: 0.6240 - acc: 0.7914 - val_loss: 0.8164 - val_acc: 0.7375\n",
      "Epoch 41/80\n",
      "29639/29639 [==============================] - 3s 89us/step - loss: 0.6224 - acc: 0.7927 - val_loss: 0.7817 - val_acc: 0.7454\n",
      "Epoch 42/80\n",
      "29639/29639 [==============================] - 3s 85us/step - loss: 0.6175 - acc: 0.7951 - val_loss: 0.8412 - val_acc: 0.7250\n",
      "Epoch 43/80\n",
      "29639/29639 [==============================] - 3s 86us/step - loss: 0.6151 - acc: 0.7940 - val_loss: 0.7935 - val_acc: 0.7386\n",
      "Epoch 44/80\n",
      "29639/29639 [==============================] - 3s 98us/step - loss: 0.6128 - acc: 0.7955 - val_loss: 0.7850 - val_acc: 0.7496\n",
      "Epoch 45/80\n",
      "29639/29639 [==============================] - 3s 102us/step - loss: 0.6135 - acc: 0.7948 - val_loss: 0.8328 - val_acc: 0.7336\n",
      "Epoch 46/80\n",
      "29639/29639 [==============================] - 3s 99us/step - loss: 0.6059 - acc: 0.7970 - val_loss: 0.8692 - val_acc: 0.7265\n",
      "Inner step: 37 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 5s 178us/step - loss: 1.3560 - acc: 0.3963 - val_loss: 1.2123 - val_acc: 0.4921\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 3s 101us/step - loss: 1.1665 - acc: 0.5061 - val_loss: 1.1749 - val_acc: 0.5214\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 3s 99us/step - loss: 1.1323 - acc: 0.5256 - val_loss: 1.2235 - val_acc: 0.5112\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 2s 84us/step - loss: 1.1150 - acc: 0.5335 - val_loss: 1.1938 - val_acc: 0.5203\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 2s 83us/step - loss: 1.0392 - acc: 0.5646 - val_loss: 1.0589 - val_acc: 0.5706\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 3s 86us/step - loss: 1.0041 - acc: 0.5804 - val_loss: 1.0539 - val_acc: 0.5843\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 3s 85us/step - loss: 0.9780 - acc: 0.5922 - val_loss: 1.0428 - val_acc: 0.5786\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 3s 90us/step - loss: 0.9654 - acc: 0.6053 - val_loss: 1.0091 - val_acc: 0.6050\n",
      "Epoch 9/80\n",
      "29639/29639 [==============================] - 3s 85us/step - loss: 0.9494 - acc: 0.6209 - val_loss: 1.0070 - val_acc: 0.6058\n",
      "Epoch 10/80\n",
      "29639/29639 [==============================] - 3s 90us/step - loss: 0.9300 - acc: 0.6350 - val_loss: 1.0438 - val_acc: 0.6041\n",
      "Epoch 11/80\n",
      "29639/29639 [==============================] - 3s 87us/step - loss: 0.9148 - acc: 0.6452 - val_loss: 0.9923 - val_acc: 0.6187\n",
      "Epoch 12/80\n",
      "29639/29639 [==============================] - 3s 91us/step - loss: 0.8952 - acc: 0.6562 - val_loss: 1.0561 - val_acc: 0.6009\n",
      "Epoch 13/80\n",
      "29639/29639 [==============================] - 3s 87us/step - loss: 0.8854 - acc: 0.6637 - val_loss: 1.0315 - val_acc: 0.6137\n",
      "Epoch 14/80\n",
      "29639/29639 [==============================] - 2s 83us/step - loss: 0.8616 - acc: 0.6816 - val_loss: 0.9792 - val_acc: 0.6436\n",
      "Epoch 15/80\n",
      "29639/29639 [==============================] - 3s 94us/step - loss: 0.8292 - acc: 0.6978 - val_loss: 0.9577 - val_acc: 0.6605\n",
      "Epoch 16/80\n",
      "29639/29639 [==============================] - 3s 87us/step - loss: 0.8083 - acc: 0.7098 - val_loss: 0.9085 - val_acc: 0.6800\n",
      "Epoch 17/80\n",
      "29639/29639 [==============================] - 3s 85us/step - loss: 0.7977 - acc: 0.7143 - val_loss: 0.9176 - val_acc: 0.6775\n",
      "Epoch 18/80\n",
      "29639/29639 [==============================] - 3s 85us/step - loss: 0.7846 - acc: 0.7205 - val_loss: 0.9324 - val_acc: 0.6602\n",
      "Epoch 19/80\n",
      "29639/29639 [==============================] - 2s 84us/step - loss: 0.7787 - acc: 0.7224 - val_loss: 0.9422 - val_acc: 0.6733\n",
      "Epoch 20/80\n",
      "29639/29639 [==============================] - 2s 83us/step - loss: 0.7693 - acc: 0.7285 - val_loss: 0.8917 - val_acc: 0.6852\n",
      "Epoch 21/80\n",
      "29639/29639 [==============================] - 3s 90us/step - loss: 0.7602 - acc: 0.7343 - val_loss: 0.8848 - val_acc: 0.6924\n",
      "Epoch 22/80\n",
      "29639/29639 [==============================] - 3s 91us/step - loss: 0.7487 - acc: 0.7398 - val_loss: 0.8919 - val_acc: 0.6965\n",
      "Epoch 23/80\n",
      "29639/29639 [==============================] - 3s 87us/step - loss: 0.7402 - acc: 0.7426 - val_loss: 0.8493 - val_acc: 0.7121\n",
      "Epoch 24/80\n",
      "29639/29639 [==============================] - 3s 92us/step - loss: 0.7260 - acc: 0.7518 - val_loss: 0.8505 - val_acc: 0.7117\n",
      "Epoch 25/80\n",
      "29639/29639 [==============================] - 3s 101us/step - loss: 0.7188 - acc: 0.7544 - val_loss: 0.9533 - val_acc: 0.6819\n",
      "Epoch 26/80\n",
      "29639/29639 [==============================] - 3s 95us/step - loss: 0.6986 - acc: 0.7635 - val_loss: 0.8998 - val_acc: 0.6895\n",
      "Epoch 27/80\n",
      "29639/29639 [==============================] - 3s 89us/step - loss: 0.7009 - acc: 0.7600 - val_loss: 0.8484 - val_acc: 0.7123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/80\n",
      "29639/29639 [==============================] - 3s 85us/step - loss: 0.6894 - acc: 0.7657 - val_loss: 0.8417 - val_acc: 0.7032\n",
      "Epoch 29/80\n",
      "29639/29639 [==============================] - 3s 89us/step - loss: 0.6777 - acc: 0.7694 - val_loss: 0.8138 - val_acc: 0.7268\n",
      "Epoch 30/80\n",
      "29639/29639 [==============================] - 3s 85us/step - loss: 0.6701 - acc: 0.7725 - val_loss: 0.8050 - val_acc: 0.7298\n",
      "Epoch 31/80\n",
      "29639/29639 [==============================] - 2s 83us/step - loss: 0.6630 - acc: 0.7774 - val_loss: 0.8223 - val_acc: 0.7223\n",
      "Epoch 32/80\n",
      "29639/29639 [==============================] - 2s 83us/step - loss: 0.6575 - acc: 0.7773 - val_loss: 0.8369 - val_acc: 0.7147\n",
      "Epoch 33/80\n",
      "29639/29639 [==============================] - 2s 83us/step - loss: 0.6566 - acc: 0.7761 - val_loss: 0.8317 - val_acc: 0.7244\n",
      "Epoch 34/80\n",
      "29639/29639 [==============================] - 3s 85us/step - loss: 0.6453 - acc: 0.7805 - val_loss: 0.7837 - val_acc: 0.7382\n",
      "Epoch 35/80\n",
      "29639/29639 [==============================] - 3s 85us/step - loss: 0.6393 - acc: 0.7821 - val_loss: 0.8199 - val_acc: 0.7302\n",
      "Epoch 36/80\n",
      "29639/29639 [==============================] - 2s 84us/step - loss: 0.6408 - acc: 0.7800 - val_loss: 0.7800 - val_acc: 0.7384\n",
      "Epoch 37/80\n",
      "29639/29639 [==============================] - 3s 85us/step - loss: 0.6343 - acc: 0.7834 - val_loss: 0.8338 - val_acc: 0.7220\n",
      "Epoch 38/80\n",
      "29639/29639 [==============================] - 3s 87us/step - loss: 0.6301 - acc: 0.7852 - val_loss: 0.8547 - val_acc: 0.7045\n",
      "Epoch 39/80\n",
      "29639/29639 [==============================] - 3s 95us/step - loss: 0.6266 - acc: 0.7867 - val_loss: 0.8444 - val_acc: 0.7225\n",
      "Epoch 40/80\n",
      "29639/29639 [==============================] - 3s 94us/step - loss: 0.6255 - acc: 0.7852 - val_loss: 0.8412 - val_acc: 0.7234\n",
      "Epoch 41/80\n",
      "29639/29639 [==============================] - 3s 91us/step - loss: 0.6142 - acc: 0.7904 - val_loss: 0.8209 - val_acc: 0.7112\n",
      "Inner step: 38 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 5s 164us/step - loss: 1.5349 - acc: 0.2891 - val_loss: 1.5070 - val_acc: 0.2928\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 3s 93us/step - loss: 1.5077 - acc: 0.3013 - val_loss: 1.5118 - val_acc: 0.3123\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 3s 86us/step - loss: 1.5036 - acc: 0.3037 - val_loss: 1.5037 - val_acc: 0.3090\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 3s 85us/step - loss: 1.3928 - acc: 0.3658 - val_loss: 1.2874 - val_acc: 0.4371\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 3s 98us/step - loss: 1.2708 - acc: 0.4575 - val_loss: 1.2668 - val_acc: 0.4669\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 3s 86us/step - loss: 1.2250 - acc: 0.4981 - val_loss: 1.2449 - val_acc: 0.4982\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 3s 85us/step - loss: 1.1974 - acc: 0.5152 - val_loss: 1.2218 - val_acc: 0.5157\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 3s 85us/step - loss: 1.1532 - acc: 0.5334 - val_loss: 1.2283 - val_acc: 0.5103\n",
      "Epoch 9/80\n",
      "29639/29639 [==============================] - 3s 88us/step - loss: 1.1297 - acc: 0.5399 - val_loss: 1.2151 - val_acc: 0.5096\n",
      "Epoch 10/80\n",
      "29639/29639 [==============================] - 3s 90us/step - loss: 1.1102 - acc: 0.5441 - val_loss: 1.1481 - val_acc: 0.5323\n",
      "Epoch 11/80\n",
      "29639/29639 [==============================] - 3s 93us/step - loss: 1.0936 - acc: 0.5469 - val_loss: 1.1766 - val_acc: 0.5177\n",
      "Epoch 12/80\n",
      "29639/29639 [==============================] - 3s 101us/step - loss: 1.0900 - acc: 0.5519 - val_loss: 1.2065 - val_acc: 0.5306\n",
      "Epoch 13/80\n",
      "29639/29639 [==============================] - 3s 112us/step - loss: 1.0764 - acc: 0.5568 - val_loss: 1.1790 - val_acc: 0.5399\n",
      "Epoch 14/80\n",
      "29639/29639 [==============================] - 3s 111us/step - loss: 1.0685 - acc: 0.5636 - val_loss: 1.1478 - val_acc: 0.5469\n",
      "Epoch 15/80\n",
      "29639/29639 [==============================] - 3s 105us/step - loss: 1.0349 - acc: 0.5819 - val_loss: 1.0759 - val_acc: 0.5674\n",
      "Epoch 16/80\n",
      "29639/29639 [==============================] - 3s 94us/step - loss: 0.9794 - acc: 0.6033 - val_loss: 1.0369 - val_acc: 0.5979\n",
      "Epoch 17/80\n",
      "29639/29639 [==============================] - 3s 103us/step - loss: 0.9455 - acc: 0.6217 - val_loss: 1.0197 - val_acc: 0.6005\n",
      "Epoch 18/80\n",
      "29639/29639 [==============================] - 4s 129us/step - loss: 0.9165 - acc: 0.6346 - val_loss: 0.9959 - val_acc: 0.6238\n",
      "Epoch 19/80\n",
      "29639/29639 [==============================] - 4s 145us/step - loss: 0.8952 - acc: 0.6507 - val_loss: 1.0008 - val_acc: 0.6330\n",
      "Epoch 20/80\n",
      "29639/29639 [==============================] - 3s 106us/step - loss: 0.8639 - acc: 0.6746 - val_loss: 0.9647 - val_acc: 0.6580\n",
      "Epoch 21/80\n",
      "29639/29639 [==============================] - 3s 98us/step - loss: 0.8320 - acc: 0.6959 - val_loss: 0.9229 - val_acc: 0.6729\n",
      "Epoch 22/80\n",
      "29639/29639 [==============================] - 3s 97us/step - loss: 0.8039 - acc: 0.7069 - val_loss: 0.8904 - val_acc: 0.6733\n",
      "Epoch 23/80\n",
      "29639/29639 [==============================] - 3s 86us/step - loss: 0.7849 - acc: 0.7187 - val_loss: 0.9220 - val_acc: 0.6697\n",
      "Epoch 24/80\n",
      "29639/29639 [==============================] - 3s 86us/step - loss: 0.7717 - acc: 0.7212 - val_loss: 0.8724 - val_acc: 0.6897\n",
      "Epoch 25/80\n",
      "29639/29639 [==============================] - 3s 85us/step - loss: 0.7575 - acc: 0.7294 - val_loss: 0.8692 - val_acc: 0.6981\n",
      "Epoch 26/80\n",
      "29639/29639 [==============================] - 3s 85us/step - loss: 0.7512 - acc: 0.7320 - val_loss: 0.8999 - val_acc: 0.6913\n",
      "Epoch 27/80\n",
      "29639/29639 [==============================] - 3s 87us/step - loss: 0.7376 - acc: 0.7402 - val_loss: 0.8468 - val_acc: 0.7033\n",
      "Epoch 28/80\n",
      "29639/29639 [==============================] - 3s 87us/step - loss: 0.7236 - acc: 0.7490 - val_loss: 0.8549 - val_acc: 0.7005\n",
      "Epoch 29/80\n",
      "29639/29639 [==============================] - 3s 88us/step - loss: 0.7145 - acc: 0.7530 - val_loss: 0.8517 - val_acc: 0.7076\n",
      "Epoch 30/80\n",
      "29639/29639 [==============================] - 3s 86us/step - loss: 0.7090 - acc: 0.7562 - val_loss: 0.8405 - val_acc: 0.7203\n",
      "Epoch 31/80\n",
      "29639/29639 [==============================] - 3s 86us/step - loss: 0.6975 - acc: 0.7627 - val_loss: 0.8250 - val_acc: 0.7225\n",
      "Epoch 32/80\n",
      "29639/29639 [==============================] - 3s 88us/step - loss: 0.6917 - acc: 0.7648 - val_loss: 0.8065 - val_acc: 0.7279\n",
      "Epoch 33/80\n",
      "29639/29639 [==============================] - 3s 85us/step - loss: 0.6871 - acc: 0.7670 - val_loss: 0.7928 - val_acc: 0.7300\n",
      "Epoch 34/80\n",
      "29639/29639 [==============================] - 3s 88us/step - loss: 0.6781 - acc: 0.7725 - val_loss: 0.8046 - val_acc: 0.7265\n",
      "Epoch 35/80\n",
      "29639/29639 [==============================] - 3s 86us/step - loss: 0.6709 - acc: 0.7712 - val_loss: 0.8100 - val_acc: 0.7256\n",
      "Epoch 36/80\n",
      "29639/29639 [==============================] - 3s 84us/step - loss: 0.6692 - acc: 0.7724 - val_loss: 0.8363 - val_acc: 0.7130\n",
      "Epoch 37/80\n",
      "29639/29639 [==============================] - 3s 98us/step - loss: 0.6615 - acc: 0.7778 - val_loss: 0.8088 - val_acc: 0.7328\n",
      "Epoch 38/80\n",
      "29639/29639 [==============================] - 3s 101us/step - loss: 0.6588 - acc: 0.7780 - val_loss: 0.7829 - val_acc: 0.7395\n",
      "Epoch 39/80\n",
      "29639/29639 [==============================] - 3s 87us/step - loss: 0.6452 - acc: 0.7823 - val_loss: 0.8453 - val_acc: 0.7199\n",
      "Epoch 40/80\n",
      "29639/29639 [==============================] - 3s 87us/step - loss: 0.6457 - acc: 0.7831 - val_loss: 0.8046 - val_acc: 0.7282\n",
      "Epoch 41/80\n",
      "29639/29639 [==============================] - 3s 87us/step - loss: 0.6402 - acc: 0.7850 - val_loss: 0.7944 - val_acc: 0.7407\n",
      "Epoch 42/80\n",
      "29639/29639 [==============================] - 3s 87us/step - loss: 0.6359 - acc: 0.7883 - val_loss: 0.7782 - val_acc: 0.7425\n",
      "Epoch 43/80\n",
      "29639/29639 [==============================] - 3s 88us/step - loss: 0.6329 - acc: 0.7876 - val_loss: 0.7991 - val_acc: 0.7377\n",
      "Epoch 44/80\n",
      "29639/29639 [==============================] - 3s 87us/step - loss: 0.6212 - acc: 0.7927 - val_loss: 0.9167 - val_acc: 0.6902\n",
      "Epoch 45/80\n",
      "29639/29639 [==============================] - 3s 88us/step - loss: 0.6217 - acc: 0.7907 - val_loss: 0.7861 - val_acc: 0.7354\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/80\n",
      "29639/29639 [==============================] - 3s 90us/step - loss: 0.6193 - acc: 0.7920 - val_loss: 0.8068 - val_acc: 0.7336\n",
      "Epoch 47/80\n",
      "29639/29639 [==============================] - 3s 89us/step - loss: 0.6191 - acc: 0.7889 - val_loss: 0.7794 - val_acc: 0.7420\n",
      "Inner step: 39 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 5s 168us/step - loss: 1.5405 - acc: 0.2848 - val_loss: 1.5081 - val_acc: 0.3073\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 3s 87us/step - loss: 1.5058 - acc: 0.3053 - val_loss: 1.5086 - val_acc: 0.3072\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 3s 89us/step - loss: 1.5055 - acc: 0.3027 - val_loss: 1.5125 - val_acc: 0.3134\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 3s 92us/step - loss: 1.4643 - acc: 0.3379 - val_loss: 1.3467 - val_acc: 0.4291\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 3s 89us/step - loss: 1.1895 - acc: 0.5131 - val_loss: 1.1510 - val_acc: 0.5387\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 3s 86us/step - loss: 1.1183 - acc: 0.5448 - val_loss: 1.1324 - val_acc: 0.5402\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 3s 85us/step - loss: 1.0870 - acc: 0.5614 - val_loss: 1.1761 - val_acc: 0.5388\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 3s 85us/step - loss: 1.0482 - acc: 0.5767 - val_loss: 1.0745 - val_acc: 0.5767\n",
      "Epoch 9/80\n",
      "29639/29639 [==============================] - 3s 91us/step - loss: 0.9947 - acc: 0.6025 - val_loss: 1.0609 - val_acc: 0.5891\n",
      "Epoch 10/80\n",
      "29639/29639 [==============================] - 3s 87us/step - loss: 0.9621 - acc: 0.6213 - val_loss: 1.0095 - val_acc: 0.6076\n",
      "Epoch 11/80\n",
      "29639/29639 [==============================] - 3s 86us/step - loss: 0.9325 - acc: 0.6411 - val_loss: 1.0332 - val_acc: 0.6241\n",
      "Epoch 12/80\n",
      "29639/29639 [==============================] - 3s 86us/step - loss: 0.9108 - acc: 0.6585 - val_loss: 0.9855 - val_acc: 0.6408\n",
      "Epoch 13/80\n",
      "29639/29639 [==============================] - 3s 86us/step - loss: 0.8893 - acc: 0.6734 - val_loss: 1.0628 - val_acc: 0.6054\n",
      "Epoch 14/80\n",
      "29639/29639 [==============================] - 3s 86us/step - loss: 0.8434 - acc: 0.6923 - val_loss: 0.9090 - val_acc: 0.6744\n",
      "Epoch 15/80\n",
      "29639/29639 [==============================] - 3s 88us/step - loss: 0.8201 - acc: 0.7020 - val_loss: 0.9353 - val_acc: 0.6713\n",
      "Epoch 16/80\n",
      "29639/29639 [==============================] - 3s 87us/step - loss: 0.7957 - acc: 0.7120 - val_loss: 0.8904 - val_acc: 0.6905\n",
      "Epoch 17/80\n",
      "29639/29639 [==============================] - 3s 86us/step - loss: 0.7756 - acc: 0.7278 - val_loss: 0.8688 - val_acc: 0.7005\n",
      "Epoch 18/80\n",
      "29639/29639 [==============================] - 3s 87us/step - loss: 0.7568 - acc: 0.7379 - val_loss: 0.8918 - val_acc: 0.6967\n",
      "Epoch 19/80\n",
      "29639/29639 [==============================] - 3s 89us/step - loss: 0.7432 - acc: 0.7486 - val_loss: 0.8416 - val_acc: 0.7183\n",
      "Epoch 20/80\n",
      "29639/29639 [==============================] - 3s 97us/step - loss: 0.7260 - acc: 0.7535 - val_loss: 0.8369 - val_acc: 0.7212\n",
      "Epoch 21/80\n",
      "29639/29639 [==============================] - 3s 88us/step - loss: 0.7155 - acc: 0.7577 - val_loss: 0.8662 - val_acc: 0.7126\n",
      "Epoch 22/80\n",
      "29639/29639 [==============================] - 3s 88us/step - loss: 0.6958 - acc: 0.7655 - val_loss: 0.7846 - val_acc: 0.7348\n",
      "Epoch 23/80\n",
      "29639/29639 [==============================] - 3s 88us/step - loss: 0.6886 - acc: 0.7674 - val_loss: 0.8400 - val_acc: 0.7191\n",
      "Epoch 24/80\n",
      "29639/29639 [==============================] - 3s 88us/step - loss: 0.6741 - acc: 0.7743 - val_loss: 0.8662 - val_acc: 0.7152\n",
      "Epoch 25/80\n",
      "29639/29639 [==============================] - 3s 88us/step - loss: 0.6705 - acc: 0.7730 - val_loss: 0.8056 - val_acc: 0.7252\n",
      "Epoch 26/80\n",
      "29639/29639 [==============================] - 2s 83us/step - loss: 0.6589 - acc: 0.7795 - val_loss: 0.8205 - val_acc: 0.7300\n",
      "Epoch 27/80\n",
      "29639/29639 [==============================] - 3s 86us/step - loss: 0.6556 - acc: 0.7808 - val_loss: 0.8344 - val_acc: 0.7189\n",
      "Inner step: 40 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 5s 159us/step - loss: 1.5537 - acc: 0.2762 - val_loss: 1.5061 - val_acc: 0.2726\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 3s 89us/step - loss: 1.3905 - acc: 0.3470 - val_loss: 1.3958 - val_acc: 0.3760\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 3s 87us/step - loss: 1.3332 - acc: 0.3844 - val_loss: 1.3364 - val_acc: 0.4175\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 3s 86us/step - loss: 1.2176 - acc: 0.4639 - val_loss: 1.2217 - val_acc: 0.4692\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 3s 85us/step - loss: 1.1759 - acc: 0.4800 - val_loss: 1.2061 - val_acc: 0.4743\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 3s 85us/step - loss: 1.1560 - acc: 0.4931 - val_loss: 1.1997 - val_acc: 0.4841\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 3s 96us/step - loss: 1.1223 - acc: 0.5266 - val_loss: 1.2621 - val_acc: 0.4939\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 3s 88us/step - loss: 1.0771 - acc: 0.5528 - val_loss: 1.1492 - val_acc: 0.5406\n",
      "Epoch 9/80\n",
      "29639/29639 [==============================] - 3s 92us/step - loss: 1.0462 - acc: 0.5653 - val_loss: 1.1143 - val_acc: 0.5455\n",
      "Epoch 10/80\n",
      "29639/29639 [==============================] - 3s 99us/step - loss: 1.0273 - acc: 0.5706 - val_loss: 1.1096 - val_acc: 0.5578\n",
      "Epoch 11/80\n",
      "29639/29639 [==============================] - 3s 91us/step - loss: 1.0034 - acc: 0.5845 - val_loss: 1.0815 - val_acc: 0.5758\n",
      "Epoch 12/80\n",
      "29639/29639 [==============================] - 3s 91us/step - loss: 0.9735 - acc: 0.6111 - val_loss: 1.0409 - val_acc: 0.6099\n",
      "Epoch 13/80\n",
      "29639/29639 [==============================] - 3s 91us/step - loss: 0.9375 - acc: 0.6363 - val_loss: 1.0540 - val_acc: 0.6169\n",
      "Epoch 14/80\n",
      "29639/29639 [==============================] - 2s 84us/step - loss: 0.9010 - acc: 0.6600 - val_loss: 0.9835 - val_acc: 0.6329\n",
      "Epoch 15/80\n",
      "29639/29639 [==============================] - 3s 85us/step - loss: 0.8555 - acc: 0.6874 - val_loss: 0.9715 - val_acc: 0.6516\n",
      "Epoch 16/80\n",
      "29639/29639 [==============================] - 3s 100us/step - loss: 0.8154 - acc: 0.7092 - val_loss: 0.9942 - val_acc: 0.6637\n",
      "Epoch 17/80\n",
      "29639/29639 [==============================] - 3s 91us/step - loss: 0.7882 - acc: 0.7242 - val_loss: 0.9477 - val_acc: 0.6586\n",
      "Epoch 18/80\n",
      "29639/29639 [==============================] - 3s 85us/step - loss: 0.7693 - acc: 0.7321 - val_loss: 0.8929 - val_acc: 0.6864\n",
      "Epoch 19/80\n",
      "29639/29639 [==============================] - 3s 93us/step - loss: 0.7427 - acc: 0.7422 - val_loss: 0.8810 - val_acc: 0.6976\n",
      "Epoch 20/80\n",
      "29639/29639 [==============================] - 3s 90us/step - loss: 0.7398 - acc: 0.7465 - val_loss: 0.8671 - val_acc: 0.6998\n",
      "Epoch 21/80\n",
      "29639/29639 [==============================] - 3s 93us/step - loss: 0.7312 - acc: 0.7482 - val_loss: 1.0725 - val_acc: 0.6476\n",
      "Epoch 22/80\n",
      "29639/29639 [==============================] - 3s 90us/step - loss: 0.7260 - acc: 0.7509 - val_loss: 0.8508 - val_acc: 0.7124\n",
      "Epoch 23/80\n",
      "29639/29639 [==============================] - 3s 88us/step - loss: 0.7193 - acc: 0.7549 - val_loss: 0.8670 - val_acc: 0.6982\n",
      "Epoch 24/80\n",
      "29639/29639 [==============================] - 3s 85us/step - loss: 0.7068 - acc: 0.7578 - val_loss: 0.8689 - val_acc: 0.6997\n",
      "Epoch 25/80\n",
      "29639/29639 [==============================] - 3s 89us/step - loss: 0.7034 - acc: 0.7645 - val_loss: 0.8578 - val_acc: 0.7039\n",
      "Epoch 26/80\n",
      "29639/29639 [==============================] - 3s 99us/step - loss: 0.7102 - acc: 0.7562 - val_loss: 0.8805 - val_acc: 0.6993\n",
      "Epoch 27/80\n",
      "29639/29639 [==============================] - 3s 89us/step - loss: 0.6990 - acc: 0.7618 - val_loss: 0.8565 - val_acc: 0.7068\n",
      "Training model 3 / 4\n",
      "Inner step: 1 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 5s 170us/step - loss: 1.5952 - acc: 0.2451 - val_loss: 1.5941 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 3s 92us/step - loss: 1.5949 - acc: 0.2466 - val_loss: 1.5939 - val_acc: 0.2478\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 3s 113us/step - loss: 1.5950 - acc: 0.2461 - val_loss: 1.5942 - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 3s 97us/step - loss: 1.5953 - acc: 0.2487 - val_loss: 1.5976 - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 3s 87us/step - loss: 1.5952 - acc: 0.2495 - val_loss: 1.5959 - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 3s 87us/step - loss: 1.5948 - acc: 0.2477 - val_loss: 1.5948 - val_acc: 0.2342\n",
      "Inner step: 2 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 6s 190us/step - loss: 1.5949 - acc: 0.2462 - val_loss: 1.5948 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 3s 91us/step - loss: 1.5952 - acc: 0.2471 - val_loss: 1.5933 - val_acc: 0.2342\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 3s 90us/step - loss: 1.5950 - acc: 0.2504 - val_loss: 1.5958 - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 3s 89us/step - loss: 1.5949 - acc: 0.2469 - val_loss: 1.5936 - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 3s 87us/step - loss: 1.5953 - acc: 0.2490 - val_loss: 1.5947 - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 3s 87us/step - loss: 1.5952 - acc: 0.2467 - val_loss: 1.5960 - val_acc: 0.2478\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 3s 87us/step - loss: 1.5952 - acc: 0.2470 - val_loss: 1.5953 - val_acc: 0.2478\n",
      "Inner step: 3 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 5s 170us/step - loss: 1.5949 - acc: 0.2465 - val_loss: 1.5928 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 3s 92us/step - loss: 1.5947 - acc: 0.2492 - val_loss: 1.5965 - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 3s 105us/step - loss: 1.5948 - acc: 0.2478 - val_loss: 1.6002 - val_acc: 0.2342\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 3s 100us/step - loss: 1.5952 - acc: 0.2461 - val_loss: 1.5945 - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 3s 91us/step - loss: 1.5950 - acc: 0.2447 - val_loss: 1.5953 - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 3s 93us/step - loss: 1.5952 - acc: 0.2469 - val_loss: 1.5939 - val_acc: 0.2478\n",
      "Inner step: 4 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 5s 179us/step - loss: 1.5952 - acc: 0.2459 - val_loss: 1.5956 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 3s 92us/step - loss: 1.5950 - acc: 0.2478 - val_loss: 1.5935 - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 3s 94us/step - loss: 1.5950 - acc: 0.2480 - val_loss: 1.5930 - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 3s 89us/step - loss: 1.5949 - acc: 0.2476 - val_loss: 1.5933 - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 3s 89us/step - loss: 1.5948 - acc: 0.2490 - val_loss: 1.5969 - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 3s 91us/step - loss: 1.5951 - acc: 0.2489 - val_loss: 1.5940 - val_acc: 0.2478\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 3s 90us/step - loss: 1.5950 - acc: 0.2469 - val_loss: 1.5946 - val_acc: 0.2478\n",
      "Inner step: 5 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 5s 173us/step - loss: 1.5954 - acc: 0.2471 - val_loss: 1.5960 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 3s 100us/step - loss: 1.5949 - acc: 0.2490 - val_loss: 1.5973 - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 3s 93us/step - loss: 1.5949 - acc: 0.2491 - val_loss: 1.5945 - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 3s 100us/step - loss: 1.5952 - acc: 0.2478 - val_loss: 1.5969 - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 3s 94us/step - loss: 1.5950 - acc: 0.2482 - val_loss: 1.5945 - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 3s 101us/step - loss: 1.5953 - acc: 0.2467 - val_loss: 1.5947 - val_acc: 0.2478\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 3s 101us/step - loss: 1.5951 - acc: 0.2472 - val_loss: 1.5978 - val_acc: 0.2478\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 3s 93us/step - loss: 1.5951 - acc: 0.2490 - val_loss: 1.5936 - val_acc: 0.2478\n",
      "Inner step: 6 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 5s 177us/step - loss: 1.5952 - acc: 0.2483 - val_loss: 1.5942 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 3s 90us/step - loss: 1.5948 - acc: 0.2481 - val_loss: 1.5982 - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 3s 90us/step - loss: 1.5951 - acc: 0.2456 - val_loss: 1.5932 - val_acc: 0.2342\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 3s 91us/step - loss: 1.5953 - acc: 0.2478 - val_loss: 1.5969 - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 3s 91us/step - loss: 1.5952 - acc: 0.2449 - val_loss: 1.5954 - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 3s 91us/step - loss: 1.5945 - acc: 0.2475 - val_loss: 1.5969 - val_acc: 0.2478\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 3s 95us/step - loss: 1.5949 - acc: 0.2484 - val_loss: 1.5931 - val_acc: 0.2478\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 3s 92us/step - loss: 1.5952 - acc: 0.2469 - val_loss: 1.5948 - val_acc: 0.2478\n",
      "Inner step: 7 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 5s 183us/step - loss: 1.5952 - acc: 0.2466 - val_loss: 1.5960 - val_acc: 0.2342\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 3s 98us/step - loss: 1.5951 - acc: 0.2480 - val_loss: 1.5947 - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 3s 105us/step - loss: 1.5948 - acc: 0.2492 - val_loss: 1.5962 - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 3s 89us/step - loss: 1.5950 - acc: 0.2477 - val_loss: 1.5946 - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 3s 93us/step - loss: 1.5949 - acc: 0.2501 - val_loss: 1.5939 - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 3s 93us/step - loss: 1.5951 - acc: 0.2478 - val_loss: 1.5934 - val_acc: 0.2478\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 3s 93us/step - loss: 1.5949 - acc: 0.2492 - val_loss: 1.5940 - val_acc: 0.2478\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 3s 96us/step - loss: 1.5954 - acc: 0.2485 - val_loss: 1.5966 - val_acc: 0.2478\n",
      "Epoch 9/80\n",
      "29639/29639 [==============================] - 3s 94us/step - loss: 1.5954 - acc: 0.2512 - val_loss: 1.5935 - val_acc: 0.2342\n",
      "Epoch 10/80\n",
      "29639/29639 [==============================] - 3s 101us/step - loss: 1.5952 - acc: 0.2471 - val_loss: 1.5955 - val_acc: 0.2342\n",
      "Epoch 11/80\n",
      "29639/29639 [==============================] - 3s 90us/step - loss: 1.5949 - acc: 0.2477 - val_loss: 1.5971 - val_acc: 0.2342\n",
      "Inner step: 8 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 5s 174us/step - loss: 1.5949 - acc: 0.2493 - val_loss: 1.5948 - val_acc: 0.2342\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 3s 87us/step - loss: 1.5950 - acc: 0.2501 - val_loss: 1.5940 - val_acc: 0.2342\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 3s 88us/step - loss: 1.5951 - acc: 0.2520 - val_loss: 1.5935 - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 3s 91us/step - loss: 1.5947 - acc: 0.2494 - val_loss: 1.5943 - val_acc: 0.2342\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 3s 97us/step - loss: 1.5950 - acc: 0.2476 - val_loss: 1.5944 - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 3s 88us/step - loss: 1.5949 - acc: 0.2484 - val_loss: 1.5943 - val_acc: 0.2342\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 3s 94us/step - loss: 1.5951 - acc: 0.2472 - val_loss: 1.5950 - val_acc: 0.2478\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 3s 90us/step - loss: 1.5950 - acc: 0.2461 - val_loss: 1.5937 - val_acc: 0.2478\n",
      "Inner step: 9 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 6s 190us/step - loss: 1.5949 - acc: 0.2501 - val_loss: 1.5946 - val_acc: 0.2342\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 3s 90us/step - loss: 1.5951 - acc: 0.2470 - val_loss: 1.5932 - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 3s 89us/step - loss: 1.5950 - acc: 0.2484 - val_loss: 1.5950 - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 3s 89us/step - loss: 1.5948 - acc: 0.2484 - val_loss: 1.5934 - val_acc: 0.2342\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 3s 90us/step - loss: 1.5952 - acc: 0.2469 - val_loss: 1.5936 - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 3s 88us/step - loss: 1.5949 - acc: 0.2473 - val_loss: 1.5980 - val_acc: 0.2478\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 3s 87us/step - loss: 1.5950 - acc: 0.2472 - val_loss: 1.5941 - val_acc: 0.2478\n",
      "Inner step: 10 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 5s 176us/step - loss: 1.5951 - acc: 0.2471 - val_loss: 1.5950 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 3s 89us/step - loss: 1.5950 - acc: 0.2471 - val_loss: 1.5933 - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 3s 90us/step - loss: 1.5951 - acc: 0.2468 - val_loss: 1.5937 - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 3s 91us/step - loss: 1.5949 - acc: 0.2505 - val_loss: 1.5965 - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 3s 88us/step - loss: 1.5953 - acc: 0.2486 - val_loss: 1.5930 - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 3s 101us/step - loss: 1.5950 - acc: 0.2497 - val_loss: 1.5945 - val_acc: 0.2478\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 3s 95us/step - loss: 1.5951 - acc: 0.2489 - val_loss: 1.5931 - val_acc: 0.2478\n",
      "Inner step: 11 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 6s 188us/step - loss: 1.5954 - acc: 0.2481 - val_loss: 1.5949 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 3s 104us/step - loss: 1.5955 - acc: 0.2433 - val_loss: 1.5930 - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 4s 131us/step - loss: 1.5954 - acc: 0.2477 - val_loss: 1.5933 - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 4s 123us/step - loss: 1.5948 - acc: 0.2460 - val_loss: 1.5954 - val_acc: 0.2342\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 3s 97us/step - loss: 1.5745 - acc: 0.2656 - val_loss: 1.5228 - val_acc: 0.3096\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 3s 96us/step - loss: 1.5122 - acc: 0.3013 - val_loss: 1.5062 - val_acc: 0.3103\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 3s 93us/step - loss: 1.4988 - acc: 0.3082 - val_loss: 1.4355 - val_acc: 0.3457\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 3s 97us/step - loss: 1.2684 - acc: 0.4578 - val_loss: 1.1383 - val_acc: 0.5486\n",
      "Epoch 9/80\n",
      "29639/29639 [==============================] - 3s 96us/step - loss: 1.0482 - acc: 0.5642 - val_loss: 1.0699 - val_acc: 0.5663\n",
      "Epoch 10/80\n",
      "29639/29639 [==============================] - 3s 94us/step - loss: 1.0114 - acc: 0.5751 - val_loss: 1.0642 - val_acc: 0.5755\n",
      "Epoch 11/80\n",
      "29639/29639 [==============================] - 3s 92us/step - loss: 1.0013 - acc: 0.5808 - val_loss: 1.0498 - val_acc: 0.5742\n",
      "Epoch 12/80\n",
      "29639/29639 [==============================] - 3s 92us/step - loss: 0.9872 - acc: 0.5876 - val_loss: 1.0752 - val_acc: 0.5615\n",
      "Epoch 13/80\n",
      "29639/29639 [==============================] - 3s 93us/step - loss: 0.9686 - acc: 0.5918 - val_loss: 1.0498 - val_acc: 0.5874\n",
      "Epoch 14/80\n",
      "29639/29639 [==============================] - 3s 97us/step - loss: 0.9550 - acc: 0.6050 - val_loss: 1.0131 - val_acc: 0.6043\n",
      "Epoch 15/80\n",
      "29639/29639 [==============================] - 3s 99us/step - loss: 0.9451 - acc: 0.6185 - val_loss: 1.0079 - val_acc: 0.6205\n",
      "Epoch 16/80\n",
      "29639/29639 [==============================] - 3s 96us/step - loss: 0.9042 - acc: 0.6450 - val_loss: 0.9496 - val_acc: 0.6555\n",
      "Epoch 17/80\n",
      "29639/29639 [==============================] - 3s 101us/step - loss: 0.8652 - acc: 0.6722 - val_loss: 0.9776 - val_acc: 0.6308\n",
      "Epoch 18/80\n",
      "29639/29639 [==============================] - 3s 99us/step - loss: 0.8347 - acc: 0.6908 - val_loss: 0.9130 - val_acc: 0.6681\n",
      "Epoch 19/80\n",
      "29639/29639 [==============================] - 3s 97us/step - loss: 0.8003 - acc: 0.7126 - val_loss: 0.8725 - val_acc: 0.6922\n",
      "Epoch 20/80\n",
      "29639/29639 [==============================] - 3s 104us/step - loss: 0.7761 - acc: 0.7257 - val_loss: 0.8897 - val_acc: 0.6993\n",
      "Epoch 21/80\n",
      "29639/29639 [==============================] - 4s 139us/step - loss: 0.7464 - acc: 0.7390 - val_loss: 0.8541 - val_acc: 0.7136\n",
      "Epoch 22/80\n",
      "29639/29639 [==============================] - 3s 118us/step - loss: 0.7257 - acc: 0.7506 - val_loss: 0.8301 - val_acc: 0.7154\n",
      "Epoch 23/80\n",
      "29639/29639 [==============================] - 3s 101us/step - loss: 0.7142 - acc: 0.7571 - val_loss: 0.8100 - val_acc: 0.7306\n",
      "Epoch 24/80\n",
      "29639/29639 [==============================] - 3s 99us/step - loss: 0.6910 - acc: 0.7648 - val_loss: 0.8613 - val_acc: 0.7120\n",
      "Epoch 25/80\n",
      "29639/29639 [==============================] - 3s 100us/step - loss: 0.6843 - acc: 0.7692 - val_loss: 0.8356 - val_acc: 0.7286\n",
      "Epoch 26/80\n",
      "29639/29639 [==============================] - 3s 98us/step - loss: 0.6696 - acc: 0.7749 - val_loss: 0.8075 - val_acc: 0.7214\n",
      "Epoch 27/80\n",
      "29639/29639 [==============================] - 3s 96us/step - loss: 0.6650 - acc: 0.7784 - val_loss: 0.8481 - val_acc: 0.7202\n",
      "Epoch 28/80\n",
      "29639/29639 [==============================] - 3s 97us/step - loss: 0.6483 - acc: 0.7847 - val_loss: 0.8058 - val_acc: 0.7363\n",
      "Epoch 29/80\n",
      "29639/29639 [==============================] - 3s 94us/step - loss: 0.6472 - acc: 0.7826 - val_loss: 0.8104 - val_acc: 0.7422\n",
      "Epoch 30/80\n",
      "29639/29639 [==============================] - 3s 97us/step - loss: 0.6392 - acc: 0.7864 - val_loss: 0.8358 - val_acc: 0.7096\n",
      "Epoch 31/80\n",
      "29639/29639 [==============================] - 3s 99us/step - loss: 0.6371 - acc: 0.7856 - val_loss: 0.8577 - val_acc: 0.7186\n",
      "Epoch 32/80\n",
      "29639/29639 [==============================] - 3s 94us/step - loss: 0.6236 - acc: 0.7920 - val_loss: 0.7762 - val_acc: 0.7418\n",
      "Epoch 33/80\n",
      "29639/29639 [==============================] - 3s 91us/step - loss: 0.6218 - acc: 0.7898 - val_loss: 0.8145 - val_acc: 0.7296\n",
      "Epoch 34/80\n",
      "29639/29639 [==============================] - 3s 93us/step - loss: 0.6177 - acc: 0.7911 - val_loss: 0.7983 - val_acc: 0.7352\n",
      "Epoch 35/80\n",
      "29639/29639 [==============================] - 3s 90us/step - loss: 0.6112 - acc: 0.7953 - val_loss: 0.7680 - val_acc: 0.7468\n",
      "Epoch 36/80\n",
      "29639/29639 [==============================] - 3s 90us/step - loss: 0.6089 - acc: 0.7953 - val_loss: 0.7815 - val_acc: 0.7355\n",
      "Epoch 37/80\n",
      "29639/29639 [==============================] - 3s 95us/step - loss: 0.6026 - acc: 0.7977 - val_loss: 0.7630 - val_acc: 0.7440\n",
      "Epoch 38/80\n",
      "29639/29639 [==============================] - 3s 91us/step - loss: 0.5998 - acc: 0.7978 - val_loss: 0.7781 - val_acc: 0.7435\n",
      "Epoch 39/80\n",
      "29639/29639 [==============================] - 3s 91us/step - loss: 0.5933 - acc: 0.7981 - val_loss: 0.7995 - val_acc: 0.7154\n",
      "Epoch 40/80\n",
      "29639/29639 [==============================] - 3s 90us/step - loss: 0.5938 - acc: 0.7994 - val_loss: 0.7745 - val_acc: 0.7419\n",
      "Epoch 41/80\n",
      "29639/29639 [==============================] - 3s 92us/step - loss: 0.5894 - acc: 0.7998 - val_loss: 0.7891 - val_acc: 0.7361\n",
      "Epoch 42/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29639/29639 [==============================] - 3s 89us/step - loss: 0.5835 - acc: 0.8033 - val_loss: 0.7891 - val_acc: 0.7264\n",
      "Inner step: 12 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 6s 191us/step - loss: 1.5953 - acc: 0.2480 - val_loss: 1.5958 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 3s 98us/step - loss: 1.5952 - acc: 0.2483 - val_loss: 1.5957 - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 3s 92us/step - loss: 1.5948 - acc: 0.2497 - val_loss: 1.5992 - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 3s 100us/step - loss: 1.5950 - acc: 0.2472 - val_loss: 1.5952 - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 3s 94us/step - loss: 1.5952 - acc: 0.2470 - val_loss: 1.5963 - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 3s 94us/step - loss: 1.5950 - acc: 0.2473 - val_loss: 1.5935 - val_acc: 0.2342\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 3s 94us/step - loss: 1.5946 - acc: 0.2470 - val_loss: 1.5990 - val_acc: 0.2478\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 3s 96us/step - loss: 1.5953 - acc: 0.2502 - val_loss: 1.5948 - val_acc: 0.2478\n",
      "Epoch 9/80\n",
      "29639/29639 [==============================] - 3s 92us/step - loss: 1.5947 - acc: 0.2487 - val_loss: 1.5946 - val_acc: 0.2478\n",
      "Epoch 10/80\n",
      "29639/29639 [==============================] - 3s 97us/step - loss: 1.5950 - acc: 0.2474 - val_loss: 1.5935 - val_acc: 0.2478\n",
      "Epoch 11/80\n",
      "29639/29639 [==============================] - 3s 99us/step - loss: 1.5947 - acc: 0.2493 - val_loss: 1.5956 - val_acc: 0.2478\n",
      "Inner step: 13 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 6s 201us/step - loss: 1.5951 - acc: 0.2464 - val_loss: 1.5931 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 3s 101us/step - loss: 1.5950 - acc: 0.2474 - val_loss: 1.5941 - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 3s 108us/step - loss: 1.5947 - acc: 0.2467 - val_loss: 1.5966 - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 3s 100us/step - loss: 1.5951 - acc: 0.2458 - val_loss: 1.5971 - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 3s 105us/step - loss: 1.5952 - acc: 0.2483 - val_loss: 1.5941 - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 3s 98us/step - loss: 1.5953 - acc: 0.2484 - val_loss: 1.5936 - val_acc: 0.2478\n",
      "Inner step: 14 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 7s 234us/step - loss: 1.5950 - acc: 0.2491 - val_loss: 1.5945 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 3s 101us/step - loss: 1.5952 - acc: 0.2473 - val_loss: 1.5944 - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 3s 106us/step - loss: 1.5949 - acc: 0.2492 - val_loss: 1.5958 - val_acc: 0.2342\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 3s 95us/step - loss: 1.5950 - acc: 0.2492 - val_loss: 1.5934 - val_acc: 0.2342\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 3s 92us/step - loss: 1.5951 - acc: 0.2465 - val_loss: 1.5953 - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 3s 90us/step - loss: 1.5953 - acc: 0.2487 - val_loss: 1.5955 - val_acc: 0.2478\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 3s 90us/step - loss: 1.5950 - acc: 0.2466 - val_loss: 1.5986 - val_acc: 0.2478\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 3s 91us/step - loss: 1.5950 - acc: 0.2500 - val_loss: 1.5949 - val_acc: 0.2478\n",
      "Epoch 9/80\n",
      "29639/29639 [==============================] - 3s 92us/step - loss: 1.5950 - acc: 0.2480 - val_loss: 1.5961 - val_acc: 0.2478\n",
      "Inner step: 15 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 5s 184us/step - loss: 1.5956 - acc: 0.2465 - val_loss: 1.5954 - val_acc: 0.2342\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 3s 94us/step - loss: 1.5949 - acc: 0.2489 - val_loss: 1.5933 - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 3s 92us/step - loss: 1.5948 - acc: 0.2497 - val_loss: 1.5954 - val_acc: 0.2342\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 3s 92us/step - loss: 1.5951 - acc: 0.2450 - val_loss: 1.5929 - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 3s 92us/step - loss: 1.5936 - acc: 0.2480 - val_loss: 1.5535 - val_acc: 0.2823\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 3s 96us/step - loss: 1.5179 - acc: 0.2975 - val_loss: 1.5064 - val_acc: 0.3085\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 3s 110us/step - loss: 1.5051 - acc: 0.3008 - val_loss: 1.5079 - val_acc: 0.3104\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 3s 102us/step - loss: 1.4403 - acc: 0.3469 - val_loss: 1.3350 - val_acc: 0.4109\n",
      "Epoch 9/80\n",
      "29639/29639 [==============================] - 3s 103us/step - loss: 1.2853 - acc: 0.4452 - val_loss: 1.1977 - val_acc: 0.5085\n",
      "Epoch 10/80\n",
      "29639/29639 [==============================] - 3s 95us/step - loss: 1.1189 - acc: 0.5361 - val_loss: 1.0958 - val_acc: 0.5639\n",
      "Epoch 11/80\n",
      "29639/29639 [==============================] - 3s 91us/step - loss: 1.0345 - acc: 0.5692 - val_loss: 1.0707 - val_acc: 0.5673\n",
      "Epoch 12/80\n",
      "29639/29639 [==============================] - 3s 93us/step - loss: 1.0042 - acc: 0.5806 - val_loss: 1.0472 - val_acc: 0.5782\n",
      "Epoch 13/80\n",
      "29639/29639 [==============================] - 3s 92us/step - loss: 0.9879 - acc: 0.5878 - val_loss: 1.0361 - val_acc: 0.5828\n",
      "Epoch 14/80\n",
      "29639/29639 [==============================] - 3s 95us/step - loss: 0.9807 - acc: 0.5906 - val_loss: 1.0480 - val_acc: 0.5818\n",
      "Epoch 15/80\n",
      "29639/29639 [==============================] - 3s 103us/step - loss: 0.9683 - acc: 0.5959 - val_loss: 1.0381 - val_acc: 0.5950\n",
      "Epoch 16/80\n",
      "29639/29639 [==============================] - 3s 99us/step - loss: 0.9579 - acc: 0.6016 - val_loss: 0.9995 - val_acc: 0.6064\n",
      "Epoch 17/80\n",
      "29639/29639 [==============================] - 3s 109us/step - loss: 0.9430 - acc: 0.6139 - val_loss: 0.9922 - val_acc: 0.6160\n",
      "Epoch 18/80\n",
      "29639/29639 [==============================] - 3s 99us/step - loss: 0.9220 - acc: 0.6299 - val_loss: 0.9751 - val_acc: 0.6310\n",
      "Epoch 19/80\n",
      "29639/29639 [==============================] - 3s 100us/step - loss: 0.8987 - acc: 0.6492 - val_loss: 0.9591 - val_acc: 0.6477\n",
      "Epoch 20/80\n",
      "29639/29639 [==============================] - 3s 107us/step - loss: 0.8556 - acc: 0.6838 - val_loss: 0.9179 - val_acc: 0.6716\n",
      "Epoch 21/80\n",
      "29639/29639 [==============================] - 3s 100us/step - loss: 0.8084 - acc: 0.7112 - val_loss: 0.8685 - val_acc: 0.6929\n",
      "Epoch 22/80\n",
      "29639/29639 [==============================] - 3s 101us/step - loss: 0.7639 - acc: 0.7309 - val_loss: 0.8860 - val_acc: 0.6881\n",
      "Epoch 23/80\n",
      "29639/29639 [==============================] - 4s 126us/step - loss: 0.7486 - acc: 0.7402 - val_loss: 0.8422 - val_acc: 0.7100\n",
      "Epoch 24/80\n",
      "29639/29639 [==============================] - 4s 128us/step - loss: 0.7244 - acc: 0.7502 - val_loss: 0.8356 - val_acc: 0.7145\n",
      "Epoch 25/80\n",
      "29639/29639 [==============================] - 3s 109us/step - loss: 0.7028 - acc: 0.7627 - val_loss: 0.8130 - val_acc: 0.7291\n",
      "Epoch 26/80\n",
      "29639/29639 [==============================] - 3s 100us/step - loss: 0.6764 - acc: 0.7757 - val_loss: 0.7797 - val_acc: 0.7413\n",
      "Epoch 27/80\n",
      "29639/29639 [==============================] - 3s 100us/step - loss: 0.6598 - acc: 0.7828 - val_loss: 0.8108 - val_acc: 0.7263\n",
      "Epoch 28/80\n",
      "29639/29639 [==============================] - 3s 98us/step - loss: 0.6511 - acc: 0.7845 - val_loss: 0.7576 - val_acc: 0.7504\n",
      "Epoch 29/80\n",
      "29639/29639 [==============================] - 3s 105us/step - loss: 0.6362 - acc: 0.7888 - val_loss: 0.7808 - val_acc: 0.7443\n",
      "Epoch 30/80\n",
      "29639/29639 [==============================] - 3s 93us/step - loss: 0.6235 - acc: 0.7916 - val_loss: 0.7579 - val_acc: 0.7462\n",
      "Epoch 31/80\n",
      "29639/29639 [==============================] - 3s 93us/step - loss: 0.6225 - acc: 0.7939 - val_loss: 0.8355 - val_acc: 0.7195\n",
      "Epoch 32/80\n",
      "29639/29639 [==============================] - 3s 92us/step - loss: 0.6095 - acc: 0.7964 - val_loss: 0.7583 - val_acc: 0.7447\n",
      "Epoch 33/80\n",
      "29639/29639 [==============================] - 3s 97us/step - loss: 0.6074 - acc: 0.7970 - val_loss: 0.7651 - val_acc: 0.7523\n",
      "Inner step: 16 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 6s 212us/step - loss: 1.5948 - acc: 0.2454 - val_loss: 1.5957 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 3s 95us/step - loss: 1.5951 - acc: 0.2467 - val_loss: 1.5967 - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 3s 98us/step - loss: 1.5947 - acc: 0.2487 - val_loss: 1.5992 - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 3s 95us/step - loss: 1.5952 - acc: 0.2489 - val_loss: 1.5944 - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 3s 95us/step - loss: 1.5952 - acc: 0.2464 - val_loss: 1.5938 - val_acc: 0.2342\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 3s 104us/step - loss: 1.5793 - acc: 0.2582 - val_loss: 1.5148 - val_acc: 0.2933\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 3s 94us/step - loss: 1.5098 - acc: 0.3050 - val_loss: 1.5050 - val_acc: 0.3089\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 3s 95us/step - loss: 1.5038 - acc: 0.3038 - val_loss: 1.5085 - val_acc: 0.3070\n",
      "Epoch 9/80\n",
      "29639/29639 [==============================] - 3s 110us/step - loss: 1.4201 - acc: 0.3519 - val_loss: 1.3643 - val_acc: 0.3978\n",
      "Epoch 10/80\n",
      "29639/29639 [==============================] - 3s 117us/step - loss: 1.2962 - acc: 0.4243 - val_loss: 1.2512 - val_acc: 0.4883\n",
      "Epoch 11/80\n",
      "29639/29639 [==============================] - 3s 117us/step - loss: 1.1472 - acc: 0.5210 - val_loss: 1.1146 - val_acc: 0.5621\n",
      "Epoch 12/80\n",
      "29639/29639 [==============================] - 3s 106us/step - loss: 1.0379 - acc: 0.5692 - val_loss: 1.0531 - val_acc: 0.5707\n",
      "Epoch 13/80\n",
      "29639/29639 [==============================] - 3s 105us/step - loss: 1.0000 - acc: 0.5896 - val_loss: 1.0323 - val_acc: 0.5918\n",
      "Epoch 14/80\n",
      "29639/29639 [==============================] - 3s 109us/step - loss: 0.9731 - acc: 0.6024 - val_loss: 1.0512 - val_acc: 0.6073\n",
      "Epoch 15/80\n",
      "29639/29639 [==============================] - 3s 94us/step - loss: 0.9364 - acc: 0.6287 - val_loss: 0.9893 - val_acc: 0.6124\n",
      "Epoch 16/80\n",
      "29639/29639 [==============================] - 3s 95us/step - loss: 0.9003 - acc: 0.6481 - val_loss: 0.9793 - val_acc: 0.6293\n",
      "Epoch 17/80\n",
      "29639/29639 [==============================] - 3s 92us/step - loss: 0.8800 - acc: 0.6656 - val_loss: 0.9566 - val_acc: 0.6445\n",
      "Epoch 18/80\n",
      "29639/29639 [==============================] - 3s 111us/step - loss: 0.8454 - acc: 0.6871 - val_loss: 0.9302 - val_acc: 0.6651\n",
      "Epoch 19/80\n",
      "29639/29639 [==============================] - 3s 118us/step - loss: 0.8248 - acc: 0.7044 - val_loss: 0.9435 - val_acc: 0.6714\n",
      "Epoch 20/80\n",
      "29639/29639 [==============================] - 3s 99us/step - loss: 0.7940 - acc: 0.7220 - val_loss: 0.9236 - val_acc: 0.6751\n",
      "Epoch 21/80\n",
      "29639/29639 [==============================] - 3s 111us/step - loss: 0.7704 - acc: 0.7355 - val_loss: 0.8855 - val_acc: 0.6890\n",
      "Epoch 22/80\n",
      "29639/29639 [==============================] - 3s 105us/step - loss: 0.7558 - acc: 0.7437 - val_loss: 0.9472 - val_acc: 0.6725\n",
      "Epoch 23/80\n",
      "29639/29639 [==============================] - 3s 92us/step - loss: 0.7425 - acc: 0.7468 - val_loss: 0.8863 - val_acc: 0.7004\n",
      "Epoch 24/80\n",
      "29639/29639 [==============================] - 3s 93us/step - loss: 0.7332 - acc: 0.7510 - val_loss: 0.8375 - val_acc: 0.7124\n",
      "Epoch 25/80\n",
      "29639/29639 [==============================] - 3s 104us/step - loss: 0.7159 - acc: 0.7551 - val_loss: 0.8504 - val_acc: 0.7215\n",
      "Epoch 26/80\n",
      "29639/29639 [==============================] - 4s 138us/step - loss: 0.6879 - acc: 0.7667 - val_loss: 0.7985 - val_acc: 0.7316\n",
      "Epoch 27/80\n",
      "29639/29639 [==============================] - 3s 100us/step - loss: 0.6667 - acc: 0.7759 - val_loss: 0.8045 - val_acc: 0.7257\n",
      "Epoch 28/80\n",
      "29639/29639 [==============================] - 4s 141us/step - loss: 0.6523 - acc: 0.7819 - val_loss: 0.8126 - val_acc: 0.7371\n",
      "Epoch 29/80\n",
      "29639/29639 [==============================] - 4s 118us/step - loss: 0.6472 - acc: 0.7827 - val_loss: 0.7969 - val_acc: 0.7193\n",
      "Epoch 30/80\n",
      "29639/29639 [==============================] - 3s 96us/step - loss: 0.6377 - acc: 0.7858 - val_loss: 0.7787 - val_acc: 0.7351\n",
      "Epoch 31/80\n",
      "29639/29639 [==============================] - 3s 99us/step - loss: 0.6292 - acc: 0.7873 - val_loss: 0.7536 - val_acc: 0.7524\n",
      "Epoch 32/80\n",
      "29639/29639 [==============================] - 3s 106us/step - loss: 0.6127 - acc: 0.7957 - val_loss: 0.7671 - val_acc: 0.7533\n",
      "Epoch 33/80\n",
      "29639/29639 [==============================] - 3s 106us/step - loss: 0.6051 - acc: 0.7967 - val_loss: 0.7405 - val_acc: 0.7573\n",
      "Epoch 34/80\n",
      "29639/29639 [==============================] - 3s 102us/step - loss: 0.6011 - acc: 0.7995 - val_loss: 0.7883 - val_acc: 0.7472\n",
      "Epoch 35/80\n",
      "29639/29639 [==============================] - 3s 107us/step - loss: 0.5966 - acc: 0.7992 - val_loss: 0.7507 - val_acc: 0.7562\n",
      "Epoch 36/80\n",
      "29639/29639 [==============================] - 3s 103us/step - loss: 0.5935 - acc: 0.8008 - val_loss: 0.7229 - val_acc: 0.7652\n",
      "Epoch 37/80\n",
      "29639/29639 [==============================] - 3s 98us/step - loss: 0.5846 - acc: 0.8034 - val_loss: 0.7319 - val_acc: 0.7669\n",
      "Epoch 38/80\n",
      "29639/29639 [==============================] - 3s 92us/step - loss: 0.5810 - acc: 0.8053 - val_loss: 0.7857 - val_acc: 0.7457\n",
      "Epoch 39/80\n",
      "29639/29639 [==============================] - 3s 97us/step - loss: 0.5693 - acc: 0.8078 - val_loss: 0.7599 - val_acc: 0.7494\n",
      "Epoch 40/80\n",
      "29639/29639 [==============================] - 3s 92us/step - loss: 0.5704 - acc: 0.8074 - val_loss: 0.7506 - val_acc: 0.7519\n",
      "Epoch 41/80\n",
      "29639/29639 [==============================] - 3s 100us/step - loss: 0.5688 - acc: 0.8086 - val_loss: 0.7486 - val_acc: 0.7441\n",
      "Inner step: 17 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 6s 216us/step - loss: 1.5952 - acc: 0.2488 - val_loss: 1.5944 - val_acc: 0.2342\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 3s 94us/step - loss: 1.5953 - acc: 0.2495 - val_loss: 1.5938 - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 3s 95us/step - loss: 1.5949 - acc: 0.2505 - val_loss: 1.5937 - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 3s 98us/step - loss: 1.5952 - acc: 0.2464 - val_loss: 1.5939 - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 3s 103us/step - loss: 1.5951 - acc: 0.2489 - val_loss: 1.5974 - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 3s 97us/step - loss: 1.5948 - acc: 0.2475 - val_loss: 1.5953 - val_acc: 0.2342\n",
      "Inner step: 18 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 6s 198us/step - loss: 1.5950 - acc: 0.2499 - val_loss: 1.5939 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 3s 103us/step - loss: 1.5949 - acc: 0.2490 - val_loss: 1.5984 - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 3s 94us/step - loss: 1.5950 - acc: 0.2486 - val_loss: 1.5955 - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 3s 97us/step - loss: 1.5948 - acc: 0.2491 - val_loss: 1.5951 - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 3s 97us/step - loss: 1.5919 - acc: 0.2497 - val_loss: 1.5913 - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 3s 103us/step - loss: 1.5307 - acc: 0.2932 - val_loss: 1.5796 - val_acc: 0.3020\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 3s 95us/step - loss: 1.5057 - acc: 0.3043 - val_loss: 1.5462 - val_acc: 0.3025\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 3s 96us/step - loss: 1.5003 - acc: 0.3066 - val_loss: 1.5039 - val_acc: 0.3373\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/80\n",
      "29639/29639 [==============================] - 3s 114us/step - loss: 1.4261 - acc: 0.3578 - val_loss: 1.3582 - val_acc: 0.4216\n",
      "Epoch 10/80\n",
      "29639/29639 [==============================] - 3s 98us/step - loss: 1.2636 - acc: 0.4576 - val_loss: 1.2908 - val_acc: 0.4551\n",
      "Epoch 11/80\n",
      "29639/29639 [==============================] - 3s 109us/step - loss: 1.2045 - acc: 0.4891 - val_loss: 1.1678 - val_acc: 0.5320\n",
      "Epoch 12/80\n",
      "29639/29639 [==============================] - 3s 109us/step - loss: 1.1145 - acc: 0.5412 - val_loss: 1.1349 - val_acc: 0.5394\n",
      "Epoch 13/80\n",
      "29639/29639 [==============================] - 3s 105us/step - loss: 1.1018 - acc: 0.5454 - val_loss: 1.1483 - val_acc: 0.5399\n",
      "Epoch 14/80\n",
      "29639/29639 [==============================] - 3s 105us/step - loss: 1.0913 - acc: 0.5471 - val_loss: 1.1764 - val_acc: 0.5300\n",
      "Epoch 15/80\n",
      "29639/29639 [==============================] - 3s 98us/step - loss: 1.0881 - acc: 0.5485 - val_loss: 1.1260 - val_acc: 0.5415\n",
      "Epoch 16/80\n",
      "29639/29639 [==============================] - 3s 117us/step - loss: 1.0808 - acc: 0.5505 - val_loss: 1.1961 - val_acc: 0.5338\n",
      "Epoch 17/80\n",
      "29639/29639 [==============================] - 3s 103us/step - loss: 1.0782 - acc: 0.5513 - val_loss: 1.1315 - val_acc: 0.5410\n",
      "Epoch 18/80\n",
      "29639/29639 [==============================] - 3s 96us/step - loss: 1.0763 - acc: 0.5529 - val_loss: 1.1419 - val_acc: 0.5372\n",
      "Epoch 19/80\n",
      "29639/29639 [==============================] - 3s 93us/step - loss: 1.0714 - acc: 0.5543 - val_loss: 1.1884 - val_acc: 0.5307\n",
      "Epoch 20/80\n",
      "29639/29639 [==============================] - 3s 110us/step - loss: 1.0658 - acc: 0.5563 - val_loss: 1.1664 - val_acc: 0.5326\n",
      "Inner step: 19 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 6s 204us/step - loss: 1.5952 - acc: 0.2480 - val_loss: 1.5940 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 3s 114us/step - loss: 1.5952 - acc: 0.2477 - val_loss: 1.5951 - val_acc: 0.2342\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 3s 107us/step - loss: 1.5954 - acc: 0.2463 - val_loss: 1.5942 - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 3s 107us/step - loss: 1.5947 - acc: 0.2451 - val_loss: 1.6011 - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 3s 104us/step - loss: 1.5951 - acc: 0.2474 - val_loss: 1.5942 - val_acc: 0.2342\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 3s 108us/step - loss: 1.5863 - acc: 0.2559 - val_loss: 1.6008 - val_acc: 0.2626\n",
      "Inner step: 20 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 6s 205us/step - loss: 1.5951 - acc: 0.2460 - val_loss: 1.5967 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 4s 119us/step - loss: 1.5950 - acc: 0.2477 - val_loss: 1.5947 - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 3s 111us/step - loss: 1.5952 - acc: 0.2486 - val_loss: 1.5944 - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 3s 113us/step - loss: 1.5950 - acc: 0.2475 - val_loss: 1.5952 - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 3s 111us/step - loss: 1.5950 - acc: 0.2473 - val_loss: 1.5969 - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 3s 112us/step - loss: 1.5947 - acc: 0.2472 - val_loss: 1.5954 - val_acc: 0.2478\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 4s 119us/step - loss: 1.5944 - acc: 0.2518 - val_loss: 1.5933 - val_acc: 0.2478\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 3s 109us/step - loss: 1.5952 - acc: 0.2479 - val_loss: 1.5933 - val_acc: 0.2478\n",
      "Epoch 9/80\n",
      "29639/29639 [==============================] - 3s 102us/step - loss: 1.5954 - acc: 0.2476 - val_loss: 1.5952 - val_acc: 0.2478\n",
      "Epoch 10/80\n",
      "29639/29639 [==============================] - 3s 98us/step - loss: 1.5951 - acc: 0.2485 - val_loss: 1.5962 - val_acc: 0.2478\n",
      "Epoch 11/80\n",
      "29639/29639 [==============================] - 3s 98us/step - loss: 1.5950 - acc: 0.2454 - val_loss: 1.5949 - val_acc: 0.2342\n",
      "Epoch 12/80\n",
      "29639/29639 [==============================] - 3s 100us/step - loss: 1.5953 - acc: 0.2452 - val_loss: 1.5943 - val_acc: 0.2478\n",
      "Inner step: 21 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 6s 199us/step - loss: 1.5952 - acc: 0.2466 - val_loss: 1.5948 - val_acc: 0.2342\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 3s 97us/step - loss: 1.5949 - acc: 0.2491 - val_loss: 1.5937 - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 3s 100us/step - loss: 1.5951 - acc: 0.2481 - val_loss: 1.5932 - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 3s 105us/step - loss: 1.5949 - acc: 0.2482 - val_loss: 1.5958 - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 3s 101us/step - loss: 1.5953 - acc: 0.2458 - val_loss: 1.5945 - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 3s 96us/step - loss: 1.5950 - acc: 0.2479 - val_loss: 1.5938 - val_acc: 0.2478\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 3s 102us/step - loss: 1.5951 - acc: 0.2487 - val_loss: 1.5936 - val_acc: 0.2478\n",
      "Inner step: 22 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 6s 210us/step - loss: 1.5951 - acc: 0.2493 - val_loss: 1.5970 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 3s 100us/step - loss: 1.5950 - acc: 0.2469 - val_loss: 1.5947 - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 3s 113us/step - loss: 1.5953 - acc: 0.2465 - val_loss: 1.5942 - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 3s 108us/step - loss: 1.5953 - acc: 0.2478 - val_loss: 1.5944 - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 4s 118us/step - loss: 1.5952 - acc: 0.2485 - val_loss: 1.5935 - val_acc: 0.2342\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 3s 109us/step - loss: 1.5948 - acc: 0.2488 - val_loss: 1.5958 - val_acc: 0.2478\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 3s 113us/step - loss: 1.5952 - acc: 0.2480 - val_loss: 1.5941 - val_acc: 0.2478\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 3s 107us/step - loss: 1.5948 - acc: 0.2490 - val_loss: 1.5936 - val_acc: 0.2478\n",
      "Epoch 9/80\n",
      "29639/29639 [==============================] - 3s 107us/step - loss: 1.5949 - acc: 0.2460 - val_loss: 1.5958 - val_acc: 0.2478\n",
      "Epoch 10/80\n",
      "29639/29639 [==============================] - 3s 106us/step - loss: 1.5949 - acc: 0.2476 - val_loss: 1.5937 - val_acc: 0.2478\n",
      "Inner step: 23 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 6s 209us/step - loss: 1.5954 - acc: 0.2457 - val_loss: 1.5946 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 3s 95us/step - loss: 1.5953 - acc: 0.2471 - val_loss: 1.5939 - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 4s 121us/step - loss: 1.5948 - acc: 0.2480 - val_loss: 1.5960 - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 3s 101us/step - loss: 1.5946 - acc: 0.2498 - val_loss: 1.5973 - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 3s 99us/step - loss: 1.5952 - acc: 0.2471 - val_loss: 1.5944 - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 3s 103us/step - loss: 1.5951 - acc: 0.2449 - val_loss: 1.5957 - val_acc: 0.2478\n",
      "Inner step: 24 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 6s 210us/step - loss: 1.5952 - acc: 0.2452 - val_loss: 1.5951 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 3s 113us/step - loss: 1.5951 - acc: 0.2490 - val_loss: 1.5957 - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 3s 114us/step - loss: 1.5952 - acc: 0.2492 - val_loss: 1.5944 - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 3s 109us/step - loss: 1.5951 - acc: 0.2467 - val_loss: 1.5909 - val_acc: 0.2342\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 3s 109us/step - loss: 1.5492 - acc: 0.2912 - val_loss: 1.5535 - val_acc: 0.2918\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 4s 127us/step - loss: 1.5303 - acc: 0.3023 - val_loss: 1.5391 - val_acc: 0.2971\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 3s 111us/step - loss: 1.5263 - acc: 0.3041 - val_loss: 1.5652 - val_acc: 0.2894\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 4s 124us/step - loss: 1.4950 - acc: 0.3199 - val_loss: 1.4601 - val_acc: 0.3415\n",
      "Epoch 9/80\n",
      "29639/29639 [==============================] - 3s 102us/step - loss: 1.3131 - acc: 0.4264 - val_loss: 1.2818 - val_acc: 0.4676\n",
      "Epoch 10/80\n",
      "29639/29639 [==============================] - 3s 105us/step - loss: 1.2020 - acc: 0.4845 - val_loss: 1.2261 - val_acc: 0.4889\n",
      "Epoch 11/80\n",
      "29639/29639 [==============================] - 3s 100us/step - loss: 1.1655 - acc: 0.4989 - val_loss: 1.2221 - val_acc: 0.5006\n",
      "Epoch 12/80\n",
      "29639/29639 [==============================] - 3s 117us/step - loss: 1.1538 - acc: 0.5050 - val_loss: 1.1899 - val_acc: 0.5083\n",
      "Epoch 13/80\n",
      "29639/29639 [==============================] - 3s 99us/step - loss: 1.1466 - acc: 0.5106 - val_loss: 1.2005 - val_acc: 0.5066\n",
      "Epoch 14/80\n",
      "29639/29639 [==============================] - 3s 102us/step - loss: 1.1329 - acc: 0.5158 - val_loss: 1.1874 - val_acc: 0.5164\n",
      "Epoch 15/80\n",
      "29639/29639 [==============================] - 3s 102us/step - loss: 1.1290 - acc: 0.5203 - val_loss: 1.2769 - val_acc: 0.4808\n",
      "Epoch 16/80\n",
      "29639/29639 [==============================] - 3s 100us/step - loss: 1.1176 - acc: 0.5293 - val_loss: 1.1864 - val_acc: 0.5274\n",
      "Epoch 17/80\n",
      "29639/29639 [==============================] - 3s 112us/step - loss: 1.0632 - acc: 0.5605 - val_loss: 1.1134 - val_acc: 0.5744\n",
      "Epoch 18/80\n",
      "29639/29639 [==============================] - 3s 109us/step - loss: 0.9985 - acc: 0.5928 - val_loss: 1.0507 - val_acc: 0.5822\n",
      "Epoch 19/80\n",
      "29639/29639 [==============================] - 3s 116us/step - loss: 0.9696 - acc: 0.6053 - val_loss: 1.0858 - val_acc: 0.5803\n",
      "Epoch 20/80\n",
      "29639/29639 [==============================] - 3s 112us/step - loss: 0.9438 - acc: 0.6199 - val_loss: 1.0285 - val_acc: 0.6041\n",
      "Epoch 21/80\n",
      "29639/29639 [==============================] - 3s 95us/step - loss: 0.9218 - acc: 0.6287 - val_loss: 1.0232 - val_acc: 0.6141\n",
      "Epoch 22/80\n",
      "29639/29639 [==============================] - 3s 100us/step - loss: 0.9086 - acc: 0.6380 - val_loss: 1.0131 - val_acc: 0.6212\n",
      "Epoch 23/80\n",
      "29639/29639 [==============================] - 3s 101us/step - loss: 0.8861 - acc: 0.6544 - val_loss: 0.9834 - val_acc: 0.6308\n",
      "Epoch 24/80\n",
      "29639/29639 [==============================] - 3s 107us/step - loss: 0.8610 - acc: 0.6715 - val_loss: 1.0114 - val_acc: 0.6320\n",
      "Epoch 25/80\n",
      "29639/29639 [==============================] - 3s 97us/step - loss: 0.8447 - acc: 0.6803 - val_loss: 0.9758 - val_acc: 0.6520\n",
      "Epoch 26/80\n",
      "29639/29639 [==============================] - 3s 96us/step - loss: 0.8298 - acc: 0.6875 - val_loss: 0.9132 - val_acc: 0.6666\n",
      "Epoch 27/80\n",
      "29639/29639 [==============================] - 3s 112us/step - loss: 0.8193 - acc: 0.6935 - val_loss: 0.9495 - val_acc: 0.6497\n",
      "Epoch 28/80\n",
      "29639/29639 [==============================] - 3s 114us/step - loss: 0.8059 - acc: 0.7007 - val_loss: 0.9606 - val_acc: 0.6594\n",
      "Epoch 29/80\n",
      "29639/29639 [==============================] - 3s 105us/step - loss: 0.7991 - acc: 0.7054 - val_loss: 0.9042 - val_acc: 0.6748\n",
      "Epoch 30/80\n",
      "29639/29639 [==============================] - 3s 114us/step - loss: 0.7894 - acc: 0.7084 - val_loss: 1.0219 - val_acc: 0.6287\n",
      "Epoch 31/80\n",
      "29639/29639 [==============================] - 4s 137us/step - loss: 0.7866 - acc: 0.7133 - val_loss: 0.9253 - val_acc: 0.6773\n",
      "Epoch 32/80\n",
      "29639/29639 [==============================] - 4s 136us/step - loss: 0.7776 - acc: 0.7155 - val_loss: 0.9123 - val_acc: 0.6760\n",
      "Epoch 33/80\n",
      "29639/29639 [==============================] - 3s 108us/step - loss: 0.7694 - acc: 0.7210 - val_loss: 0.9947 - val_acc: 0.6450\n",
      "Epoch 34/80\n",
      "29639/29639 [==============================] - 3s 102us/step - loss: 0.7679 - acc: 0.7235 - val_loss: 0.9487 - val_acc: 0.6714\n",
      "Inner step: 25 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 6s 215us/step - loss: 1.5950 - acc: 0.2436 - val_loss: 1.5966 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 3s 103us/step - loss: 1.5339 - acc: 0.2917 - val_loss: 1.4784 - val_acc: 0.3314\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 3s 109us/step - loss: 1.3779 - acc: 0.3900 - val_loss: 1.2523 - val_acc: 0.4743\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 3s 105us/step - loss: 1.1875 - acc: 0.4782 - val_loss: 1.2167 - val_acc: 0.4839\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 3s 109us/step - loss: 1.1563 - acc: 0.4863 - val_loss: 1.1925 - val_acc: 0.4840\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 3s 105us/step - loss: 1.1436 - acc: 0.4957 - val_loss: 1.2046 - val_acc: 0.4879\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 3s 115us/step - loss: 1.1271 - acc: 0.5075 - val_loss: 1.1863 - val_acc: 0.4998\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 3s 111us/step - loss: 1.1012 - acc: 0.5328 - val_loss: 1.1567 - val_acc: 0.5179\n",
      "Epoch 9/80\n",
      "29639/29639 [==============================] - 3s 104us/step - loss: 1.0537 - acc: 0.5634 - val_loss: 1.1161 - val_acc: 0.5457\n",
      "Epoch 10/80\n",
      "29639/29639 [==============================] - 3s 113us/step - loss: 0.9949 - acc: 0.5953 - val_loss: 1.0639 - val_acc: 0.5982\n",
      "Epoch 11/80\n",
      "29639/29639 [==============================] - 3s 103us/step - loss: 0.9392 - acc: 0.6358 - val_loss: 1.0052 - val_acc: 0.6264\n",
      "Epoch 12/80\n",
      "29639/29639 [==============================] - 3s 102us/step - loss: 0.8681 - acc: 0.6835 - val_loss: 1.0054 - val_acc: 0.6357\n",
      "Epoch 13/80\n",
      "29639/29639 [==============================] - 3s 103us/step - loss: 0.8098 - acc: 0.7201 - val_loss: 0.8880 - val_acc: 0.6865\n",
      "Epoch 14/80\n",
      "29639/29639 [==============================] - 3s 109us/step - loss: 0.7748 - acc: 0.7356 - val_loss: 0.8587 - val_acc: 0.7029\n",
      "Epoch 15/80\n",
      "29639/29639 [==============================] - 3s 107us/step - loss: 0.7504 - acc: 0.7458 - val_loss: 0.8553 - val_acc: 0.7151\n",
      "Epoch 16/80\n",
      "29639/29639 [==============================] - 4s 119us/step - loss: 0.7387 - acc: 0.7545 - val_loss: 0.8422 - val_acc: 0.7150\n",
      "Epoch 17/80\n",
      "29639/29639 [==============================] - 3s 109us/step - loss: 0.7251 - acc: 0.7611 - val_loss: 0.8524 - val_acc: 0.7160\n",
      "Epoch 18/80\n",
      "29639/29639 [==============================] - 3s 99us/step - loss: 0.7060 - acc: 0.7707 - val_loss: 0.8333 - val_acc: 0.7335\n",
      "Epoch 19/80\n",
      "29639/29639 [==============================] - 3s 96us/step - loss: 0.6931 - acc: 0.7727 - val_loss: 0.8246 - val_acc: 0.7260\n",
      "Epoch 20/80\n",
      "29639/29639 [==============================] - 3s 97us/step - loss: 0.6805 - acc: 0.7795 - val_loss: 0.8208 - val_acc: 0.7368\n",
      "Epoch 21/80\n",
      "29639/29639 [==============================] - 3s 104us/step - loss: 0.6675 - acc: 0.7846 - val_loss: 0.7983 - val_acc: 0.7351\n",
      "Epoch 22/80\n",
      "29639/29639 [==============================] - 3s 110us/step - loss: 0.6580 - acc: 0.7879 - val_loss: 0.8965 - val_acc: 0.7136\n",
      "Epoch 23/80\n",
      "29639/29639 [==============================] - 3s 99us/step - loss: 0.6511 - acc: 0.7903 - val_loss: 0.7900 - val_acc: 0.7462\n",
      "Epoch 24/80\n",
      "29639/29639 [==============================] - 3s 109us/step - loss: 0.6489 - acc: 0.7918 - val_loss: 0.7935 - val_acc: 0.7341\n",
      "Epoch 25/80\n",
      "29639/29639 [==============================] - 3s 114us/step - loss: 0.6425 - acc: 0.7922 - val_loss: 0.8191 - val_acc: 0.7360\n",
      "Epoch 26/80\n",
      "29639/29639 [==============================] - 3s 96us/step - loss: 0.6306 - acc: 0.7957 - val_loss: 0.7709 - val_acc: 0.7465\n",
      "Epoch 27/80\n",
      "29639/29639 [==============================] - 3s 99us/step - loss: 0.6352 - acc: 0.7932 - val_loss: 0.7846 - val_acc: 0.7463\n",
      "Epoch 28/80\n",
      "29639/29639 [==============================] - 3s 97us/step - loss: 0.6279 - acc: 0.7956 - val_loss: 0.7874 - val_acc: 0.7466\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/80\n",
      "29639/29639 [==============================] - 3s 97us/step - loss: 0.6180 - acc: 0.7997 - val_loss: 0.7893 - val_acc: 0.7417\n",
      "Epoch 30/80\n",
      "29639/29639 [==============================] - 3s 99us/step - loss: 0.6156 - acc: 0.7994 - val_loss: 0.8110 - val_acc: 0.7400\n",
      "Epoch 31/80\n",
      "29639/29639 [==============================] - 3s 97us/step - loss: 0.6166 - acc: 0.7989 - val_loss: 0.7971 - val_acc: 0.7395\n",
      "Inner step: 26 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 6s 202us/step - loss: 1.5953 - acc: 0.2466 - val_loss: 1.5952 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 3s 98us/step - loss: 1.5952 - acc: 0.2486 - val_loss: 1.5937 - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 3s 97us/step - loss: 1.5950 - acc: 0.2485 - val_loss: 1.5950 - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 3s 103us/step - loss: 1.5946 - acc: 0.2490 - val_loss: 1.5950 - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 3s 105us/step - loss: 1.5950 - acc: 0.2488 - val_loss: 1.5967 - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 3s 111us/step - loss: 1.5949 - acc: 0.2479 - val_loss: 1.5934 - val_acc: 0.2478\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 3s 108us/step - loss: 1.5949 - acc: 0.2498 - val_loss: 1.5944 - val_acc: 0.2342\n",
      "Inner step: 27 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 6s 211us/step - loss: 1.5950 - acc: 0.2474 - val_loss: 1.5943 - val_acc: 0.2342\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 3s 100us/step - loss: 1.5951 - acc: 0.2454 - val_loss: 1.5947 - val_acc: 0.2342\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 3s 101us/step - loss: 1.5951 - acc: 0.2466 - val_loss: 1.5942 - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 3s 107us/step - loss: 1.5948 - acc: 0.2484 - val_loss: 1.5949 - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 3s 107us/step - loss: 1.5950 - acc: 0.2483 - val_loss: 1.5927 - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 3s 102us/step - loss: 1.5949 - acc: 0.2502 - val_loss: 1.5943 - val_acc: 0.2478\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 3s 102us/step - loss: 1.5949 - acc: 0.2495 - val_loss: 1.5936 - val_acc: 0.2342\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 3s 101us/step - loss: 1.5952 - acc: 0.2484 - val_loss: 1.5947 - val_acc: 0.2478\n",
      "Epoch 9/80\n",
      "29639/29639 [==============================] - 3s 99us/step - loss: 1.5948 - acc: 0.2480 - val_loss: 1.5950 - val_acc: 0.2342\n",
      "Epoch 10/80\n",
      "29639/29639 [==============================] - 3s 106us/step - loss: 1.5947 - acc: 0.2494 - val_loss: 1.5959 - val_acc: 0.2478\n",
      "Inner step: 28 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 7s 245us/step - loss: 1.5948 - acc: 0.2489 - val_loss: 1.5974 - val_acc: 0.2342\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 3s 101us/step - loss: 1.5951 - acc: 0.2493 - val_loss: 1.5955 - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 3s 102us/step - loss: 1.5952 - acc: 0.2467 - val_loss: 1.5943 - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 3s 102us/step - loss: 1.5952 - acc: 0.2469 - val_loss: 1.5950 - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 3s 102us/step - loss: 1.5949 - acc: 0.2454 - val_loss: 1.5959 - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 4s 124us/step - loss: 1.5950 - acc: 0.2481 - val_loss: 1.5940 - val_acc: 0.2478\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 3s 102us/step - loss: 1.5952 - acc: 0.2474 - val_loss: 1.5948 - val_acc: 0.2478\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 3s 102us/step - loss: 1.5950 - acc: 0.2467 - val_loss: 1.5935 - val_acc: 0.2342\n",
      "Inner step: 29 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 7s 245us/step - loss: 1.5953 - acc: 0.2466 - val_loss: 1.5944 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 3s 106us/step - loss: 1.5950 - acc: 0.2471 - val_loss: 1.5946 - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 3s 103us/step - loss: 1.5951 - acc: 0.2467 - val_loss: 1.5957 - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 3s 109us/step - loss: 1.5948 - acc: 0.2485 - val_loss: 1.5943 - val_acc: 0.2342\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 4s 135us/step - loss: 1.5953 - acc: 0.2449 - val_loss: 1.5957 - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 3s 113us/step - loss: 1.5952 - acc: 0.2462 - val_loss: 1.5935 - val_acc: 0.2478\n",
      "Inner step: 30 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 7s 245us/step - loss: 1.5952 - acc: 0.2451 - val_loss: 1.5957 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 3s 115us/step - loss: 1.5952 - acc: 0.2469 - val_loss: 1.5945 - val_acc: 0.2342\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 3s 109us/step - loss: 1.5949 - acc: 0.2509 - val_loss: 1.5944 - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 3s 110us/step - loss: 1.5947 - acc: 0.2493 - val_loss: 1.5952 - val_acc: 0.2342\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 3s 111us/step - loss: 1.5950 - acc: 0.2479 - val_loss: 1.5969 - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 3s 104us/step - loss: 1.5950 - acc: 0.2454 - val_loss: 1.5966 - val_acc: 0.2478\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 3s 107us/step - loss: 1.5948 - acc: 0.2482 - val_loss: 1.5956 - val_acc: 0.2342\n",
      "Inner step: 31 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 7s 223us/step - loss: 1.5949 - acc: 0.2512 - val_loss: 1.5931 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 3s 103us/step - loss: 1.5950 - acc: 0.2475 - val_loss: 1.5948 - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 3s 107us/step - loss: 1.5950 - acc: 0.2520 - val_loss: 1.5930 - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 3s 102us/step - loss: 1.5948 - acc: 0.2493 - val_loss: 1.5991 - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 3s 104us/step - loss: 1.5954 - acc: 0.2457 - val_loss: 1.5935 - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 3s 103us/step - loss: 1.5951 - acc: 0.2450 - val_loss: 1.5933 - val_acc: 0.2478\n",
      "Inner step: 32 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 6s 210us/step - loss: 1.5955 - acc: 0.2466 - val_loss: 1.5943 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 3s 99us/step - loss: 1.5947 - acc: 0.2477 - val_loss: 1.5952 - val_acc: 0.2342\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 3s 108us/step - loss: 1.5948 - acc: 0.2472 - val_loss: 1.5942 - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 3s 107us/step - loss: 1.5950 - acc: 0.2492 - val_loss: 1.5960 - val_acc: 0.2342\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 3s 104us/step - loss: 1.5953 - acc: 0.2440 - val_loss: 1.5945 - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 3s 109us/step - loss: 1.5951 - acc: 0.2487 - val_loss: 1.5950 - val_acc: 0.2478\n",
      "Inner step: 33 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 8s 264us/step - loss: 1.5951 - acc: 0.2453 - val_loss: 1.5941 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 3s 112us/step - loss: 1.5952 - acc: 0.2463 - val_loss: 1.5933 - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 3s 113us/step - loss: 1.5950 - acc: 0.2480 - val_loss: 1.5939 - val_acc: 0.2342\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 3s 109us/step - loss: 1.5954 - acc: 0.2482 - val_loss: 1.5942 - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 3s 104us/step - loss: 1.5952 - acc: 0.2476 - val_loss: 1.5943 - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 3s 111us/step - loss: 1.5946 - acc: 0.2499 - val_loss: 1.5951 - val_acc: 0.2342\n",
      "Inner step: 34 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 7s 247us/step - loss: 1.5949 - acc: 0.2470 - val_loss: 1.5941 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 3s 99us/step - loss: 1.5947 - acc: 0.2479 - val_loss: 1.5934 - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 3s 97us/step - loss: 1.5951 - acc: 0.2444 - val_loss: 1.5938 - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 3s 97us/step - loss: 1.5950 - acc: 0.2485 - val_loss: 1.5941 - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 3s 112us/step - loss: 1.5950 - acc: 0.2499 - val_loss: 1.5964 - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 3s 100us/step - loss: 1.5950 - acc: 0.2492 - val_loss: 1.5954 - val_acc: 0.2478\n",
      "Inner step: 35 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 7s 224us/step - loss: 1.5950 - acc: 0.2491 - val_loss: 1.5928 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 3s 106us/step - loss: 1.5947 - acc: 0.2472 - val_loss: 1.5934 - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 3s 105us/step - loss: 1.5950 - acc: 0.2471 - val_loss: 1.5950 - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 3s 103us/step - loss: 1.5948 - acc: 0.2467 - val_loss: 1.5975 - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 3s 113us/step - loss: 1.5954 - acc: 0.2478 - val_loss: 1.5947 - val_acc: 0.2342\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 3s 102us/step - loss: 1.5951 - acc: 0.2467 - val_loss: 1.5931 - val_acc: 0.2478\n",
      "Inner step: 36 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 6s 214us/step - loss: 1.5953 - acc: 0.2460 - val_loss: 1.5963 - val_acc: 0.2342\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 3s 103us/step - loss: 1.5948 - acc: 0.2486 - val_loss: 1.5936 - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 3s 103us/step - loss: 1.5949 - acc: 0.2478 - val_loss: 1.5938 - val_acc: 0.2342\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 3s 103us/step - loss: 1.5946 - acc: 0.2464 - val_loss: 1.5947 - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 3s 103us/step - loss: 1.5950 - acc: 0.2480 - val_loss: 1.5932 - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 3s 106us/step - loss: 1.5950 - acc: 0.2495 - val_loss: 1.5968 - val_acc: 0.2342\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 3s 113us/step - loss: 1.5951 - acc: 0.2473 - val_loss: 1.5942 - val_acc: 0.2342\n",
      "Inner step: 37 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 7s 228us/step - loss: 1.5950 - acc: 0.2467 - val_loss: 1.5949 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 3s 103us/step - loss: 1.5954 - acc: 0.2486 - val_loss: 1.5991 - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 3s 115us/step - loss: 1.5952 - acc: 0.2501 - val_loss: 1.5937 - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 3s 115us/step - loss: 1.5953 - acc: 0.2475 - val_loss: 1.5938 - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 3s 103us/step - loss: 1.5948 - acc: 0.2474 - val_loss: 1.5929 - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 3s 107us/step - loss: 1.5950 - acc: 0.2464 - val_loss: 1.5964 - val_acc: 0.2478\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 3s 101us/step - loss: 1.5951 - acc: 0.2474 - val_loss: 1.5934 - val_acc: 0.2478\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 3s 103us/step - loss: 1.5950 - acc: 0.2473 - val_loss: 1.5928 - val_acc: 0.2478\n",
      "Inner step: 38 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 6s 217us/step - loss: 1.5949 - acc: 0.2447 - val_loss: 1.5929 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 3s 100us/step - loss: 1.5953 - acc: 0.2464 - val_loss: 1.5944 - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 3s 100us/step - loss: 1.5949 - acc: 0.2454 - val_loss: 1.5932 - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 3s 99us/step - loss: 1.5952 - acc: 0.2481 - val_loss: 1.5937 - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 3s 115us/step - loss: 1.5948 - acc: 0.2495 - val_loss: 1.5941 - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 3s 110us/step - loss: 1.5954 - acc: 0.2481 - val_loss: 1.5929 - val_acc: 0.2478\n",
      "Inner step: 39 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 7s 222us/step - loss: 1.5950 - acc: 0.2487 - val_loss: 1.5944 - val_acc: 0.2342\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 3s 110us/step - loss: 1.5949 - acc: 0.2462 - val_loss: 1.5932 - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 3s 110us/step - loss: 1.5951 - acc: 0.2485 - val_loss: 1.5928 - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 3s 104us/step - loss: 1.5950 - acc: 0.2491 - val_loss: 1.5951 - val_acc: 0.2342\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 3s 111us/step - loss: 1.5951 - acc: 0.2485 - val_loss: 1.5962 - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 3s 115us/step - loss: 1.5954 - acc: 0.2462 - val_loss: 1.5928 - val_acc: 0.2478\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 3s 115us/step - loss: 1.5951 - acc: 0.2439 - val_loss: 1.5934 - val_acc: 0.2478\n",
      "Inner step: 40 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 8s 256us/step - loss: 1.5951 - acc: 0.2462 - val_loss: 1.5958 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 4s 137us/step - loss: 1.5954 - acc: 0.2465 - val_loss: 1.5939 - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 4s 128us/step - loss: 1.5953 - acc: 0.2478 - val_loss: 1.5947 - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 4s 118us/step - loss: 1.5952 - acc: 0.2464 - val_loss: 1.5943 - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 3s 106us/step - loss: 1.5951 - acc: 0.2474 - val_loss: 1.5953 - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 3s 114us/step - loss: 1.5948 - acc: 0.2482 - val_loss: 1.5982 - val_acc: 0.2478\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 3s 111us/step - loss: 1.5948 - acc: 0.2475 - val_loss: 1.5969 - val_acc: 0.2478\n",
      "Training model 4 / 4\n",
      "Inner step: 1 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 8s 270us/step - loss: 1.5951 - acc: 0.2484 - val_loss: 1.5929 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 4s 129us/step - loss: 1.5948 - acc: 0.2461 - val_loss: 1.5933 - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 4s 132us/step - loss: 1.5950 - acc: 0.2482 - val_loss: 1.5947 - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 3s 113us/step - loss: 1.5950 - acc: 0.2482 - val_loss: 1.5952 - val_acc: 0.2478\n",
      "Epoch 5/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29639/29639 [==============================] - 3s 116us/step - loss: 1.5951 - acc: 0.2502 - val_loss: 1.5942 - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 3s 117us/step - loss: 1.5949 - acc: 0.2473 - val_loss: 1.5973 - val_acc: 0.2478\n",
      "Inner step: 2 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 8s 266us/step - loss: 1.5953 - acc: 0.2472 - val_loss: 1.5933 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 4s 142us/step - loss: 1.5950 - acc: 0.2474 - val_loss: 1.5940 - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 4s 151us/step - loss: 1.5951 - acc: 0.2451 - val_loss: 1.5980 - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 3s 116us/step - loss: 1.5950 - acc: 0.2473 - val_loss: 1.5935 - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 3s 116us/step - loss: 1.5951 - acc: 0.2482 - val_loss: 1.5961 - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 3s 115us/step - loss: 1.5947 - acc: 0.2466 - val_loss: 1.5982 - val_acc: 0.2478\n",
      "Inner step: 3 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 7s 243us/step - loss: 1.5949 - acc: 0.2486 - val_loss: 1.5945 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 4s 128us/step - loss: 1.5953 - acc: 0.2500 - val_loss: 1.5932 - val_acc: 0.2342\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 4s 118us/step - loss: 1.5951 - acc: 0.2463 - val_loss: 1.5951 - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 4s 123us/step - loss: 1.5949 - acc: 0.2456 - val_loss: 1.5937 - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 3s 115us/step - loss: 1.5948 - acc: 0.2494 - val_loss: 1.5931 - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 3s 114us/step - loss: 1.5949 - acc: 0.2481 - val_loss: 1.5941 - val_acc: 0.2478\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 3s 117us/step - loss: 1.5951 - acc: 0.2500 - val_loss: 1.5940 - val_acc: 0.2478\n",
      "Inner step: 4 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 8s 282us/step - loss: 1.5952 - acc: 0.2485 - val_loss: 1.5943 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 3s 107us/step - loss: 1.5950 - acc: 0.2491 - val_loss: 1.5962 - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 3s 107us/step - loss: 1.5950 - acc: 0.2480 - val_loss: 1.5957 - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 3s 107us/step - loss: 1.5948 - acc: 0.2481 - val_loss: 1.5955 - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 3s 108us/step - loss: 1.5951 - acc: 0.2505 - val_loss: 1.5940 - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 3s 107us/step - loss: 1.5951 - acc: 0.2467 - val_loss: 1.5951 - val_acc: 0.2478\n",
      "Inner step: 5 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 7s 238us/step - loss: 1.5948 - acc: 0.2474 - val_loss: 1.5951 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 3s 107us/step - loss: 1.5949 - acc: 0.2486 - val_loss: 1.5930 - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 3s 106us/step - loss: 1.5952 - acc: 0.2509 - val_loss: 1.5938 - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 3s 110us/step - loss: 1.5945 - acc: 0.2492 - val_loss: 1.5954 - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 3s 109us/step - loss: 1.5952 - acc: 0.2486 - val_loss: 1.5952 - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 3s 108us/step - loss: 1.5948 - acc: 0.2494 - val_loss: 1.5941 - val_acc: 0.2478\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 3s 112us/step - loss: 1.5952 - acc: 0.2466 - val_loss: 1.5940 - val_acc: 0.2478\n",
      "Inner step: 6 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 7s 240us/step - loss: 1.5954 - acc: 0.2463 - val_loss: 1.5961 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 3s 109us/step - loss: 1.5954 - acc: 0.2454 - val_loss: 1.5946 - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 3s 109us/step - loss: 1.5953 - acc: 0.2488 - val_loss: 1.5933 - val_acc: 0.2342\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 4s 119us/step - loss: 1.5951 - acc: 0.2458 - val_loss: 1.5928 - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 4s 121us/step - loss: 1.5950 - acc: 0.2484 - val_loss: 1.5965 - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 4s 119us/step - loss: 1.5953 - acc: 0.2477 - val_loss: 1.5955 - val_acc: 0.2342\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 3s 114us/step - loss: 1.5947 - acc: 0.2483 - val_loss: 1.5942 - val_acc: 0.2342\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 3s 107us/step - loss: 1.5949 - acc: 0.2477 - val_loss: 1.5934 - val_acc: 0.2478\n",
      "Inner step: 7 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 7s 252us/step - loss: 1.5950 - acc: 0.2453 - val_loss: 1.5952 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 4s 119us/step - loss: 1.5950 - acc: 0.2473 - val_loss: 1.5931 - val_acc: 0.2342\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 4s 131us/step - loss: 1.5949 - acc: 0.2478 - val_loss: 1.5959 - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 4s 123us/step - loss: 1.5951 - acc: 0.2462 - val_loss: 1.5969 - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 4s 122us/step - loss: 1.5951 - acc: 0.2440 - val_loss: 1.5970 - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 3s 108us/step - loss: 1.5951 - acc: 0.2484 - val_loss: 1.5961 - val_acc: 0.2478\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 3s 109us/step - loss: 1.5951 - acc: 0.2491 - val_loss: 1.5938 - val_acc: 0.2478\n",
      "Inner step: 8 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 7s 250us/step - loss: 1.5952 - acc: 0.2493 - val_loss: 1.5963 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 4s 118us/step - loss: 1.5947 - acc: 0.2475 - val_loss: 1.5934 - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 4s 128us/step - loss: 1.5949 - acc: 0.2500 - val_loss: 1.5936 - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 4s 134us/step - loss: 1.5951 - acc: 0.2471 - val_loss: 1.5937 - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 4s 146us/step - loss: 1.5951 - acc: 0.2477 - val_loss: 1.5936 - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 5s 156us/step - loss: 1.5950 - acc: 0.2487 - val_loss: 1.5942 - val_acc: 0.2342\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 5s 180us/step - loss: 1.5952 - acc: 0.2490 - val_loss: 1.5936 - val_acc: 0.2478\n",
      "Inner step: 9 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 11s 357us/step - loss: 1.5949 - acc: 0.2492 - val_loss: 1.5929 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 4s 130us/step - loss: 1.5953 - acc: 0.2467 - val_loss: 1.5940 - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 4s 120us/step - loss: 1.5950 - acc: 0.2476 - val_loss: 1.5929 - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 4s 147us/step - loss: 1.5949 - acc: 0.2464 - val_loss: 1.5936 - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 5s 158us/step - loss: 1.5947 - acc: 0.2470 - val_loss: 1.5934 - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 4s 127us/step - loss: 1.5948 - acc: 0.2489 - val_loss: 1.5950 - val_acc: 0.2478\n",
      "Inner step: 10 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 8s 276us/step - loss: 1.5952 - acc: 0.2478 - val_loss: 1.5941 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 5s 163us/step - loss: 1.5951 - acc: 0.2490 - val_loss: 1.5951 - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 5s 166us/step - loss: 1.5952 - acc: 0.2479 - val_loss: 1.5935 - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 4s 143us/step - loss: 1.5949 - acc: 0.2490 - val_loss: 1.5950 - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 5s 153us/step - loss: 1.5949 - acc: 0.2467 - val_loss: 1.5946 - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 5s 167us/step - loss: 1.5951 - acc: 0.2445 - val_loss: 1.5936 - val_acc: 0.2478\n",
      "Inner step: 11 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 9s 303us/step - loss: 1.5949 - acc: 0.2486 - val_loss: 1.5967 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 4s 128us/step - loss: 1.5953 - acc: 0.2481 - val_loss: 1.5950 - val_acc: 0.2342\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 4s 127us/step - loss: 1.5951 - acc: 0.2471 - val_loss: 1.5938 - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 4s 124us/step - loss: 1.5953 - acc: 0.2480 - val_loss: 1.5935 - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 4s 128us/step - loss: 1.5952 - acc: 0.2489 - val_loss: 1.5933 - val_acc: 0.2342\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 4s 128us/step - loss: 1.5952 - acc: 0.2478 - val_loss: 1.5945 - val_acc: 0.2478\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 4s 127us/step - loss: 1.5954 - acc: 0.2462 - val_loss: 1.5934 - val_acc: 0.2342\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 4s 126us/step - loss: 1.5950 - acc: 0.2482 - val_loss: 1.5947 - val_acc: 0.2478\n",
      "Inner step: 12 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 8s 271us/step - loss: 1.5955 - acc: 0.2477 - val_loss: 1.5933 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 4s 125us/step - loss: 1.5951 - acc: 0.2478 - val_loss: 1.5930 - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 4s 129us/step - loss: 1.5947 - acc: 0.2472 - val_loss: 1.5958 - val_acc: 0.2342\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 4s 131us/step - loss: 1.5945 - acc: 0.2495 - val_loss: 1.5949 - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 4s 129us/step - loss: 1.5951 - acc: 0.2468 - val_loss: 1.5935 - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 4s 128us/step - loss: 1.5951 - acc: 0.2458 - val_loss: 1.5932 - val_acc: 0.2478\n",
      "Inner step: 13 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 8s 270us/step - loss: 1.5953 - acc: 0.2472 - val_loss: 1.5941 - val_acc: 0.2342\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 4s 128us/step - loss: 1.5951 - acc: 0.2454 - val_loss: 1.5933 - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 4s 128us/step - loss: 1.5951 - acc: 0.2482 - val_loss: 1.5945 - val_acc: 0.2342\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 4s 129us/step - loss: 1.5949 - acc: 0.2489 - val_loss: 1.5946 - val_acc: 0.2342\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 4s 146us/step - loss: 1.5953 - acc: 0.2476 - val_loss: 1.5941 - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 5s 159us/step - loss: 1.5949 - acc: 0.2490 - val_loss: 1.5967 - val_acc: 0.2478\n",
      "Inner step: 14 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 9s 309us/step - loss: 1.5951 - acc: 0.2498 - val_loss: 1.5953 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 4s 145us/step - loss: 1.5951 - acc: 0.2471 - val_loss: 1.5965 - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 4s 139us/step - loss: 1.5952 - acc: 0.2467 - val_loss: 1.5936 - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 4s 145us/step - loss: 1.5949 - acc: 0.2501 - val_loss: 1.5935 - val_acc: 0.2342\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 4s 144us/step - loss: 1.5948 - acc: 0.2501 - val_loss: 1.5935 - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 4s 132us/step - loss: 1.5949 - acc: 0.2498 - val_loss: 1.5933 - val_acc: 0.2478\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 5s 154us/step - loss: 1.5953 - acc: 0.2468 - val_loss: 1.5940 - val_acc: 0.2342\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 4s 143us/step - loss: 1.5952 - acc: 0.2467 - val_loss: 1.5943 - val_acc: 0.2478\n",
      "Inner step: 15 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 9s 307us/step - loss: 1.5951 - acc: 0.2462 - val_loss: 1.5933 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 4s 146us/step - loss: 1.5951 - acc: 0.2501 - val_loss: 1.5938 - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 4s 132us/step - loss: 1.5954 - acc: 0.2494 - val_loss: 1.5962 - val_acc: 0.2342\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 4s 143us/step - loss: 1.5952 - acc: 0.2494 - val_loss: 1.5940 - val_acc: 0.2342\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 5s 154us/step - loss: 1.5953 - acc: 0.2449 - val_loss: 1.5937 - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 4s 138us/step - loss: 1.5949 - acc: 0.2479 - val_loss: 1.5943 - val_acc: 0.2478\n",
      "Inner step: 16 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 9s 291us/step - loss: 1.5953 - acc: 0.2459 - val_loss: 1.5940 - val_acc: 0.2342\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 4s 132us/step - loss: 1.5953 - acc: 0.2455 - val_loss: 1.5948 - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 4s 129us/step - loss: 1.5951 - acc: 0.2478 - val_loss: 1.5938 - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 4s 133us/step - loss: 1.5950 - acc: 0.2471 - val_loss: 1.5945 - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 4s 132us/step - loss: 1.5952 - acc: 0.2457 - val_loss: 1.5966 - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 4s 130us/step - loss: 1.5953 - acc: 0.2476 - val_loss: 1.5959 - val_acc: 0.2342\n",
      "Inner step: 17 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 8s 281us/step - loss: 1.5951 - acc: 0.2487 - val_loss: 1.5976 - val_acc: 0.2342\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 4s 130us/step - loss: 1.5952 - acc: 0.2468 - val_loss: 1.5935 - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 4s 133us/step - loss: 1.5952 - acc: 0.2459 - val_loss: 1.5951 - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 4s 131us/step - loss: 1.5952 - acc: 0.2481 - val_loss: 1.5932 - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 4s 130us/step - loss: 1.5951 - acc: 0.2468 - val_loss: 1.5948 - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 4s 135us/step - loss: 1.5950 - acc: 0.2472 - val_loss: 1.5970 - val_acc: 0.2478\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 4s 133us/step - loss: 1.5953 - acc: 0.2469 - val_loss: 1.5938 - val_acc: 0.2478\n",
      "Inner step: 18 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 8s 284us/step - loss: 1.5951 - acc: 0.2482 - val_loss: 1.5937 - val_acc: 0.2478\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 4s 140us/step - loss: 1.5950 - acc: 0.2458 - val_loss: 1.5937 - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 4s 149us/step - loss: 1.5950 - acc: 0.2459 - val_loss: 1.5952 - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 4s 140us/step - loss: 1.5948 - acc: 0.2481 - val_loss: 1.5939 - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 8s 279us/step - loss: 1.5953 - acc: 0.2492 - val_loss: 1.5945 - val_acc: 0.2342\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 5s 175us/step - loss: 1.5950 - acc: 0.2474 - val_loss: 1.5933 - val_acc: 0.2478\n",
      "Inner step: 19 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 9s 296us/step - loss: 1.5952 - acc: 0.2467 - val_loss: 1.5952 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 4s 132us/step - loss: 1.5949 - acc: 0.2463 - val_loss: 1.5932 - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 4s 149us/step - loss: 1.5947 - acc: 0.2475 - val_loss: 1.5932 - val_acc: 0.2342\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 4s 151us/step - loss: 1.5950 - acc: 0.2492 - val_loss: 1.5930 - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 4s 145us/step - loss: 1.5951 - acc: 0.2467 - val_loss: 1.5967 - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 4s 141us/step - loss: 1.5951 - acc: 0.2498 - val_loss: 1.5946 - val_acc: 0.2478\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 4s 146us/step - loss: 1.5947 - acc: 0.2484 - val_loss: 1.5957 - val_acc: 0.2478\n",
      "Inner step: 20 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 9s 296us/step - loss: 1.5954 - acc: 0.2476 - val_loss: 1.5958 - val_acc: 0.2342\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 4s 135us/step - loss: 1.5949 - acc: 0.2493 - val_loss: 1.5952 - val_acc: 0.2342\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 4s 146us/step - loss: 1.5951 - acc: 0.2477 - val_loss: 1.5936 - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 5s 166us/step - loss: 1.5953 - acc: 0.2484 - val_loss: 1.5942 - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 6s 209us/step - loss: 1.5948 - acc: 0.2471 - val_loss: 1.5951 - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 5s 179us/step - loss: 1.5950 - acc: 0.2476 - val_loss: 1.5944 - val_acc: 0.2478\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 5s 171us/step - loss: 1.5952 - acc: 0.2478 - val_loss: 1.5931 - val_acc: 0.2478\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 4s 144us/step - loss: 1.5953 - acc: 0.2469 - val_loss: 1.5931 - val_acc: 0.2478\n",
      "Inner step: 21 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 12s 412us/step - loss: 1.5949 - acc: 0.2454 - val_loss: 1.5934 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 4s 144us/step - loss: 1.5948 - acc: 0.2492 - val_loss: 1.5946 - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 4s 150us/step - loss: 1.5951 - acc: 0.2498 - val_loss: 1.5936 - val_acc: 0.2342\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 4s 145us/step - loss: 1.5950 - acc: 0.2450 - val_loss: 1.5935 - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 5s 153us/step - loss: 1.5946 - acc: 0.2488 - val_loss: 1.5955 - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 4s 135us/step - loss: 1.5950 - acc: 0.2463 - val_loss: 1.5934 - val_acc: 0.2478\n",
      "Inner step: 22 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 9s 294us/step - loss: 1.5952 - acc: 0.2483 - val_loss: 1.5931 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 4s 138us/step - loss: 1.5950 - acc: 0.2468 - val_loss: 1.5933 - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 4s 141us/step - loss: 1.5952 - acc: 0.2477 - val_loss: 1.5938 - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 4s 138us/step - loss: 1.5952 - acc: 0.2469 - val_loss: 1.5939 - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 4s 140us/step - loss: 1.5950 - acc: 0.2485 - val_loss: 1.5933 - val_acc: 0.2342\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 4s 142us/step - loss: 1.5947 - acc: 0.2455 - val_loss: 1.5942 - val_acc: 0.2478\n",
      "Inner step: 23 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 9s 297us/step - loss: 1.5955 - acc: 0.2476 - val_loss: 1.5949 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 4s 141us/step - loss: 1.5950 - acc: 0.2482 - val_loss: 1.5931 - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 4s 139us/step - loss: 1.5950 - acc: 0.2502 - val_loss: 1.5947 - val_acc: 0.2342\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 4s 138us/step - loss: 1.5951 - acc: 0.2465 - val_loss: 1.5950 - val_acc: 0.2342\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 4s 138us/step - loss: 1.5949 - acc: 0.2491 - val_loss: 1.5944 - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 4s 137us/step - loss: 1.5953 - acc: 0.2450 - val_loss: 1.5953 - val_acc: 0.2478\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 4s 139us/step - loss: 1.5949 - acc: 0.2485 - val_loss: 1.5946 - val_acc: 0.2478\n",
      "Inner step: 24 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 10s 344us/step - loss: 1.5947 - acc: 0.2487 - val_loss: 1.5941 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 4s 135us/step - loss: 1.5951 - acc: 0.2465 - val_loss: 1.5929 - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 4s 134us/step - loss: 1.5951 - acc: 0.2487 - val_loss: 1.5954 - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 4s 136us/step - loss: 1.5953 - acc: 0.2485 - val_loss: 1.5931 - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 4s 137us/step - loss: 1.5948 - acc: 0.2491 - val_loss: 1.5953 - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 4s 135us/step - loss: 1.5949 - acc: 0.2465 - val_loss: 1.5940 - val_acc: 0.2342\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 4s 136us/step - loss: 1.5949 - acc: 0.2484 - val_loss: 1.5929 - val_acc: 0.2478\n",
      "Inner step: 25 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 9s 299us/step - loss: 1.5952 - acc: 0.2463 - val_loss: 1.5947 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 4s 136us/step - loss: 1.5950 - acc: 0.2475 - val_loss: 1.5954 - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 4s 137us/step - loss: 1.5948 - acc: 0.2465 - val_loss: 1.5949 - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 4s 136us/step - loss: 1.5950 - acc: 0.2482 - val_loss: 1.5955 - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 4s 138us/step - loss: 1.5948 - acc: 0.2492 - val_loss: 1.5951 - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 4s 137us/step - loss: 1.5950 - acc: 0.2457 - val_loss: 1.5951 - val_acc: 0.2478\n",
      "Inner step: 26 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 9s 308us/step - loss: 1.5950 - acc: 0.2485 - val_loss: 1.5933 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 6s 192us/step - loss: 1.5950 - acc: 0.2486 - val_loss: 1.5966 - val_acc: 0.2342\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 8s 279us/step - loss: 1.5949 - acc: 0.2483 - val_loss: 1.5942 - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 6s 194us/step - loss: 1.5950 - acc: 0.2469 - val_loss: 1.5942 - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 5s 169us/step - loss: 1.5950 - acc: 0.2475 - val_loss: 1.5944 - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 5s 181us/step - loss: 1.5955 - acc: 0.2461 - val_loss: 1.5934 - val_acc: 0.2478\n",
      "Inner step: 27 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 12s 401us/step - loss: 1.5954 - acc: 0.2464 - val_loss: 1.5946 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 4s 151us/step - loss: 1.5950 - acc: 0.2464 - val_loss: 1.5947 - val_acc: 0.2342\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 4s 150us/step - loss: 1.5951 - acc: 0.2465 - val_loss: 1.5931 - val_acc: 0.2342\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 4s 150us/step - loss: 1.5944 - acc: 0.2485 - val_loss: 1.5929 - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 5s 152us/step - loss: 1.5949 - acc: 0.2456 - val_loss: 1.5939 - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 5s 166us/step - loss: 1.5950 - acc: 0.2471 - val_loss: 1.5934 - val_acc: 0.2478\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 5s 162us/step - loss: 1.5951 - acc: 0.2496 - val_loss: 1.5953 - val_acc: 0.2478\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 5s 156us/step - loss: 1.5950 - acc: 0.2484 - val_loss: 1.5956 - val_acc: 0.2478\n",
      "Inner step: 28 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 10s 347us/step - loss: 1.5951 - acc: 0.2471 - val_loss: 1.5945 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 5s 153us/step - loss: 1.5949 - acc: 0.2493 - val_loss: 1.5964 - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 5s 153us/step - loss: 1.5948 - acc: 0.2468 - val_loss: 1.5952 - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 5s 166us/step - loss: 1.5955 - acc: 0.2479 - val_loss: 1.5945 - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 5s 169us/step - loss: 1.5948 - acc: 0.2484 - val_loss: 1.5980 - val_acc: 0.2342\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 5s 157us/step - loss: 1.5950 - acc: 0.2481 - val_loss: 1.5939 - val_acc: 0.2342\n",
      "Inner step: 29 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 10s 337us/step - loss: 1.5953 - acc: 0.2491 - val_loss: 1.5928 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 5s 161us/step - loss: 1.5950 - acc: 0.2474 - val_loss: 1.5934 - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 5s 159us/step - loss: 1.5947 - acc: 0.2479 - val_loss: 1.5936 - val_acc: 0.2342\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 5s 175us/step - loss: 1.5950 - acc: 0.2481 - val_loss: 1.5948 - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 5s 162us/step - loss: 1.5951 - acc: 0.2481 - val_loss: 1.5943 - val_acc: 0.2342\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 5s 173us/step - loss: 1.5951 - acc: 0.2468 - val_loss: 1.5928 - val_acc: 0.2478\n",
      "Inner step: 30 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 11s 357us/step - loss: 1.5950 - acc: 0.2477 - val_loss: 1.5954 - val_acc: 0.2342\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 5s 160us/step - loss: 1.5949 - acc: 0.2482 - val_loss: 1.5933 - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 6s 197us/step - loss: 1.5949 - acc: 0.2478 - val_loss: 1.5960 - val_acc: 0.2342\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 6s 218us/step - loss: 1.5947 - acc: 0.2475 - val_loss: 1.5927 - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 6s 210us/step - loss: 1.5949 - acc: 0.2486 - val_loss: 1.5942 - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 7s 239us/step - loss: 1.5953 - acc: 0.2469 - val_loss: 1.5935 - val_acc: 0.2478\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 7s 247us/step - loss: 1.5957 - acc: 0.2465 - val_loss: 1.5951 - val_acc: 0.2478\n",
      "Inner step: 31 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 11s 384us/step - loss: 1.5953 - acc: 0.2452 - val_loss: 1.5944 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 7s 221us/step - loss: 1.5950 - acc: 0.2482 - val_loss: 1.5956 - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 7s 230us/step - loss: 1.5948 - acc: 0.2498 - val_loss: 1.5985 - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 6s 187us/step - loss: 1.5951 - acc: 0.2496 - val_loss: 1.5945 - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 5s 169us/step - loss: 1.5947 - acc: 0.2504 - val_loss: 1.5955 - val_acc: 0.2342\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 6s 200us/step - loss: 1.5953 - acc: 0.2473 - val_loss: 1.5952 - val_acc: 0.2342\n",
      "Inner step: 32 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 12s 403us/step - loss: 1.5953 - acc: 0.2500 - val_loss: 1.5980 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 6s 188us/step - loss: 1.5952 - acc: 0.2469 - val_loss: 1.5939 - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 6s 186us/step - loss: 1.5951 - acc: 0.2462 - val_loss: 1.5951 - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 6s 193us/step - loss: 1.5951 - acc: 0.2463 - val_loss: 1.5930 - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 5s 183us/step - loss: 1.5946 - acc: 0.2513 - val_loss: 1.5942 - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 5s 184us/step - loss: 1.5949 - acc: 0.2478 - val_loss: 1.5951 - val_acc: 0.2478\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 6s 211us/step - loss: 1.5951 - acc: 0.2468 - val_loss: 1.5956 - val_acc: 0.2478\n",
      "Inner step: 33 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 12s 421us/step - loss: 1.5950 - acc: 0.2451 - val_loss: 1.5945 - val_acc: 0.2342\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 8s 269us/step - loss: 1.5947 - acc: 0.2497 - val_loss: 1.5965 - val_acc: 0.2342\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 5s 161us/step - loss: 1.5948 - acc: 0.2484 - val_loss: 1.5933 - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 5s 156us/step - loss: 1.5955 - acc: 0.2477 - val_loss: 1.5946 - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 5s 173us/step - loss: 1.5948 - acc: 0.2468 - val_loss: 1.5934 - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 5s 172us/step - loss: 1.5950 - acc: 0.2476 - val_loss: 1.5969 - val_acc: 0.2342\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 5s 165us/step - loss: 1.5950 - acc: 0.2477 - val_loss: 1.5933 - val_acc: 0.2342\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 5s 159us/step - loss: 1.5948 - acc: 0.2503 - val_loss: 1.5996 - val_acc: 0.2342\n",
      "Inner step: 34 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 10s 350us/step - loss: 1.5955 - acc: 0.2472 - val_loss: 1.5972 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 7s 225us/step - loss: 1.5955 - acc: 0.2471 - val_loss: 1.5933 - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 6s 202us/step - loss: 1.5949 - acc: 0.2466 - val_loss: 1.5946 - val_acc: 0.2342\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 6s 186us/step - loss: 1.5948 - acc: 0.2495 - val_loss: 1.5931 - val_acc: 0.2478\n",
      "Epoch 5/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29639/29639 [==============================] - 5s 166us/step - loss: 1.5949 - acc: 0.2499 - val_loss: 1.5946 - val_acc: 0.2342\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 5s 171us/step - loss: 1.5952 - acc: 0.2488 - val_loss: 1.5965 - val_acc: 0.2478\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 5s 184us/step - loss: 1.5949 - acc: 0.2489 - val_loss: 1.5957 - val_acc: 0.2478\n",
      "Inner step: 35 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 12s 409us/step - loss: 1.5949 - acc: 0.2463 - val_loss: 1.5940 - val_acc: 0.2342\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 7s 251us/step - loss: 1.5949 - acc: 0.2470 - val_loss: 1.5932 - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 8s 282us/step - loss: 1.5950 - acc: 0.2499 - val_loss: 1.5943 - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 6s 193us/step - loss: 1.5951 - acc: 0.2508 - val_loss: 1.5935 - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 6s 191us/step - loss: 1.5952 - acc: 0.2449 - val_loss: 1.5932 - val_acc: 0.2342\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 5s 172us/step - loss: 1.5951 - acc: 0.2491 - val_loss: 1.5953 - val_acc: 0.2478\n",
      "Inner step: 36 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 11s 378us/step - loss: 1.5952 - acc: 0.2470 - val_loss: 1.5949 - val_acc: 0.2342\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 6s 204us/step - loss: 1.5948 - acc: 0.2467 - val_loss: 1.5969 - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 6s 208us/step - loss: 1.5953 - acc: 0.2447 - val_loss: 1.5959 - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 5s 184us/step - loss: 1.5949 - acc: 0.2504 - val_loss: 1.5937 - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 6s 188us/step - loss: 1.5951 - acc: 0.2482 - val_loss: 1.5939 - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 6s 189us/step - loss: 1.5950 - acc: 0.2489 - val_loss: 1.5942 - val_acc: 0.2478\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 6s 188us/step - loss: 1.5950 - acc: 0.2460 - val_loss: 1.5939 - val_acc: 0.2478\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 6s 216us/step - loss: 1.5953 - acc: 0.2441 - val_loss: 1.6002 - val_acc: 0.2478\n",
      "Epoch 9/80\n",
      "29639/29639 [==============================] - 6s 203us/step - loss: 1.5949 - acc: 0.2491 - val_loss: 1.5971 - val_acc: 0.2478\n",
      "Inner step: 37 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 11s 369us/step - loss: 1.5955 - acc: 0.2482 - val_loss: 1.5934 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 5s 161us/step - loss: 1.5949 - acc: 0.2471 - val_loss: 1.5942 - val_acc: 0.2342\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 6s 193us/step - loss: 1.5950 - acc: 0.2496 - val_loss: 1.5970 - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 5s 164us/step - loss: 1.5951 - acc: 0.2467 - val_loss: 1.5953 - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 6s 208us/step - loss: 1.5946 - acc: 0.2459 - val_loss: 1.5942 - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 6s 198us/step - loss: 1.5946 - acc: 0.2511 - val_loss: 1.5965 - val_acc: 0.2478\n",
      "Inner step: 38 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 13s 424us/step - loss: 1.5950 - acc: 0.2495 - val_loss: 1.5944 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 6s 200us/step - loss: 1.5952 - acc: 0.2469 - val_loss: 1.5951 - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 6s 207us/step - loss: 1.5945 - acc: 0.2494 - val_loss: 1.5934 - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 6s 190us/step - loss: 1.5951 - acc: 0.2479 - val_loss: 1.5941 - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 5s 181us/step - loss: 1.5948 - acc: 0.2494 - val_loss: 1.5957 - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 6s 196us/step - loss: 1.5950 - acc: 0.2461 - val_loss: 1.5972 - val_acc: 0.2478\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 7s 243us/step - loss: 1.5950 - acc: 0.2480 - val_loss: 1.5931 - val_acc: 0.2478\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 8s 286us/step - loss: 1.5950 - acc: 0.2463 - val_loss: 1.5935 - val_acc: 0.2478\n",
      "Inner step: 39 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 16s 535us/step - loss: 1.5956 - acc: 0.2486 - val_loss: 1.5942 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 5s 186us/step - loss: 1.5950 - acc: 0.2457 - val_loss: 1.6000 - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 6s 191us/step - loss: 1.5952 - acc: 0.2444 - val_loss: 1.5946 - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 7s 221us/step - loss: 1.5950 - acc: 0.2463 - val_loss: 1.5971 - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 6s 214us/step - loss: 1.5953 - acc: 0.2491 - val_loss: 1.5939 - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 6s 213us/step - loss: 1.5950 - acc: 0.2476 - val_loss: 1.5937 - val_acc: 0.2478\n",
      "Inner step: 40 / 40\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 13s 441us/step - loss: 1.5951 - acc: 0.2475 - val_loss: 1.5935 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 7s 222us/step - loss: 1.5948 - acc: 0.2476 - val_loss: 1.5946 - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 7s 237us/step - loss: 1.5950 - acc: 0.2480 - val_loss: 1.5939 - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 6s 188us/step - loss: 1.5951 - acc: 0.2488 - val_loss: 1.5933 - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 6s 187us/step - loss: 1.5954 - acc: 0.2463 - val_loss: 1.5935 - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 5s 186us/step - loss: 1.5951 - acc: 0.2477 - val_loss: 1.5941 - val_acc: 0.2478\n"
     ]
    }
   ],
   "source": [
    "model_factories = [model_1_hidden_factory, model_2_hidden_factory, model_3_hidden_factory, model_4_hidden_factory]\n",
    "\n",
    "try_per_model = 40\n",
    "\n",
    "results = []\n",
    "\n",
    "for i, factory in enumerate(model_factories):\n",
    "    print('Training model', i + 1, '/', len(model_factories))\n",
    "    \n",
    "    curr_model_res = []\n",
    "    \n",
    "    for i in range(try_per_model):\n",
    "        print('Inner step:', i+1, '/', try_per_model)\n",
    "        \n",
    "        model = factory()\n",
    "        \n",
    "        compile_model(model)\n",
    "        \n",
    "        h = fit_model(model, x_train, y_train, x_test, y_test)\n",
    "        \n",
    "        curr_model_res.append(h)\n",
    "        \n",
    "    results.append(curr_model_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot summary statistics of the best validation loss and accuracy versus the number of hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Not sure about that, val_loss and val_acc in this case are not necessarily linked (not the same epoch)\n",
    "\n",
    "best_results = [[[min(res.history['val_loss']), max(res.history['val_acc'])] for res in model_res] for model_res in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlMAAAF1CAYAAADMXG9eAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nONVJ+BsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x187b554cc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlMAAAF1CAYAAADMXG9eAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAHS5JREFUeJzt3Xm0LGdZL+DfKyfIlDDliEIIBwWy\niIiCAREXggKaEAYVFFCQ8QZ0CYrxQhAUrorGq5cLiohhMBAwEQNoNA6gGAYZwyACYRJCEgjJIQHC\npCTc9/5RtUNnc/bZO/vrk3P2yfOstdfu7qr66u2q6upffVXdXd0dAAA251v2dgEAAFuZMAUAMECY\nAgAYIEwBAAwQpgAABghTAAADhKmriap6QVX9xpLaOrSqvlRV15jvn1FVj11G23N7/1BVj1hWe1di\nvr9TVZ+tqs8sud0Tq+p35tt3q6oPb2TcTc7rS1X1nZudfkRV3aOqztsb874yquoXquqCeVndeG/X\nsztXZnuoqrOr6l6D89vta290+1xn3rtte3fbdlU9sqrevJtpl7qPWmj3mVX18mW3y9YjTO0H5p3o\nV6vqi1X1+ap6S1U9vqouX7/d/fju/u0NtrXbHXJ3n9Pd1+vury+h9m/aGXX3Ud390tG2r2QdN09y\nbJLDu/vb99R8uvtN3X3YMtra1RvEvF4+vqS27zHazr6mqg5I8uwkPzYvq4v2dk37ksXX3noBZT1V\ndZuq+puq2llVF1fVP1XVprf9ZW3bsCcIU/uP+3X3gUlukeT4JE9J8uJlz6Sqti27zX3ELZJc1N0X\n7u1C2KNukuRaST5wZSesiX3mxt0gyWlJDsu03N+R5G/2akX7gf14H7yl2THsZ7r7C919WpIHJ3lE\nVd0u+aZTTQdX1d/NvVgXV9WbqupbquqkJIcm+du5S/3JVbWjqrqqHlNV5yR5/cJjiy/q76qqd1TV\nF+aj0RvN8/qmUz8rvV9VdWSSX0/y4Hl+/z4Pv7zHZa7r6VX1yaq6sKpeVlXXn4et1PGIqjpnPkX3\ntLWWTVVdf55+59ze0+f275XkdUluOtdx4i6mPauq7rtwf9s8vzvO9/+qqj4zP/83VtV3r1HDFZZH\nVd2hqt499yr+ZaY3+pVhN5zX086q+tx8+5B52LOS3C3J8+aanzc/3lV1q90933nYI6vqzVX1h3Pb\nn6iqo9ao+c5VdWZVXVLT6bFnr7WMV01323ldfr6qPlBV918Ydp+q+uD8vD9VVb82P77LbXMedtOq\netX8fD5RVU+8MjVW1W2SrJxi/XxVvX5+/K5V9c553b2zqu66MM0ZVfWsqvq3JF9J8k2nmebt+X9W\n1fuq6stV9eKquklNp8y+WFX/XFU3XBj//vPy+Pzc/m0Xhq25PczD71tV761v9EDffgPr4Zbz+CvL\n8UVVdeHC8JdX1a8sPN/HzjW9IMkPztvX5xeavGFVnT7X+Paq+q5dzbe739HdL+7ui7v70iT/N8lh\ntftTq2u2vWrbvnFVnTav73ckuUINVXXvqvrQvE6fl6RWDX90Ta/pz9XUY3aLVfN5fFV9dB7+J1V1\nhenXUmvsB6rqTvN2uW1h3AdW1Xvn299SVcdV1X9W1UVV9cr6xj50V/vga83r7aJ53b6zqm6ykRrZ\nQ7rb3xb/S3J2knvt4vFzkvzCfPvEJL8z3/69TDvKA+a/uyWpXbWVZEeSTvKyJNdNcu2Fx7bN45yR\n5FNJbjeP86okL5+H3SPJeWvVm+SZK+MuDD8jyWPn249O8rFMb2LXS/LqJCetqu2Fc13fm+S/k9x2\njeX0skxHxgfO034kyWPWqnPVtL+Z5BUL949O8qGF+4+e2/3WJM9J8t6FYYvL/vL5JLlmkk8medK8\nHh6U5NKFcW+c5IFJrjO3/VdJ/npXy2nhsU5yqw0830fO8/ofSa6R5BeSfHplO1jV5luTPHy+fb0k\nd1ljGS0+twPm9fbr8/P80SRfTHLYPPz8JHebb98wyR13t21mOvB717werjlvDx9P8uNXssaVbWZl\n271Rks8leXiSbUkeOt+/8cIyPifJd8/DD1jj9fe2TL0vN0tyYZJ3J7nDvD28Pskz5nFvk+TLSe49\nP78nz8vpmhvYHu44t/0D8zp7xDzvb93dfmBhX/D98+0Pz8vutgvD7rCL194jk7x5VTsnJrk4yZ3n\n5fGKJKdscD/1E0nO383w3badK27bpyR5Zab9ze0y7X/ePA87OMkl8/I7YF6ely08r5+Yl/lt5/k8\nPclbVs3n7zL1rB2aZGeSI9eo+ZlZ2H9l9/uBDyY5auH+a5IcO9/+lXkbOmSe9s+SnLybffDjkvxt\npn3DNZJ8f5KDNrIe/O2ZPz1T+7dPZ3qzWO3SJN+R5BbdfWlP1/Gs9yONz+zuL3f3V9cYflJ3v7+7\nv5zkN5L8TM0XqA/6uSTP7u6Pd/eXkjw1yUPqir1i/6u7v9rd/57k3zOFqiuYa3lwkqd29xe7++wk\n/yfTm+hG/EWS+1fVdeb7Pzs/liTp7pfM7f53ph3s99bcg7Ybd8m0s3/OvB5OTfLOhTYv6u5XdfdX\nuvuLSZ6V5O4bKXaDz/eT3f3Cnq59e2mmbWJXR7eXJrlVVR3c3V/q7rdtoIS7ZAo1x3f317r79Zne\noB660ObhVXVQd3+uu9+98Piuts07Jdne3b81t/fxTCH6IQM1JlMo/mh3n9Tdl3X3yUk+lOR+C+Oc\n2N0fmIdfukY7f9zdF3T3p5K8Kcnbu/s98/bwmkzBKpnWyend/bq5rT/M9OZ416yzPWQKvn/W3W/v\n7q/3dG3Tf8/TrecNSe5eVSvXA546379lkoMyvW426tU99TpdlinwfN96E9TUo/onSX51tO15235g\nkt+c90nvz7T9rrhPkg9296nzMn5OksUPlTwuye9191nzfH43yfct9k5l2m4/393nJPnXjTzHZN39\nwEuTPGx+DjdK8uP5xj7kcUme1t3nLUz7oFX7ucV98KWZDrZuNW8L7+ruSzZSI3uGMLV/u1mmI73V\n/iDTkdlrq+rjVXXcBto690oM/2SmN4WDN1Tl7t10bm+x7W254pv+4o7yK5nexFc7ON848l9s62Yb\nKaK7P5bkrCT3mwPV/TPvCKvqGlV1/NxFf0mmHoKVee7OTZN8alWQvby+qrpOVf1ZTafoLknyxiQ3\n2GBI3cjzvXy5dfdX5pu7WnaPydSj8qH5dMJ9dzHOajdNcm53/7815v/ATG96n6yqN1TVD86Pr7Vt\n3iLTadjPr/xl6vVa2Q42U+NKnZ9c9djq5bTetp8kFyzc/uou7q8s1yvMb14+587z2+32kGkZHLtq\nGdx8nm49b8jUc/jDmbajMzIF87snedOq9bSejbzeLldV25O8Nsnz57A62vb2TPuA1fucFTddHDYv\nz8Vxb5HkuQvL8OJMvZ+7fG3spo4r2MB+4OWZ9h/XS/IzmZb7+Qs1vWahprOSfD1X3M8tPoeTkvxT\nklOq6tNV9b9r+nAFe4kwtZ+qqjtl2jl806dx5iOnY7v7OzMdgf9qVd1zZfAaTa7Xc3XzhduHZjpy\n+mymUxorvTkrR5Xbr0S7n860o1ls+7Jc8c1qIz4717S6rU9diTZOztSz8oBMR74fmx//2fmxeyW5\nfqZu+WTVdRq7cH6Sm626HuPQhdvHZrp49we6+6BMb4SL7e5u2S3j+U4z6f5odz80ybcl+f0kp1bV\nddeZ7NNJbl5XvGD78vl39zu7+wFzm3+d6ZTN7rbNc5N8ortvsPB3YHffZ6DGlTpvseqx1ctpvW30\nyrjC/OZ1f/N5futtD+cmedaqZXCdDQSUZApTd8sUqN6Qab/wQ5nC1BvWmGb4edd0rdhrk5zW3c8a\nbW+2M9M+YPU+Z8X5i8MWlvGKc5M8btVyvHZ3v2Wwrt3uB+Zey7cm+clMPcQnrarpqFU1XWueZsXl\n62Puufxf3X14pl7N+yb5+cH6GSBM7Weq6qD5qPyUTOfy/2MX49y3qm4172QuyXQEtPI1BxdkFxfZ\nbsDDqurwudfmt5KcOp8++kiSa1XV0fOR09MzXROw4oIkO2rtT0mdnORJNV1Ee71MXfJ/OXfPb9hc\nyyuTPKuqDpy79H8109HiRp2S5McyXV/0FwuPH5jpdMtFmYLj726wvbdmelN4Yk0XtP9UputFFtv9\naqaLpW+U5Bmrpl9zXS3p+SZJquphVbV97r1YuRB5va/FeHumIP3kqjqgpq9ZuF+mI+lrVtXPVdX1\n59MwK9vg7rbNdyS5pKqeUlXXnnsBbjcfNGy2xiT5+yS3qaqfndfBg5McnumU5J7wyiRHV9U959fD\nsZm2nbdk/e3hhUkeX1U/UJPrzq+rA9ebaXd/NNO29LAkb5xPCV2QqYdwrTB1QZJDquqam3miVXVQ\npt6Tf+vujfR+b8i8bb86yTPn3tvDM10/tuL0JN9dVT81nyZ7YpLFrzt5QZKn1jcuDr9+Vf30Ekrb\nyH7gZZmuk/ueTKd/F2t61sqpxqraXlUPWGtGVfUjVfU988HpJZkOnIa/qobNE6b2H39bVV/MdITz\ntEzfpfOoNca9dZJ/TvKlTDvw53f3GfOw30vy9Lm7+deuxPxPynQB6WcyfQLpicn06cIkv5jkRZmO\nvr+cZPHTfX81/7+oqt6db/aSue03JvlEkv9K8oQrUdeiJ8zz/3imI/O/mNvfkLlL/q2ZjgT/cmHQ\nyzKdZvhUpotMN3S9Tnd/LclPZbrQ93OZrqd59cIoz8l0Pc1n5zb/cVUTz810XcXnquqPdjGLoee7\n4MgkH6iqL83zfEh3/9fuJpif2/2THDXX//wkP9/dH5pHeXiSs+fTIY/PfC1J1tg25zfQ+2W6duUT\nc5svytQDsKka5zovynRUf2ymN8EnJ7lvd392vWk3o7s/nOm5/nGm53C/TF9r8rX1tofuPjPTdVPP\nm4d/bB53o96Q6es/zlm4X0nes8b4r8/0FRKfqarNLI+fzHSt26Nq+kTgyt+h6024Ab+U6dTbZzLt\nd/58ZcC87n4601fEXJRpm/q3heGvydR7ecq8/b0/03Y6aiP7gddkPqXX0/WlK56b6WskXjvvx9+W\n6YMGa/n2TNe9XZLplOAbsokDJZZn5RNcAMAeVlX/mek04z/v7VpYHj1TAHAVqKoHZrr26fV7uxaW\nyzepAsAeVlVnZLoe7+FX8tOTbAFO8wEADHCaDwBggDAFADDgKr1m6uCDD+4dO3ZclbMEANiUd73r\nXZ/t7u3rjXeVhqkdO3bkzDPPvCpnCQCwKVW1+uemdslpPgCAAcIUAMAAYQoAYIAwBQAwQJgCABgg\nTAEADBCmAAAGCFMAAAOEKQCAAcIUAMAAYQoAYIAwBQAwQJgCABiwbW8XcHW047jTl9re2ccfvdT2\nAICN0zMFADBAmAIAGCBMAQAMEKYAAAYIUwAAA4QpAIABwhQAwABhCgBggDAFADBAmAIAGCBMAQAM\nEKYAAAYIUwAAA4QpAIABwhQAwABhCgBggDAFADBAmAIAGCBMAQAMEKYAAAasG6aq6iVVdWFVvX/h\nsT+oqg9V1fuq6jVVdYM9WyYAwL5pIz1TJyY5ctVjr0tyu+6+fZKPJHnqkusCANgS1g1T3f3GJBev\neuy13X3ZfPdtSQ7ZA7UBAOzzlnHN1KOT/MNaA6vqmKo6s6rO3Llz5xJmBwCw7xgKU1X1tCSXJXnF\nWuN09wndfUR3H7F9+/aR2QEA7HO2bXbCqnpEkvsmuWd39/JKAgDYOjYVpqrqyCRPSXL37v7KcksC\nANg6NvLVCCcneWuSw6rqvKp6TJLnJTkwyeuq6r1V9YI9XCcAwD5p3Z6p7n7oLh5+8R6oBQBgy/EN\n6AAAA4QpAIABwhQAwABhCgBggDAFADBAmAIAGCBMAQAMEKYAAAYIUwAAA4QpAIABwhQAwABhCgBg\ngDAFADBAmAIAGCBMAQAMEKYAAAYIUwAAA4QpAIABwhQAwABhCgBggDAFADBAmAIAGCBMAQAMEKYA\nAAYIUwAAA4QpAIABwhQAwABhCgBggDAFADBAmAIAGCBMAQAMEKYAAAYIUwAAA4QpAIABwhQAwABh\nCgBggDAFADBAmAIAGLBumKqql1TVhVX1/oXHblRVr6uqj87/b7hnywQA2DdtpGfqxCRHrnrsuCT/\n0t23TvIv830AgKuddcNUd78xycWrHn5AkpfOt1+a5CeWXBcAwJaw2WumbtLd5yfJ/P/bllcSAMDW\nsccvQK+qY6rqzKo6c+fOnXt6dgAAV6nNhqkLquo7kmT+f+FaI3b3Cd19RHcfsX379k3ODgBg37TZ\nMHVakkfMtx+R5G+WUw4AwNayka9GODnJW5McVlXnVdVjkhyf5N5V9dEk957vAwBc7Wxbb4Tufuga\ng+655FoAALYc34AOADBAmAIAGCBMAQAMEKYAAAYIUwAAA4QpAIABwhQAwABhCgBggDAFADBAmAIA\nGCBMAQAMEKYAAAYIUwAAA4QpAIABwhQAwABhCgBggDAFADBAmAIAGCBMAQAMEKYAAAYIUwAAA4Qp\nAIABwhQAwABhCgBggDAFADBAmAIAGCBMAQAMEKYAAAYIUwAAA4QpAIABwhQAwABhCgBggDAFADBA\nmAIAGCBMAQAMEKYAAAYIUwAAA4QpAIABwhQAwIChMFVVT6qqD1TV+6vq5Kq61rIKAwDYCjYdpqrq\nZkmemOSI7r5dkmskeciyCgMA2ApGT/NtS3LtqtqW5DpJPj1eEgDA1rHpMNXdn0ryh0nOSXJ+ki90\n92uXVRgAwFYwcprvhkkekOSWSW6a5LpV9bBdjHdMVZ1ZVWfu3Llz85UCAOyDRk7z3SvJJ7p7Z3df\nmuTVSe66eqTuPqG7j+juI7Zv3z4wOwCAfc9ImDonyV2q6jpVVUnumeSs5ZQFALA1jFwz9fYkpyZ5\nd5L/mNs6YUl1AQBsCdtGJu7uZyR5xpJqAQDYcnwDOgDAAGEKAGCAMAUAMECYAgAYIEwBAAwQpgAA\nBghTAAADhCkAgAHCFADAAGEKAGCAMAUAMECYAgAYIEwBAAwQpgAABghTAAADhCkAgAHCFADAAGEK\nAGCAMAUAMGDb3i6AcTuOO32p7Z19/NFLbQ8A9md6pgAABghTAAADhCkAgAHCFADAAGEKAGCAMAUA\nMECYAgAYIEwBAAwQpgAABghTAAADhCkAgAHCFADAAGEKAGCAMAUAMECYAgAYIEwBAAwQpgAABghT\nAAADhCkAgAFDYaqqblBVp1bVh6rqrKr6wWUVBgCwFWwbnP65Sf6xux9UVddMcp0l1AQAsGVsOkxV\n1UFJfjjJI5Oku7+W5GvLKQsAYGsYOc33nUl2JvnzqnpPVb2oqq67eqSqOqaqzqyqM3fu3DkwOwCA\nfc9ImNqW5I5J/rS775Dky0mOWz1Sd5/Q3Ud09xHbt28fmB0AwL5nJEydl+S87n77fP/UTOEKAOBq\nY9Nhqrs/k+TcqjpsfuieST64lKoAALaI0U/zPSHJK+ZP8n08yaPGSwIA2DqGwlR3vzfJEUuqBQBg\ny/EN6AAAA4QpAIABwhQAwABhCgBggDAFADBAmAIAGCBMAQAMEKYAAAYIUwAAA4QpAIABwhQAwABh\nCgBggDAFADBAmAIAGCBMAQAMEKYAAAYIUwAAA4QpAIAB2/Z2AQDA/m3Hcacvvc2zjz966W1ulp4p\nAIABwhQAwABhCgBggDAFADBAmAIAGCBMAQAMEKYAAAYIUwAAA4QpAIABwhQAwABhCgBggDAFADBA\nmAIAGCBMAQAMEKYAAAYIUwAAA4QpAIABwhQAwABhCgBggDAFADBgOExV1TWq6j1V9XfLKAgAYCtZ\nRs/ULyc5awntAABsOUNhqqoOSXJ0khctpxwAgK1l2+D0z0ny5CQHrjVCVR2T5JgkOfTQQwdnt3fs\nOO70vV0CALCP2nTPVFXdN8mF3f2u3Y3X3Sd09xHdfcT27ds3OzsAgH3SyGm+H0py/6o6O8kpSX60\nql6+lKoAALaITYep7n5qdx/S3TuSPCTJ67v7YUurDABgC/A9UwAAA0YvQE+SdPcZSc5YRlsAAFuJ\nnikAgAHCFADAAGEKAGCAMAUAMECYAgAYIEwBAAwQpgAABghTAAADhCkAgAHCFADAAGEKAGCAMAUA\nMECYAgAYIEwBAAwQpgAABghTAAADhCkAgAHCFADAgG17uwCAq8qO405fantnH3/0UtsDtiY9UwAA\nA4QpAIABwhQAwABhCgBggDAFADBAmAIAGCBMAQAMEKYAAAYIUwAAA4QpAIABwhQAwABhCgBggDAF\nADBAmAIAGCBMAQAMEKYAAAYIUwAAA4QpAIABwhQAwABhCgBgwKbDVFXdvKr+tarOqqoPVNUvL7Mw\nAICtYNvAtJclOba7311VByZ5V1W9rrs/uKTaAAD2eZvumeru87v73fPtLyY5K8nNllUYAMBWMNIz\ndbmq2pHkDknevothxyQ5JkkOPfTQZcwOlm7Hcacvvc2zjz966W0C61v269lrmfUMX4BeVddL8qok\nv9Ldl6we3t0ndPcR3X3E9u3bR2cHALBPGQpTVXVApiD1iu5+9XJKAgDYOkY+zVdJXpzkrO5+9vJK\nAgDYOkZ6pn4oycOT/GhVvXf+u8+S6gIA2BI2fQF6d785SS2xFgCALcc3oAMADBCmAAAGCFMAAAOE\nKQCAAcIUAMAAYQoAYIAwBQAwQJgCABggTAEADBCmAAAGCFMAAAOEKQCAAcIUAMAAYQoAYIAwBQAw\nQJgCABggTAEADBCmAAAGbNvbBSzbjuNO39slbHl7YhmeffzRS28T9javlauHZa9n63j/o2cKAGCA\nMAUAMECYAgAYIEwBAAwQpgAABghTAAADhCkAgAHCFADAAGEKAGCAMAUAMECYAgAYIEwBAAwQpgAA\nBghTAAADhCkAgAHCFADAAGEKAGCAMAUAMECYAgAYMBSmqurIqvpwVX2sqo5bVlEAAFvFpsNUVV0j\nyZ8kOSrJ4UkeWlWHL6swAICtYKRn6s5JPtbdH+/uryU5JckDllMWAMDWMBKmbpbk3IX7582PAQBc\nbVR3b27Cqp9O8uPd/dj5/sOT3Lm7n7BqvGOSHDPfPSzJhzdfLhtwcJLP7u0iuEpY11cP1vPVh3W9\n77lFd29fb6RtAzM4L8nNF+4fkuTTq0fq7hOSnDAwH66Eqjqzu4/Y23Ww51nXVw/W89WHdb11jZzm\ne2eSW1fVLavqmkkekuS05ZQFALA1bLpnqrsvq6pfSvJPSa6R5CXd/YGlVQYAsAWMnOZLd/99kr9f\nUi0sh1OqVx/W9dWD9Xz1YV1vUZu+AB0AAD8nAwAwRJjaotb7KZ+qOrSq/rWq3lNV76uq++yNOhlT\nVS+pqgur6v1rDK+q+qN5O3hfVd3xqq6R5djAuv65eR2/r6reUlXfe1XXyLj11vPCeHeqqq9X1YOu\nqtrYPGFqC9rgT/k8Pckru/sOmT5p+fyrtkqW5MQkR+5m+FFJbj3/HZPkT6+CmtgzTszu1/Unkty9\nu2+f5Lfj+pqt6sTsfj2v7ON/P9MHvNgChKmtaSM/5dNJDppvXz+7+A4w9n3d/cYkF+9mlAckeVlP\n3pbkBlX1HVdNdSzTeuu6u9/S3Z+b774t03f7scVs4DWdJE9I8qokF+75ilgGYWpr2shP+TwzycOq\n6rxMn7h8Qtgf+Vmnq6fHJPmHvV0Ey1dVN0vyk0lesLdrYeOEqa2pdvHY6o9lPjTJid19SJL7JDmp\nqqzv/c9GtgX2I1X1I5nC1FP2di3sEc9J8pTu/vreLoSNG/qeKfaajfyUz2Myn5fv7rdW1bUy/e6T\nbuP9y4Z+1on9Q1XdPsmLkhzV3Rft7XrYI45IckpVJdM++z5VdVl3//XeLYvd0VOxNW3kp3zOSXLP\nJKmq2ya5VpKdV2mVXBVOS/Lz86f67pLkC919/t4uiuWrqkOTvDrJw7v7I3u7HvaM7r5ld+/o7h1J\nTk3yi4LUvk/P1Ba01k/5VNVvJTmzu09LcmySF1bVkzKd9nlk+4bWLaeqTk5yjyQHz9e/PSPJAUnS\n3S/IdD3cfZJ8LMlXkjxq71TKqA2s699McuMkz597LS7zo7hbzwbWM1uQb0AHABjgNB8AwABhCgBg\ngDAFADBAmAIAGCBMAQAMEKYAAAYIUwAAA4QpAIAB/x+j6f9m7nfcOwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x187b547da0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlMAAAF1CAYAAADMXG9eAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nF8KG3AsmHv4bAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x187b830240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAl0AAAF1CAYAAADSlV/tAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x187ba1a518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i, model_results in enumerate(best_results):\n",
    "    val_loss, val_acc = list(zip(*model_results))\n",
    "    plt.hist(val_loss, bins=25)\n",
    "    plt.title(\"Distribution of validation's losses for model with \" + str(i+1) + \" hidden layers\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlMAAAF1CAYAAADMXG9eAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x187bdb5588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlMAAAF1CAYAAADMXG9eAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x187c23ff28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlMAAAF1CAYAAADMXG9eAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x187c3b7ef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlMAAAF1CAYAAADMXG9eAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzt3XmYLGV99vHvLQdEBWU5o7IfFzQS\nEzHvCfGNGhEwwR2jRkk0GEnQqIlGEjVq3mCiCSYqmitqJKKAG6LRgFsUF1wiiweDC2BEcQHZDjuo\nMaK/9496Bpph5kzPPDPn9HC+n+uaa7qqnq76dVVX9d1PVXenqpAkSdLi3G5TFyBJkrSSGaYkSZI6\nGKYkSZI6GKYkSZI6GKYkSZI6GKYkSZI6GKZuA5L8S5K/WqJ57Z7khiRbtOFTk/zhUsy7ze9jSQ5Z\nqvktYLmvTHJFkkuXeL7HJnllu/3QJP89TttFLuuGJPdc7P01niRPSHJhW98P3NT1bEiSI5K8c8y2\n3fvyfMeahdSziGVvcP/Z0P6R5BlJvrCB+y7pcW5kvsu2PjRZDFMTLsl3k/w4yfVJrknyxSTPTnLT\ntquqZ1fV3445rwM21Kaqvl9V21TVz5ag9lsdSKrqkVV1XO+8F1jHbsDhwF5VdfflWk5Vfb6q7rsU\n85rt4N62ywVLNO99e+dzG/Ya4Hltff/Xpi5mkowea5Lsm+SipZhvkoclqZ43G0u1f0iLYZhaGR5b\nVdsCewBHAi8GjlnqhSRZtdTznBB7AFdW1eWbuhAtzkZ+bu4BnLOYO0736Gp8SbYE3gCcsalruS24\nDR/HJ5phagWpqmur6mTgKcAhSe4PtzrVtDrJh1sv1lVJPp/kdkneAewOfKh1h78oyZr2bvDQJN8H\nPj0ybnSHvFeSM5Ncm+SkJDu0Zd3qnel071eSA4GXAk9py/tKm35Tj0ur6+VJvpfk8iTHJ7lLmzZd\nxyFJvt9O0b1srnWT5C7t/uvb/F7e5n8AcAqwc6vj2Fnue16Sx4wMr2rL+5U2/L4kl7bH/7kkvzhH\nDbdYH0kemOTLrVfxvcDWI9O2b9tpfZKr2+1d27RXAQ8F/rnV/M9tfCW594Yeb5v2jCRfSPKaNu/v\nJHnkHDXvk2RdkuuSXJbkdXO0m7PeNn2HJG9PcnGb/u8j0x6f5Oy2jG+358atekoz0pM523Nzvm2R\n5A5JXtvWx7VtHdwhyUeS/MmMx/PVJAfNGHf7JDcAWwBfSfLtNv5+7Xl7TZJzkjxu5D7HJnlzko8m\n+SHw8FnW3akZTjN/sW3PDyXZMcm72jr5UpI1I+1/vY27tv3/9ZFp90jy2facOgVYPWNZD2rLuSbJ\nVzJGD2SSrTP0fq9uwy9PcmOSO7fhVyZ5/cjjfWWSOwEf4+b96oYkO7dZbtWem9e39bV2nhIOBz4B\nfGO+WoHt2/a8PskZSe418jhG948dk5zc1u+ZwL1GZ5LkEUm+0dbxPwOZMf2ZGY4LVyf5eJI9Zizn\n2UnOb9PfmOQW95/LXM/fJL+aYf9bNdL2iUnObrdvl+Qlbf+5MsmJufk4PNtxfOsk72xtr2nPo7uN\nU6MWqar8m+A/4LvAAbOM/z7wx+32scAr2+2/B/4F2LL9PRTIbPMC1gAFHA/cCbjDyLhVrc2pwA+A\n+7c2/wa8s03bF7hornqBI6bbjkw/FfjDdvuZwLeAewLbAB8A3jGjtn9tdT0A+AlwvznW0/HAScC2\n7b7fBA6dq84Z9/1/wLtGhh8NfGNk+JltvrcHXg+cPTJtdN3ftBxgK+B7wJ+17fAk4KcjbXcEngjc\nsc37fcC/z7aeRsYVcO8xHu8z2rL+iCEY/DFw8fTzYMY8TwOe3m5vAzxojnU0X70fAd4LbN8e78Pa\n+H2Aa4FHMLx52wX4hTmejzc9X5jluTnGtnhjW2+7tMf9663d7wBnjLR7AHAlsNUcj3V0PW/J8Bx9\nadum+wHXA/cd2f7XAg9uj2/rWeZ3apvHvYC7AOe27XUAsKo9xre3tjsAVwNPb9MObsM7jmyv17XH\n9Rutlul1tkt7XI9qtTyiDU/N9ZwaqfFzwBPb7U8A3wYeOTLtCRt6vs/Yhv/TatiC4Xh0+gb2vT3a\nuthmdN5ztD0WuIrhObUKeBdwwhzb7QTgxPbcuT/DMewLbdpq4DqGfXJLhn30Rm4+Lh3Uttf92nJe\nDnxxxnI+DGzH8AZ1PXDgHDUfwcgxkA0/f8+dXudt+IPA4e32C4DTgV3bfd8CvGcDx/FnAR9i2F+3\nAP4PcOe51q1//X+bvAD/5tlAc4ep04GXtdujB7i/YXiRvfd88xrZCe85y7jRMHXkyPS9gP9tO+i+\n9IWpTwHPGZl2X4YQsGqkjl1Hpp8JPHWWx7UFQ9Daa2Tcs4BT2+1b1Tnj/vdmeFG6Yxt+F/D/5mi7\nXavrLrOs+5uWw/BCd4sAA3yROV4sgL2Bq2dbTyPjqtU63+N9BvCtkWl3bPe9+yzL/RzwCmD1Ap+X\nN9UL7AT8HNh+lnZvAY4a57nN7GHqnhuo4aZtwRAefgw8YJZ2t2d4Ed6zDb8GeNMG5jv6ovxQ4FLg\ndiPT3wMcMbL9j59nXZ1K21fb8GuBj40MP5b2osoQos6ccf/T2jbdneFF/04j0949ss5eTHszMjL9\n48Ahcz2nRtr9LfBPDPvepcDzGS4p2Lqt19Uber7P2IafHBneC/jxBtbNScBTZs57jrbHAm8dGX4U\nt3zTM7p//JQW2tu0v+PmMPX7jAQ8hl6pi7j5uPQx2huTNnw74EfAHiPLecjI9BOBl8xR8xHMOAbO\n9vwd2X7vard3aMvcqQ2fB+w/ct+duPWxcvQ4/kyG480vL2S/9m/xf57mW7l2YXiBmOkfGd5VfSLJ\nBUleMsa8LlzA9O8xvJtbPUfbhdi5zW903quA0e7o0U/f/YjhHexMq7m5J2h0XruMU0RVfYvhYPXY\nJHcEHsfwIkWSLZIc2brXr2MIANPL3JCdgR9UO7KN1ESb7x2TvKWdkrqOIdRsl/GuuRnn8d603qrq\nR+3mbOvuUOA+wDfaqYDHzNJmvnp3A66qqqtnuetuDL0ci3XTc2+ebbGa4YX/Vsuqqp8wvOA9LcOp\n0IOBd4y5/J2BC6vq5yPjZq7r+fYfgMtGbv94luHpbTNznxhd3s4MAfaHM6ZN2wN4cjutc02Sa4CH\nMLzwzuezDOHoV4CvMZwafxjwIIZgfsUY85g2c5/dOrNcx5PkscC2VfXejnnP9pyeYjiOzDxuTdt5\ndFrbR0fb7gG8YWQdXsUQuGbdvzZQxy2McSx5J8MxaBuG3tTPV9UlIzV9cKSm84Cfcctj5ehjeAdD\nkD4hw6n3f8hwbZqWiWFqBUryqww79q0+6ltV11fV4VV1T4Z3vC9Msv/05DlmOdf4abuN3N6d4R3R\nFcAPGXo9puvaguFANu58L2Y4SIzO+0Zu+UIzjitaTTPn9YMFzOM9DC+yjwfObQEL4HfbuAMYekDW\ntPHzXSNxCbDLjGspdh+5fThDT9yvVdWdGXqyRue7oXW3FI93WEjV+VV1MHBX4NXA+9v1MDNtqN4L\ngR2SbDfL/S5kxvUqI27x/AFm+6Tl6HrY0La4guH00lzLOg74PWB/4EdVddoc7Wa6GNgtI5+e5dbr\ner7n+ULM3CdGl3cJwzVDd5oxbdqFDD1T24383amqjhxjuV9k2L5PAD5bVee2eT+aIWjNpvdx7w+s\nbdcQXcpwLegLkpzUOd/1DMeRmcetaZeMTmv76GjbC4FnzViPd6iqL3bWtcFjSVX9gKEX8gkMPZSj\ngf9ChlOAozVt3e4z7abtUVU/rapXVNVeDKe7H8PQI6dlYphaQZLcufUcnMDQdfy1Wdo8Jsm92wHi\nOoZ3L9Nfc3AZw/VJC/W0JHu1Xpu/Ad5fw1cnfJPhXeej27uelzOcUpl2GbBmxgvRqPcAf5bhotpt\nGLri31tVNy6kuFbLicCrkmzbLhZ9IcM7vXGdAPwmw/VF7x4Zvy3DKbUrGV74/27M+Z3GcED/0wwX\ntP82w7Ueo/P9MXBNu5D0r2fcf85ttUSPF4AkT0sy1XpermmjZ/tajDnrbe+ePwa8KcOF6lsmmQ5b\nxwB/kGT/dhHtLkl+oU07G3hqa7+W4RqWDZlzW7T63wa8LsnOrRfg/ya5fZt+GsOpyNcyfq8UDJ8w\n+yHwolbnvgxvUk5YwDwW4qPAfZL8bnvePIXhVNmHq+p7wDrgFUm2SvKQVsu06Z6N32qPf+sMH4rY\n9daLuaXWe3kW8FxuDk9fZDh9PFeYugzYMe1DI4vwVwy9onu3v5MZrpH8g0XOD7hp//gAcETrUd0L\nOGSkyUeAX0zy263H7E+5ZZD/F+Avc/PF4XdJ8uSemppxjiXHAy8CfonhmqnRml7V9nWSTCV5/FwL\nSvLwJL/U3uBex/Dmq/vrbjQ3w9TK8KEk1zO8O3kZwwWocx1w9gQ+CdzA8IL+pqo6tU37e+Dlrav4\nzxew/HcwXK9wKcOplD+F4dOFwHOAtzK8c/4hw7UH097X/l+Z5MuzzPdtbd6fA77D0LPwJ7O0G8ef\ntOVfwNBj9+42/7G0QHAaw7u40dMOxzOcIvgBwwWip485v/8FfpvhWperGd51f2CkyesZLhS9os3z\nP2bM4g3AkzJ8WuifZllE1+MdcSBwToZPsb2B4Zq0/5ml3Xz1Pp3hgP0N4HKGC2apqjMZnqtHMVyo\n/Vlu7nn5K4aepKsZrtt6Nxs237b4c4ZTVF9iODXzam55jDue4UVq7NDZtuPjgEcyPPY3Ab9fVeN8\n8mzBqupKhl6EwxledF8EPGbkNNvvAr/G8Pj+muExTd/3Qoaej5cy9M5cCPwF4x/nP8twCv/MkeFt\nGfbP2Wr9BsMbogvaMWXn2drNpfWiXzr9xxDWf1hVs12+sFDPYzj1dinDsevtI8u9AngywzVhVzIc\nM/9zZPoHGZ47J7TTcV9n2P69xjmWfJB2Sm/G6dw3MITNT7TXgtMZngdzuTvwfoYgdR7DtvTLQ5fR\n9Ke8JOk2LcnvA4dV1UM2dS3SXDJ8JcezquqTm7oWjc+eKUm3ee0U9XOAozd1LdJckjyR4dqnT2/q\nWrQwhilJt2lJfovhtNdlzH8qUdokkpwKvBl47oxPj2oF8DSfJElSB3umJEmSOhimJEmSOmzUX5de\nvXp1rVmzZmMuUpIkaVHOOuusK6pqar52GzVMrVmzhnXr1m3MRUqSJC1Kkpk/7zQrT/NJkiR1MExJ\nkiR1MExJkiR1MExJkiR1MExJkiR1MExJkiR1MExJkiR1MExJkiR1MExJkiR1MExJkiR1GDtMJdki\nyX8l+XAbvkeSM5Kcn+S9SbZavjIlSZIm00J6pp4PnDcy/GrgqKraE7gaOHQpC5MkSVoJxgpTSXYF\nHg28tQ0H2A94f2tyHHDQchQoSZI0yVaN2e71wIuAbdvwjsA1VXVjG74I2GW2OyY5DDgMYPfdd198\npZI2O2te8pElnd93j3z0ks5PkmCMnqkkjwEur6qzRkfP0rRmu39VHV1Va6tq7dTU1CLLlCRJmkzj\n9Ew9GHhckkcBWwN3Zuip2i7JqtY7tStw8fKVKUmSNJnm7Zmqqr+sql2rag3wVODTVfV7wGeAJ7Vm\nhwAnLVuVkiRJE6rne6ZeDLwwybcYrqE6ZmlKkiRJWjnGvQAdgKo6FTi13b4A2GfpS5IkSVo5/AZ0\nSZKkDoYpSZKkDoYpSZKkDoYpSZKkDoYpSZKkDoYpSZKkDoYpSZKkDoYpSZKkDoYpSZKkDoYpSZKk\nDoYpSZKkDoYpSZKkDoYpSZKkDoYpSZKkDoYpSZKkDoYpSZKkDoYpSZKkDoYpSZKkDoYpSZKkDoYp\nSZKkDoYpSZKkDoYpSZKkDoYpSZKkDoYpSZKkDoYpSZKkDoYpSZKkDoYpSZKkDoYpSZKkDoYpSZKk\nDoYpSZKkDvOGqSRbJzkzyVeSnJPkFW38sUm+k+Ts9rf38pcrSZI0WVaN0eYnwH5VdUOSLYEvJPlY\nm/YXVfX+5StPkiRpss0bpqqqgBva4Jbtr5azKEmSpJVirGumkmyR5GzgcuCUqjqjTXpVkq8mOSrJ\n7ee472FJ1iVZt379+iUqW5IkaTKMFaaq6mdVtTewK7BPkvsDfwn8AvCrwA7Ai+e479FVtbaq1k5N\nTS1R2ZIkSZNhQZ/mq6prgFOBA6vqkhr8BHg7sM8y1CdJkjTRxvk031SS7drtOwAHAN9IslMbF+Ag\n4OvLWagkSdIkGufTfDsBxyXZgiF8nVhVH07y6SRTQICzgWcvY52SJEkTaZxP830VeOAs4/dblook\nSZJWEL8BXZIkqYNhSpIkqYNhSpIkqYNhSpIkqYNhSpIkqYNhSpIkqYNhSpIkqYNhSpIkqYNhSpIk\nqYNhSpIkqYNhSpIkqYNhSpIkqYNhSpIkqYNhSpIkqYNhSpIkqYNhSpIkqYNhSpIkqYNhSpIkqYNh\nSpIkqYNhSpIkqYNhSpIkqYNhSpIkqYNhSpIkqYNhSpIkqYNhSpIkqYNhSpIkqYNhSpIkqYNhSpIk\nqYNhSpIkqcO8YSrJ1knOTPKVJOckeUUbf48kZyQ5P8l7k2y1/OVKkiRNlnF6pn4C7FdVDwD2Bg5M\n8iDg1cBRVbUncDVw6PKVKUmSNJnmDVM1uKENbtn+CtgPeH8bfxxw0LJUKEmSNMHGumYqyRZJzgYu\nB04Bvg1cU1U3tiYXAbssT4mSJEmTa6wwVVU/q6q9gV2BfYD7zdZstvsmOSzJuiTr1q9fv/hKJUmS\nJtCCPs1XVdcApwIPArZLsqpN2hW4eI77HF1Va6tq7dTUVE+tkiRJE2ecT/NNJdmu3b4DcABwHvAZ\n4Emt2SHASctVpCRJ0qRaNX8TdgKOS7IFQ/g6sao+nORc4IQkrwT+CzhmGeuUJEmaSPOGqar6KvDA\nWcZfwHD9lCRJ0mbLb0CXJEnqYJiSJEnqYJiSJEnqYJiSJEnqYJiSJEnqYJiSJEnqYJiSJEnqYJiS\nJEnqYJiSJEnqYJiSJEnqYJiSJEnqYJiSJEnqYJiSJEnqYJiSJEnqYJiSJEnqYJiSJEnqYJiSJEnq\nYJiSJEnqYJiSJEnqYJiSJEnqYJiSJEnqYJiSJEnqYJiSJEnqYJiSJEnqYJiSJEnqYJiSJEnqYJiS\nJEnqYJiSJEnqYJiSJEnqYJiSJEnqMG+YSrJbks8kOS/JOUme38YfkeQHSc5uf49a/nIlSZImy6ox\n2twIHF5VX06yLXBWklPatKOq6jXLV54kSdJkmzdMVdUlwCXt9vVJzgN2We7CJEmSVoIFXTOVZA3w\nQOCMNup5Sb6a5G1Jtl/i2iRJkibe2GEqyTbAvwEvqKrrgDcD9wL2Zui5eu0c9zssybok69avX78E\nJUuSJE2OscJUki0ZgtS7quoDAFV1WVX9rKp+DvwrsM9s962qo6tqbVWtnZqaWqq6JUmSJsI4n+YL\ncAxwXlW9bmT8TiPNngB8fenLkyRJmmzjfJrvwcDTga8lObuNeylwcJK9gQK+CzxrWSqUJEmaYON8\nmu8LQGaZ9NGlL0eSJGll8RvQJUmSOhimJEmSOhimJEmSOhimJEmSOhimJEmSOhimJEmSOhimJEmS\nOhimJEmSOhimJEmSOhimJEmSOhimJEmSOhimJEmSOhimJEmSOhimJEmSOhimJEmSOhimJEmSOhim\nJEmSOhimJEmSOhimJEmSOhimJEmSOhimJEmSOhimJEmSOhimJEmSOhimJEmSOhimJEmSOhimJEmS\nOhimJEmSOhimJEmSOhimJEmSOhimJEmSOswbppLsluQzSc5Lck6S57fxOyQ5Jcn57f/2y1+uJEnS\nZBmnZ+pG4PCquh/wIOC5SfYCXgJ8qqr2BD7VhiVJkjYr84apqrqkqr7cbl8PnAfsAjweOK41Ow44\naLmKlCRJmlQLumYqyRrggcAZwN2q6hIYAhdw1znuc1iSdUnWrV+/vq9aSZKkCTN2mEqyDfBvwAuq\n6rpx71dVR1fV2qpaOzU1tZgaJUmSJtZYYSrJlgxB6l1V9YE2+rIkO7XpOwGXL0+JkiRJk2ucT/MF\nOAY4r6peNzLpZOCQdvsQ4KSlL0+SJGmyrRqjzYOBpwNfS3J2G/dS4EjgxCSHAt8Hnrw8JUqSJE2u\necNUVX0ByByT91/aciRJklYWvwFdkiSpg2FKkiSpg2FKkiSpg2FKkiSpg2FKkiSpg2FKkiSpg2FK\nkiSpg2FKkiSpg2FKkiSpg2FKkiSpg2FKkiSpg2FKkiSpg2FKkiSpg2FKkiSpg2FKkiSpg2FKkiSp\ng2FKkiSpg2FKkiSpg2FKkiSpg2FKkiSpg2FKkiSpg2FKkiSpg2FKkiSpg2FKkiSpg2FKkiSpg2FK\nkiSpg2FKkiSpg2FKkiSpg2FKkiSpg2FKkiSpw7xhKsnbklye5Osj445I8oMkZ7e/Ry1vmZIkSZNp\nnJ6pY4EDZxl/VFXt3f4+urRlSZIkrQzzhqmq+hxw1UaoRZIkacXpuWbqeUm+2k4Dbj9XoySHJVmX\nZN369es7FidJkjR5Fhum3gzcC9gbuAR47VwNq+roqlpbVWunpqYWuThJkqTJtKgwVVWXVdXPqurn\nwL8C+yxtWZIkSSvDosJUkp1GBp8AfH2utpIkSbdlq+ZrkOQ9wL7A6iQXAX8N7Jtkb6CA7wLPWsYa\nJUmSJta8YaqqDp5l9DHLUIskSdKK4zegS5IkdTBMSZIkdTBMSZIkdTBMSZIkdTBMSZIkdTBMSZIk\ndTBMSZIkdTBMSZIkdTBMSZIkdTBMSZIkdTBMSZIkdTBMSZIkdTBMSZIkdTBMSZIkdTBMSZIkdTBM\nSZIkdTBMSZIkdTBMSZIkdTBMSZIkdTBMSZIkdTBMSZIkdTBMSZIkdTBMSZIkdTBMSZIkdTBMSZIk\ndTBMSZIkdTBMSZIkdTBMSZIkdTBMSZIkdTBMSZIkdZg3TCV5W5LLk3x9ZNwOSU5Jcn77v/3ylilJ\nkjSZxumZOhY4cMa4lwCfqqo9gU+1YUmSpM3OvGGqqj4HXDVj9OOB49rt44CDlrguSZKkFWGx10zd\nraouAWj/7zpXwySHJVmXZN369esXuThJkqTJtOwXoFfV0VW1tqrWTk1NLffiJEmSNqrFhqnLkuwE\n0P5fvnQlSZIkrRyLDVMnA4e024cAJy1NOZIkSSvLOF+N8B7gNOC+SS5KcihwJPCIJOcDj2jDkiRJ\nm51V8zWoqoPnmLT/EtciSZK04vgN6JIkSR0MU5IkSR0MU5IkSR0MU5IkSR0MU5IkSR0MU5IkSR0M\nU5IkSR0MU5IkSR0MU5IkSR0MU5IkSR0MU5IkSR0MU5IkSR0MU5IkSR0MU5IkSR0MU5IkSR0MU5Ik\nSR0MU5IkSR0MU5IkSR0MU5IkSR0MU5IkSR0MU5IkSR0MU5IkSR0MU5IkSR0MU5IkSR0MU5IkSR0M\nU5IkSR0MU5IkSR0MU5IkSR0MU5IkSR0MU5IkSR1W9dw5yXeB64GfATdW1dqlKEqSJGml6ApTzcOr\n6oolmI8kSdKK42k+SZKkDr1hqoBPJDkryWGzNUhyWJJ1SdatX7++c3GSJEmTpTdMPbiqfgV4JPDc\nJL8xs0FVHV1Va6tq7dTUVOfiJEmSJktXmKqqi9v/y4EPAvssRVGSJEkrxaLDVJI7Jdl2+jbwm8DX\nl6owSZKklaDn03x3Az6YZHo+766q/1iSqiRJklaIRYepqroAeMAS1iJJkrTi+NUIkiRJHQxTkiRJ\nHQxTkiRJHQxTkiRJHQxTkiRJHQxTkiRJHQxTkiRJHQxTkiRJHQxTkiRJHQxTkiRJHQxTkiRJHQxT\nkiRJHQxTkiRJHQxTkiRJHQxTkiRJHQxTkiRJHQxTkiRJHQxTkiRJHQxTkiRJHQxTkiRJHQxTkiRJ\nHQxTkiRJHQxTkiRJHQxTkiRJHQxTkiRJHQxTkiRJHQxTkiRJHQxTkiRJHQxTkiRJHbrCVJIDk/x3\nkm8leclSFSVJkrRSLDpMJdkCeCPwSGAv4OAkey1VYZIkSStBT8/UPsC3quqCqvpf4ATg8UtTliRJ\n0srQE6Z2AS4cGb6ojZMkSdpsrOq4b2YZV7dqlBwGHNYGb0jy3x3LnDSrgSs2dRG6BbfJZJqI7ZJX\nb+oKJspEbBPdittlsuwxTqOeMHURsNvI8K7AxTMbVdXRwNEdy5lYSdZV1dpNXYdu5jaZTG6XyeM2\nmUxul5Wp5zTfl4A9k9wjyVbAU4GTl6YsSZKklWHRPVNVdWOS5wEfB7YA3lZV5yxZZZIkSStAz2k+\nquqjwEeXqJaV6DZ5+nKFc5tMJrfL5HGbTCa3ywqUqltdMy5JkqQx+XMykiRJHQxTC5BkhySnJDm/\n/d9+ljZ7JzktyTlJvprkKZui1tu6+X7KKMntk7y3TT8jyZqNX+XmZYxt8sIk57b94lNJxvrIsfqM\n+7NfSZ6UpJL4SbJlNs42SfI7bX85J8m7N3aNWhhP8y1Akn8ArqqqI9sOsH1VvXhGm/sAVVXnJ9kZ\nOAu4X1VdswlKvk1qP2X0TeARDF/R8SXg4Ko6d6TNc4BfrqpnJ3kq8ISqMtgukzG3ycOBM6rqR0n+\nGNjXbbK8xtkurd22wEeArYDnVdW6jV3r5mLMfWVP4ERgv6q6Osldq+ryTVKwxmLP1MI8Hjiu3T4O\nOGhmg6r6ZlWd325fDFwOTG20CjcP4/yU0ei2ej+wf5LZvmhWS2PebVJVn6mqH7XB0xm+m07La9yf\n/fpb4B+A/9mYxW2mxtkmfwS8saquBjBITT7D1MLcraouAWj/77qhxkn2YXin9+2NUNvmZJyfMrqp\nTVXdCFwL7LhRqts8LfTnpQ4FPrasFQnG2C5JHgjsVlUf3piFbcbG2VfuA9wnyX8mOT3JgRutOi1K\n11cj3BYl+SRw91kmvWyB89kJeAdwSFX9fClq003G+SmjsX7uSEtm7PWd5GnAWuBhy1qRYJ7tkuR2\nwFHAMzZWQRprX1kF7Ansy9CD+/kk9/dykcllmJqhqg6Ya1qSy5LsVFWXtLA0a9drkjszXH/w8qo6\nfZlK3ZyN81NG020uSrIKuAtw1cYpb7M01s9LJTmA4Y3Jw6rqJxupts3ZfNtlW+D+wKntLPjdgZOT\nPM7rppbNuMev06vqp8B32m/sTZuJAAABFElEQVTa7slwfZUmkKf5FuZk4JB2+xDgpJkN2k/rfBA4\nvqretxFr25yM81NGo9vqScCny09bLKd5t0k7nfQW4HFeA7LRbHC7VNW1VbW6qtZU1RqGa9kMUstr\nnOPXvwMPB0iymuG03wUbtUotiGFqYY4EHpHkfIZPYhwJkGRtkre2Nr8D/AbwjCRnt7+9N025t03t\nGqjpnzI6Dzixqs5J8jdJHteaHQPsmORbwAuBOT8Srn5jbpN/BLYB3tf2C3/Lc5mNuV20EY25TT4O\nXJnkXOAzwF9U1ZWbpmKNw69GkCRJ6mDPlCRJUgfDlCRJUgfDlCRJUgfDlCRJUgfDlCRJUgfDlCRJ\nUgfDlCRJUgfDlCRJUof/D/A04vU0PfQoAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x187c524d68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i, model_results in enumerate(best_results):\n",
    "    val_loss, val_acc = list(zip(*model_results))\n",
    "    plt.hist(val_acc, bins=25)\n",
    "    plt.title(\"Distribution of validation's accuracy for model with \" + str(i+1) + \" hidden layers\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_trainings = []\n",
    "\n",
    "for model_results in results:\n",
    "    best_accs = [max(training_history.history['val_acc']) for training_history in model_results]\n",
    "    best_training = model_results[best_accs.index(max(best_accs))]\n",
    "    best_trainings.append(best_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6: Tricks (regularization, batch normalization, dropout)\n",
    "\n",
    "### Description\n",
    "\n",
    "Overfitting can also be counteracted with regularization and dropout. Batch normalization is supposed to mainly decrease convergence time.\n",
    "\n",
    "1. Try to improve the best validation scores of the model with 1 layer and 100 hidden neurons and the model with 4 hidden layers. Experiment with batch_normalization layers, dropout layers and l1- and l2-regularization on weights (kernels) and biases.\n",
    "2. After you have found good settings, plot for both models the learning curves of the naive model you fitted in the previous exercises together with the learning curves of the current version.\n",
    "3. For proper comparison, plot also the learning curves of the two current models in a third figure.\n",
    "\n",
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weight_l1_regularizations = [10, 1.0, 0.1, 0.01, 0.001]\n",
    "weight_l2_regularizations = [10, 1.0, 0.1, 0.01, 0.001]\n",
    "\n",
    "bias_l1_regularizations = [10, 1.0, 0.1, 0.01, 0.001]\n",
    "bias_l2_regularizations = [10, 1.0, 0.1, 0.01, 0.001]\n",
    "\n",
    "dropouts = [[0.2, 0.5], [0.2, 0.9], [0.1, 0.1]]\n",
    "\n",
    "def grid_search(model_factory, compiler, model_fiter):\n",
    "    histories = []\n",
    "    \n",
    "    def inner_loop(*param_lists):\n",
    "        for w_reg, b_reg, dropout in itertools.product(*param_lists):\n",
    "            print(w_reg, b_reg, dropout)\n",
    "            model = model_factory(keras.regularizers.l1(w_reg), keras.regularizers.l1(b_reg), dropout=dropout)\n",
    "            compiler(model)\n",
    "            h = model_fiter(model)\n",
    "            histories.append(h)\n",
    "            \n",
    "    inner_loop(weight_l1_regularizations, bias_l1_regularizations, dropouts)\n",
    "    inner_loop(weight_l2_regularizations, bias_l1_regularizations, dropouts)\n",
    "    inner_loop(weight_l1_regularizations, bias_l2_regularizations, dropouts)\n",
    "    inner_loop(weight_l2_regularizations, bias_l2_regularizations, dropouts)\n",
    "    \n",
    "    return histories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 10s 350us/step - loss: nan - acc: 0.2515 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 4s 129us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 4s 130us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 4s 131us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 4s 137us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "10 10 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 9s 307us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 4s 133us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 4s 127us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 4s 144us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 4s 128us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "10 10 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 10s 335us/step - loss: 380.0654 - acc: 0.1997 - val_loss: 323.9326 - val_acc: 0.1567\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 5s 166us/step - loss: nan - acc: 0.2263 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 4s 139us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 4s 140us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 4s 130us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 4s 141us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "10 1.0 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 9s 311us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 5s 181us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 6s 187us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 5s 152us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 6s 196us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "10 1.0 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 11s 360us/step - loss: nan - acc: 0.2515 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 5s 161us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 6s 190us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 4s 145us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 4s 134us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "10 1.0 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 10s 327us/step - loss: 384.0249 - acc: 0.2017 - val_loss: 326.3061 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 4s 150us/step - loss: 346.9976 - acc: 0.2041 - val_loss: 355.0659 - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 4s 142us/step - loss: nan - acc: 0.2434 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 4s 135us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 4s 144us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 4s 146us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "10 0.1 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 10s 337us/step - loss: nan - acc: 0.2515 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 5s 153us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 4s 146us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 4s 138us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 5s 152us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "10 0.1 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 10s 337us/step - loss: nan - acc: 0.2517 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 4s 138us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 5s 167us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 5s 157us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 5s 158us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "10 0.1 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 11s 356us/step - loss: nan - acc: 0.2271 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 4s 133us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 4s 135us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 4s 140us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 4s 134us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "10 0.01 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 10s 346us/step - loss: nan - acc: 0.2517 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 4s 137us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 4s 139us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 4s 141us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 5s 155us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "10 0.01 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 10s 342us/step - loss: nan - acc: 0.2517 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 5s 159us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 4s 137us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 4s 135us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29639/29639 [==============================] - 4s 132us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "10 0.01 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 9s 320us/step - loss: 379.7845 - acc: 0.2473 - val_loss: 323.1224 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 4s 132us/step - loss: nan - acc: 0.2513 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 4s 128us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 4s 142us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 4s 127us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 4s 127us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "10 0.001 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 10s 321us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 4s 127us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 4s 127us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 4s 129us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 4s 128us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "10 0.001 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 10s 327us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 4s 129us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 4s 128us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 4s 128us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 4s 128us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "10 0.001 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 9s 315us/step - loss: nan - acc: 0.2510 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 4s 129us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 4s 127us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 4s 127us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 4s 129us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "1.0 10 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 9s 312us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 4s 127us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 4s 127us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 4s 128us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 4s 128us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "1.0 10 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 10s 321us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 4s 131us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 4s 129us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 4s 129us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 4s 129us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "1.0 10 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 10s 324us/step - loss: 42.8178 - acc: 0.1997 - val_loss: 35.6109 - val_acc: 0.1630\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 4s 130us/step - loss: nan - acc: 0.2224 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 4s 129us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 4s 130us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 4s 130us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 4s 131us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "1.0 1.0 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 10s 332us/step - loss: nan - acc: 0.2510 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 4s 129us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 4s 133us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 4s 131us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 4s 129us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "1.0 1.0 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 9s 319us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 4s 130us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 4s 128us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 4s 128us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 4s 128us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "1.0 1.0 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 10s 343us/step - loss: nan - acc: 0.2466 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 4s 142us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 4s 130us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 4s 130us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 4s 130us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "1.0 0.1 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 10s 328us/step - loss: nan - acc: 0.2515 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 4s 128us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 4s 130us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29639/29639 [==============================] - 4s 129us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 4s 130us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "1.0 0.1 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 10s 331us/step - loss: nan - acc: 0.2515 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 4s 132us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 4s 132us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 4s 130us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 4s 132us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "1.0 0.1 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 10s 340us/step - loss: 40.2984 - acc: 0.2248 - val_loss: 33.7105 - val_acc: 0.2342\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 4s 137us/step - loss: 36.6164 - acc: 0.2254 - val_loss: 36.0487 - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 4s 135us/step - loss: 36.6545 - acc: 0.2198 - val_loss: 35.5704 - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 4s 136us/step - loss: 36.5988 - acc: 0.2141 - val_loss: 34.1673 - val_acc: 0.1983\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 4s 137us/step - loss: 36.6241 - acc: 0.2204 - val_loss: 37.5400 - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 4s 134us/step - loss: 36.5758 - acc: 0.2248 - val_loss: 32.7309 - val_acc: 0.2342\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 4s 136us/step - loss: 36.6161 - acc: 0.2211 - val_loss: 36.0094 - val_acc: 0.2342\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 4s 136us/step - loss: nan - acc: 0.2459 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 9/80\n",
      "29639/29639 [==============================] - 4s 133us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 10/80\n",
      "29639/29639 [==============================] - 4s 134us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 11/80\n",
      "29639/29639 [==============================] - 4s 135us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "1.0 0.01 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 10s 330us/step - loss: nan - acc: 0.2515 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 4s 134us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 4s 136us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 4s 134us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 4s 136us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "1.0 0.01 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 10s 336us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 4s 133us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 4s 132us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 4s 134us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 4s 134us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "1.0 0.01 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 10s 332us/step - loss: 40.7756 - acc: 0.2468 - val_loss: 34.3449 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 4s 133us/step - loss: 37.1829 - acc: 0.2460 - val_loss: 36.5147 - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 4s 132us/step - loss: 37.1963 - acc: 0.2475 - val_loss: 35.5955 - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 4s 134us/step - loss: nan - acc: 0.2505 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 4s 134us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 4s 133us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "1.0 0.001 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 10s 350us/step - loss: nan - acc: 0.2515 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 4s 135us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 4s 134us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 4s 136us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 4s 136us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "1.0 0.001 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 11s 378us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 4s 147us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 4s 135us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 4s 134us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 4s 134us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "1.0 0.001 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 10s 344us/step - loss: 41.5247 - acc: 0.2472 - val_loss: 33.5666 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 4s 137us/step - loss: 37.7617 - acc: 0.2457 - val_loss: 38.0446 - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 4s 137us/step - loss: 37.7651 - acc: 0.2477 - val_loss: 35.8343 - val_acc: 0.2342\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 4s 137us/step - loss: 37.7344 - acc: 0.2451 - val_loss: 36.3686 - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 4s 136us/step - loss: 37.7278 - acc: 0.2478 - val_loss: 37.5890 - val_acc: 0.2342\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 4s 136us/step - loss: 37.7000 - acc: 0.2448 - val_loss: 34.5235 - val_acc: 0.2478\n",
      "0.1 10 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 10s 345us/step - loss: nan - acc: 0.2517 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 4s 140us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 4s 139us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 4s 139us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 4s 139us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.1 10 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 10s 341us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 4s 134us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 4s 135us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 4s 136us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 4s 135us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.1 10 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 10s 346us/step - loss: nan - acc: 0.2347 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 4s 136us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 4s 136us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 4s 136us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 4s 134us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.1 1.0 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 10s 348us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 4s 140us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 4s 140us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 4s 137us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 4s 139us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.1 1.0 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 11s 354us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 4s 140us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 4s 142us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 4s 138us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 4s 137us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.1 1.0 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 10s 352us/step - loss: 6.8099 - acc: 0.2061 - val_loss: 4.9340 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 4s 141us/step - loss: 6.4663 - acc: 0.2042 - val_loss: 5.1311 - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 4s 142us/step - loss: nan - acc: 0.2427 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 4s 141us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 4s 138us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 4s 139us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.1 0.1 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 11s 357us/step - loss: nan - acc: 0.2517 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 4s 141us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 4s 142us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 4s 141us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 4s 139us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.1 0.1 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 11s 357us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 4s 143us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 4s 142us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 4s 143us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 4s 142us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.1 0.1 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 11s 388us/step - loss: 6.6874 - acc: 0.2248 - val_loss: 4.8522 - val_acc: 0.1630\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 4s 140us/step - loss: 6.3259 - acc: 0.2239 - val_loss: 5.0109 - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 4s 138us/step - loss: nan - acc: 0.2487 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 4s 139us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 4s 141us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 4s 141us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.1 0.01 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 11s 358us/step - loss: nan - acc: 0.2515 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 4s 141us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 4s 139us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 4s 141us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 4s 141us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.1 0.01 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 11s 364us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 4s 142us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 4s 141us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 4s 140us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 4s 142us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.1 0.01 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 11s 369us/step - loss: 6.7371 - acc: 0.2500 - val_loss: 4.8356 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 4s 145us/step - loss: 6.3451 - acc: 0.2495 - val_loss: 5.0572 - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 4s 145us/step - loss: 6.3875 - acc: 0.2473 - val_loss: 5.0252 - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 4s 142us/step - loss: 6.3831 - acc: 0.2498 - val_loss: 4.9320 - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 4s 143us/step - loss: nan - acc: 0.2505 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 6/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29639/29639 [==============================] - 4s 143us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.1 0.001 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 11s 381us/step - loss: nan - acc: 0.2517 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 4s 142us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 4s 142us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 4s 142us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 4s 143us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.1 0.001 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 11s 376us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 4s 142us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 4s 143us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 4s 143us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 4s 145us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.1 0.001 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 11s 378us/step - loss: 6.8461 - acc: 0.2474 - val_loss: 4.8099 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 4s 145us/step - loss: nan - acc: 0.2472 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 4s 145us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 4s 145us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 4s 144us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 4s 144us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.01 10 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 11s 377us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 4s 146us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 4s 148us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 4s 145us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 4s 143us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.01 10 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 11s 376us/step - loss: nan - acc: 0.2517 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 4s 144us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 4s 143us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 4s 143us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 4s 143us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.01 10 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 11s 384us/step - loss: nan - acc: 0.2257 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 4s 143us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 4s 145us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 4s 143us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 4s 147us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.01 1.0 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 11s 382us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 4s 148us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 4s 149us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 4s 148us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 4s 150us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.01 1.0 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 14s 458us/step - loss: nan - acc: 0.2517 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 4s 148us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 4s 149us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 4s 149us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 4s 149us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.01 1.0 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 11s 382us/step - loss: nan - acc: 0.2399 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 4s 147us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 4s 146us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 4s 147us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 4s 146us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.01 0.1 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 11s 383us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 4s 146us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 4s 147us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 4s 148us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 4s 150us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.01 0.1 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 12s 391us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 4s 149us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 4s 147us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 4s 146us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29639/29639 [==============================] - 4s 145us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.01 0.1 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 11s 386us/step - loss: 3.3354 - acc: 0.2275 - val_loss: 1.9344 - val_acc: 0.1630\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 4s 147us/step - loss: nan - acc: 0.2308 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 4s 146us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 4s 147us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 4s 148us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 4s 150us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.01 0.01 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 12s 392us/step - loss: nan - acc: 0.2517 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 4s 152us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 4s 149us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 4s 149us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 5s 153us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.01 0.01 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 11s 386us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 4s 150us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 4s 149us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 4s 151us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 4s 151us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.01 0.01 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 12s 397us/step - loss: 3.3265 - acc: 0.2475 - val_loss: 1.9232 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 5s 153us/step - loss: 3.3028 - acc: 0.2503 - val_loss: 1.9490 - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 4s 152us/step - loss: 3.3500 - acc: 0.2470 - val_loss: 1.9361 - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 4s 148us/step - loss: nan - acc: 0.2493 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 4s 148us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 4s 149us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.01 0.001 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 12s 388us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 4s 150us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 4s 150us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 4s 150us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 5s 153us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.01 0.001 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 12s 395us/step - loss: nan - acc: 0.2517 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 5s 153us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 4s 149us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 4s 149us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 4s 149us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.01 0.001 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 12s 391us/step - loss: 3.3503 - acc: 0.2504 - val_loss: 1.9221 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 4s 152us/step - loss: 3.3086 - acc: 0.2491 - val_loss: 1.9506 - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 4s 150us/step - loss: 3.3324 - acc: 0.2463 - val_loss: 1.9183 - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 5s 154us/step - loss: nan - acc: 0.2515 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 5s 157us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 4s 151us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 5s 152us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 4s 151us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.001 10 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 12s 403us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 4s 151us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 5s 153us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 5s 154us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 5s 154us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.001 10 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 12s 404us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 5s 154us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 5s 152us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 4s 150us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 5s 153us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.001 10 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 12s 398us/step - loss: 4.0965 - acc: 0.2328 - val_loss: 2.4138 - val_acc: 0.1983\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 5s 156us/step - loss: 4.0331 - acc: 0.2337 - val_loss: 2.4911 - val_acc: 0.2342\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 5s 154us/step - loss: 4.0338 - acc: 0.2300 - val_loss: 2.9883 - val_acc: 0.2342\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 5s 152us/step - loss: 4.0386 - acc: 0.2281 - val_loss: 3.1094 - val_acc: 0.2342\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 5s 154us/step - loss: nan - acc: 0.2476 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 6/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29639/29639 [==============================] - 5s 153us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.001 1.0 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 12s 403us/step - loss: nan - acc: 0.2517 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 5s 156us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 5s 156us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 5s 158us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 5s 154us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.001 1.0 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 12s 404us/step - loss: nan - acc: 0.2517 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 5s 155us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 5s 152us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 5s 152us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 5s 156us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.001 1.0 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 12s 406us/step - loss: 3.1319 - acc: 0.2277 - val_loss: 1.7248 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 5s 157us/step - loss: 3.0790 - acc: 0.2277 - val_loss: 1.7248 - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 5s 154us/step - loss: 3.0804 - acc: 0.2312 - val_loss: 1.7210 - val_acc: 0.2342\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 5s 154us/step - loss: nan - acc: 0.2379 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 5s 154us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 5s 157us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 5s 157us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 5s 155us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.001 0.1 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 12s 404us/step - loss: nan - acc: 0.2515 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 5s 157us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 5s 157us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 5s 157us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 5s 159us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.001 0.1 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 13s 436us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 5s 162us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 5s 154us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 5s 156us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 5s 159us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.001 0.1 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 13s 439us/step - loss: nan - acc: 0.2376 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 5s 155us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 5s 156us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 5s 159us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 5s 158us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.001 0.01 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 13s 426us/step - loss: nan - acc: 0.2518 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 5s 156us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 5s 155us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 5s 157us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 5s 160us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.001 0.01 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 12s 417us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 5s 158us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 5s 158us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 5s 158us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 5s 162us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.001 0.01 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 12s 414us/step - loss: 3.0489 - acc: 0.2480 - val_loss: 1.6325 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 5s 156us/step - loss: nan - acc: 0.2509 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 5s 156us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 5s 157us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 5s 158us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 5s 160us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.001 0.001 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 12s 417us/step - loss: nan - acc: 0.2517 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 5s 159us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 5s 158us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 5s 157us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 5s 163us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.001 0.001 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 12s 421us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 5s 159us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 5s 157us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 5s 159us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 5s 166us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.001 0.001 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 14s 460us/step - loss: 3.0170 - acc: 0.2477 - val_loss: 1.6268 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 5s 177us/step - loss: 3.0489 - acc: 0.2454 - val_loss: 1.6262 - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 5s 165us/step - loss: 2.9640 - acc: 0.2481 - val_loss: 1.6262 - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 5s 166us/step - loss: 2.9823 - acc: 0.2473 - val_loss: 1.6259 - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 5s 162us/step - loss: 2.9266 - acc: 0.2508 - val_loss: 1.6228 - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 5s 162us/step - loss: 2.9764 - acc: 0.2486 - val_loss: 1.6232 - val_acc: 0.2478\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 5s 166us/step - loss: nan - acc: 0.2493 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 5s 163us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 9/80\n",
      "29639/29639 [==============================] - 5s 160us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 10/80\n",
      "29639/29639 [==============================] - 5s 160us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "10 10 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 13s 429us/step - loss: nan - acc: 0.2518 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 5s 164us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 5s 162us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 5s 159us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 5s 160us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "10 10 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 13s 433us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 5s 165us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 5s 160us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 5s 165us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 5s 160us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "10 10 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 13s 448us/step - loss: 379.9107 - acc: 0.2042 - val_loss: 328.5905 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 5s 162us/step - loss: 344.0482 - acc: 0.1970 - val_loss: 350.6252 - val_acc: 0.2342\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 5s 164us/step - loss: 343.8648 - acc: 0.2019 - val_loss: 347.6080 - val_acc: 0.1567\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 5s 165us/step - loss: 343.7123 - acc: 0.2036 - val_loss: 333.9313 - val_acc: 0.1567\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 5s 165us/step - loss: nan - acc: 0.2487 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 5s 167us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "10 1.0 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 13s 435us/step - loss: nan - acc: 0.2515 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 5s 163us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 5s 163us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 5s 161us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 5s 165us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "10 1.0 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 13s 441us/step - loss: nan - acc: 0.2517 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 5s 166us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 5s 165us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 5s 163us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 5s 161us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "10 1.0 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 13s 436us/step - loss: 381.1152 - acc: 0.2026 - val_loss: 328.2868 - val_acc: 0.1983\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 5s 162us/step - loss: 345.1843 - acc: 0.2063 - val_loss: 350.9272 - val_acc: 0.2342\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 5s 160us/step - loss: 344.8564 - acc: 0.2049 - val_loss: 340.3900 - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 5s 162us/step - loss: 344.5136 - acc: 0.2030 - val_loss: 339.6141 - val_acc: 0.1630\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 5s 165us/step - loss: 344.5743 - acc: 0.2011 - val_loss: 368.8127 - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 5s 163us/step - loss: 344.6270 - acc: 0.2026 - val_loss: 322.4053 - val_acc: 0.1630\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 5s 162us/step - loss: nan - acc: 0.2066 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 5s 162us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 9/80\n",
      "29639/29639 [==============================] - 5s 168us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 10/80\n",
      "29639/29639 [==============================] - 5s 164us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 11/80\n",
      "29639/29639 [==============================] - 5s 160us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "10 0.1 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 13s 435us/step - loss: nan - acc: 0.2517 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 5s 167us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 5s 167us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 5s 166us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 5s 163us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "10 0.1 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29639/29639 [==============================] - 13s 440us/step - loss: nan - acc: 0.2517 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 5s 168us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 5s 164us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 5s 164us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 5s 169us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "10 0.1 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 13s 440us/step - loss: 378.6545 - acc: 0.2237 - val_loss: 328.7249 - val_acc: 0.2342\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 5s 164us/step - loss: 342.6657 - acc: 0.2250 - val_loss: 347.8906 - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 5s 167us/step - loss: 342.3045 - acc: 0.2226 - val_loss: 339.6233 - val_acc: 0.2342\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 5s 169us/step - loss: 342.2420 - acc: 0.2245 - val_loss: 336.8615 - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 5s 168us/step - loss: nan - acc: 0.2464 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 5s 163us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "10 0.01 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 14s 461us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 5s 169us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 5s 164us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 5s 162us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 5s 164us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "10 0.01 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 13s 438us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 5s 165us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 5s 167us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 5s 169us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 5s 164us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "10 0.01 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 13s 453us/step - loss: 382.9624 - acc: 0.2468 - val_loss: 330.9191 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 5s 167us/step - loss: 345.0747 - acc: 0.2465 - val_loss: 347.4624 - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 5s 167us/step - loss: 344.9247 - acc: 0.2464 - val_loss: 340.2366 - val_acc: 0.2342\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 5s 169us/step - loss: 344.6438 - acc: 0.2491 - val_loss: 337.5333 - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 5s 166us/step - loss: 344.7308 - acc: 0.2475 - val_loss: 364.9546 - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 5s 166us/step - loss: 344.6724 - acc: 0.2472 - val_loss: 321.5786 - val_acc: 0.2478\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 5s 168us/step - loss: 344.5954 - acc: 0.2460 - val_loss: 343.3603 - val_acc: 0.2478\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 5s 170us/step - loss: 344.6042 - acc: 0.2444 - val_loss: 329.9621 - val_acc: 0.2478\n",
      "Epoch 9/80\n",
      "29639/29639 [==============================] - 5s 168us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 10/80\n",
      "29639/29639 [==============================] - 5s 170us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 11/80\n",
      "29639/29639 [==============================] - 5s 168us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "10 0.001 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 13s 454us/step - loss: nan - acc: 0.2515 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 5s 175us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 5s 167us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 5s 168us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 5s 167us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "10 0.001 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 14s 456us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 5s 167us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 5s 169us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 5s 171us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 5s 169us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "10 0.001 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 14s 457us/step - loss: 375.4900 - acc: 0.2473 - val_loss: 316.4011 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 5s 170us/step - loss: 339.2346 - acc: 0.2491 - val_loss: 348.0940 - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 5s 169us/step - loss: 339.2005 - acc: 0.2461 - val_loss: 335.9013 - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 5s 168us/step - loss: nan - acc: 0.2495 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 5s 170us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 5s 172us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "1.0 10 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 14s 484us/step - loss: nan - acc: 0.2514 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 6s 217us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 6s 219us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 6s 213us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 6s 194us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "1.0 10 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 14s 477us/step - loss: nan - acc: 0.2517 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 5s 174us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 5s 172us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29639/29639 [==============================] - 5s 170us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 5s 172us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "1.0 10 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 14s 461us/step - loss: 41.9626 - acc: 0.2008 - val_loss: 34.9287 - val_acc: 0.2342\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 5s 170us/step - loss: 38.2410 - acc: 0.2007 - val_loss: 37.2991 - val_acc: 0.1983\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 5s 174us/step - loss: nan - acc: 0.2495 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 5s 169us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 5s 169us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 5s 172us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "1.0 1.0 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 14s 463us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 5s 170us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 5s 174us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 5s 172us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 5s 172us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "1.0 1.0 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 14s 461us/step - loss: nan - acc: 0.2517 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 5s 175us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 5s 172us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 5s 173us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 5s 177us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "1.0 1.0 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 14s 469us/step - loss: 41.1841 - acc: 0.2033 - val_loss: 34.3616 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 5s 179us/step - loss: 37.3819 - acc: 0.2044 - val_loss: 36.5597 - val_acc: 0.2342\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 5s 172us/step - loss: 37.3851 - acc: 0.2040 - val_loss: 35.6339 - val_acc: 0.2342\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 5s 170us/step - loss: 37.3466 - acc: 0.2072 - val_loss: 34.9556 - val_acc: 0.1983\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 5s 174us/step - loss: nan - acc: 0.2094 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 5s 174us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "1.0 0.1 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 14s 484us/step - loss: nan - acc: 0.2517 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 5s 177us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 5s 171us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 5s 172us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 5s 173us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "1.0 0.1 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 14s 466us/step - loss: nan - acc: 0.2515 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 5s 179us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 5s 174us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 5s 173us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 5s 175us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "1.0 0.1 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 14s 474us/step - loss: 39.4264 - acc: 0.2215 - val_loss: 32.5788 - val_acc: 0.1567\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 5s 174us/step - loss: nan - acc: 0.2506 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 5s 175us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 5s 173us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 5s 175us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 5s 178us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "1.0 0.01 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 15s 491us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 5s 178us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 5s 174us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 5s 174us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 5s 176us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "1.0 0.01 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 14s 477us/step - loss: nan - acc: 0.2517 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 5s 177us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 5s 178us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 5s 174us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 6s 199us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "1.0 0.01 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 15s 489us/step - loss: 40.2631 - acc: 0.2480 - val_loss: 34.3227 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 7s 227us/step - loss: 36.6382 - acc: 0.2480 - val_loss: 35.6492 - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 6s 192us/step - loss: 36.6778 - acc: 0.2473 - val_loss: 35.4211 - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 6s 193us/step - loss: 36.6654 - acc: 0.2457 - val_loss: 34.3807 - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 6s 195us/step - loss: nan - acc: 0.2512 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 6s 190us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "1.0 0.001 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29639/29639 [==============================] - 14s 487us/step - loss: nan - acc: 0.2517 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 5s 180us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 5s 178us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 5s 181us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 5s 178us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "1.0 0.001 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 14s 482us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 5s 177us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 5s 177us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 5s 181us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 5s 179us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "1.0 0.001 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 15s 505us/step - loss: 40.9199 - acc: 0.2463 - val_loss: 34.6260 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 5s 181us/step - loss: nan - acc: 0.2489 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 5s 182us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 5s 184us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 6s 186us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 5s 180us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.1 10 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 15s 495us/step - loss: nan - acc: 0.2515 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 5s 182us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 6s 215us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 6s 199us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 6s 194us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.1 10 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 15s 497us/step - loss: nan - acc: 0.2517 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 6s 186us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 6s 188us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 6s 187us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 6s 195us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.1 10 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 15s 508us/step - loss: nan - acc: 0.2319 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 6s 196us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 6s 190us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 6s 190us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 6s 187us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.1 1.0 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 15s 511us/step - loss: nan - acc: 0.2515 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 6s 193us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 6s 189us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 6s 188us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 6s 187us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.1 1.0 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 15s 497us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 5s 185us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 6s 190us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 6s 186us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 5s 184us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.1 1.0 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 15s 494us/step - loss: 6.8775 - acc: 0.2044 - val_loss: 4.9759 - val_acc: 0.1630\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 5s 183us/step - loss: nan - acc: 0.2286 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 5s 184us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 6s 187us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 5s 185us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 6s 186us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.1 0.1 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 15s 496us/step - loss: nan - acc: 0.2517 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 6s 187us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 6s 187us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 6s 193us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 5s 183us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.1 0.1 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 15s 499us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 6s 188us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 5s 185us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 5s 182us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 5s 185us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1 0.1 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 15s 504us/step - loss: nan - acc: 0.2378 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 6s 187us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 5s 183us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 6s 186us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 5s 184us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.1 0.01 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 15s 503us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 5s 183us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 6s 186us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 5s 183us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 5s 181us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.1 0.01 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 15s 505us/step - loss: nan - acc: 0.2515 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 6s 189us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 5s 184us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 5s 185us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 6s 188us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.1 0.01 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 15s 503us/step - loss: 6.6501 - acc: 0.2453 - val_loss: 4.7780 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 6s 190us/step - loss: 6.2668 - acc: 0.2464 - val_loss: 4.9554 - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 6s 188us/step - loss: nan - acc: 0.2501 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 6s 188us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 5s 181us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 5s 178us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.1 0.001 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 15s 520us/step - loss: nan - acc: 0.2515 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 6s 200us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 5s 172us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 5s 172us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 5s 175us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.1 0.001 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 18s 614us/step - loss: nan - acc: 0.2518 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 7s 220us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 6s 215us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 6s 217us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 6s 209us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.1 0.001 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 15s 516us/step - loss: 6.9322 - acc: 0.2458 - val_loss: 4.8892 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 6s 186us/step - loss: 6.5074 - acc: 0.2471 - val_loss: 5.2659 - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 5s 182us/step - loss: 6.4816 - acc: 0.2460 - val_loss: 5.0526 - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 6s 189us/step - loss: 6.5207 - acc: 0.2468 - val_loss: 5.1048 - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 5s 185us/step - loss: 6.5127 - acc: 0.2473 - val_loss: 5.3126 - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 5s 183us/step - loss: 6.5000 - acc: 0.2492 - val_loss: 4.8720 - val_acc: 0.2478\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 6s 188us/step - loss: 6.5071 - acc: 0.2484 - val_loss: 5.0636 - val_acc: 0.2478\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 5s 185us/step - loss: nan - acc: 0.2515 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 9/80\n",
      "29639/29639 [==============================] - 6s 186us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 10/80\n",
      "29639/29639 [==============================] - 6s 186us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 11/80\n",
      "29639/29639 [==============================] - 6s 188us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.01 10 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 15s 510us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 6s 188us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 6s 188us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 6s 186us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 6s 190us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.01 10 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 15s 515us/step - loss: nan - acc: 0.2517 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 5s 184us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 5s 185us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 6s 188us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 5s 184us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.01 10 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 15s 518us/step - loss: nan - acc: 0.2474 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 6s 201us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 5s 185us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 6s 187us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29639/29639 [==============================] - 6s 190us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.01 1.0 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 15s 522us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 6s 188us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 6s 191us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 6s 188us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 6s 187us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.01 1.0 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 15s 514us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 6s 188us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 6s 187us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 6s 190us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 6s 188us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.01 1.0 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 15s 520us/step - loss: 3.4493 - acc: 0.2074 - val_loss: 2.0152 - val_acc: 0.1983\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 6s 189us/step - loss: 3.3871 - acc: 0.2092 - val_loss: 2.0560 - val_acc: 0.1630\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 6s 195us/step - loss: 3.3867 - acc: 0.2027 - val_loss: 2.0386 - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 6s 189us/step - loss: 3.3848 - acc: 0.2056 - val_loss: 2.0488 - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 6s 193us/step - loss: 3.4337 - acc: 0.2093 - val_loss: 2.0581 - val_acc: 0.1983\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 6s 191us/step - loss: 3.3743 - acc: 0.2075 - val_loss: 2.0121 - val_acc: 0.1567\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 6s 190us/step - loss: 3.3964 - acc: 0.2071 - val_loss: 2.0358 - val_acc: 0.1630\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 6s 190us/step - loss: 3.3939 - acc: 0.2011 - val_loss: 2.0267 - val_acc: 0.2342\n",
      "Epoch 9/80\n",
      "29639/29639 [==============================] - 6s 188us/step - loss: nan - acc: 0.2303 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 10/80\n",
      "29639/29639 [==============================] - 6s 192us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 11/80\n",
      "29639/29639 [==============================] - 6s 188us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.01 0.1 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 15s 517us/step - loss: nan - acc: 0.2513 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 6s 189us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 6s 188us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 6s 187us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 6s 195us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.01 0.1 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 16s 541us/step - loss: nan - acc: 0.2517 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 6s 190us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 6s 192us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 6s 190us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 6s 192us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.01 0.1 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 15s 523us/step - loss: 3.3589 - acc: 0.2232 - val_loss: 1.9417 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 6s 190us/step - loss: 3.3226 - acc: 0.2252 - val_loss: 1.9640 - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 6s 188us/step - loss: nan - acc: 0.2450 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 6s 193us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 6s 189us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 6s 191us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.01 0.01 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 16s 531us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 6s 190us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 6s 195us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 6s 197us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 6s 191us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.01 0.01 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 16s 524us/step - loss: nan - acc: 0.2517 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 6s 194us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 6s 190us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 6s 196us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 6s 190us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.01 0.01 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 15s 521us/step - loss: 3.3823 - acc: 0.2480 - val_loss: 1.9273 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 6s 191us/step - loss: 3.3426 - acc: 0.2469 - val_loss: 1.9617 - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 6s 189us/step - loss: 3.3255 - acc: 0.2487 - val_loss: 1.9460 - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 6s 188us/step - loss: nan - acc: 0.2497 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 6s 192us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 6s 190us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.01 0.001 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 16s 533us/step - loss: nan - acc: 0.2515 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 6s 197us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29639/29639 [==============================] - 6s 189us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 6s 194us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 6s 192us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.01 0.001 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 16s 528us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 6s 193us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 7s 240us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 7s 226us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 7s 231us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.01 0.001 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 17s 566us/step - loss: nan - acc: 0.2484 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 7s 233us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 6s 210us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 7s 224us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 7s 223us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.001 10 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 16s 552us/step - loss: nan - acc: 0.2518 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 7s 248us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 7s 245us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 7s 245us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 7s 248us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.001 10 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 17s 586us/step - loss: nan - acc: 0.2518 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 7s 251us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 6s 210us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 6s 201us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 6s 203us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.001 10 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 17s 581us/step - loss: 4.1302 - acc: 0.2293 - val_loss: 2.3669 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 7s 228us/step - loss: 4.1177 - acc: 0.2308 - val_loss: 2.8642 - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 6s 204us/step - loss: 4.1033 - acc: 0.2278 - val_loss: 3.0232 - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 6s 206us/step - loss: 4.0818 - acc: 0.2300 - val_loss: 2.8518 - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 6s 203us/step - loss: 4.1523 - acc: 0.2314 - val_loss: 2.8099 - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 6s 204us/step - loss: 4.1207 - acc: 0.2310 - val_loss: 2.6169 - val_acc: 0.2478\n",
      "0.001 1.0 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 17s 571us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 7s 245us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 7s 235us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 6s 195us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 6s 190us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.001 1.0 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 16s 543us/step - loss: nan - acc: 0.2515 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 6s 199us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 6s 192us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 6s 198us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 6s 194us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.001 1.0 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 16s 545us/step - loss: 3.1012 - acc: 0.2305 - val_loss: 1.7224 - val_acc: 0.1983\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 6s 205us/step - loss: 3.0893 - acc: 0.2227 - val_loss: 1.7232 - val_acc: 0.1983\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 6s 194us/step - loss: 3.0849 - acc: 0.2255 - val_loss: 1.7323 - val_acc: 0.1567\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 6s 197us/step - loss: nan - acc: 0.2503 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 6s 194us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 6s 198us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.001 0.1 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 16s 537us/step - loss: nan - acc: 0.2518 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 6s 194us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 6s 198us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 6s 194us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 6s 199us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.001 0.1 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 16s 542us/step - loss: nan - acc: 0.2517 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 6s 194us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 6s 198us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 6s 195us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 6s 199us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.001 0.1 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 17s 577us/step - loss: nan - acc: 0.2446 - val_loss: nan - val_acc: 0.2478\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 6s 196us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 6s 195us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 6s 194us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 6s 199us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.001 0.01 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 17s 559us/step - loss: nan - acc: 0.2515 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 7s 240us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 7s 241us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 6s 199us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 6s 198us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.001 0.01 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 18s 592us/step - loss: nan - acc: 0.2517 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 6s 203us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 6s 196us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 6s 201us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 6s 196us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.001 0.01 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 17s 558us/step - loss: 3.0421 - acc: 0.2445 - val_loss: 1.6354 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 7s 231us/step - loss: 3.0122 - acc: 0.2485 - val_loss: 1.6363 - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 6s 200us/step - loss: 2.9869 - acc: 0.2477 - val_loss: 1.6324 - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 6s 198us/step - loss: 2.9899 - acc: 0.2453 - val_loss: 1.6313 - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 6s 199us/step - loss: 2.9928 - acc: 0.2487 - val_loss: 1.6335 - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 6s 204us/step - loss: nan - acc: 0.2478 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 6s 201us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 6s 203us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 9/80\n",
      "29639/29639 [==============================] - 6s 200us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.001 0.001 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 18s 594us/step - loss: nan - acc: 0.2515 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 7s 246us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 7s 248us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 7s 229us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 6s 201us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.001 0.001 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 18s 602us/step - loss: nan - acc: 0.2517 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 7s 244us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 6s 193us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 6s 187us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 5s 184us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.001 0.001 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 17s 559us/step - loss: 3.0170 - acc: 0.2492 - val_loss: 1.6251 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 6s 216us/step - loss: nan - acc: 0.2467 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 6s 203us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 6s 199us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 6s 201us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 6s 196us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "10 10 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 17s 576us/step - loss: nan - acc: 0.2513 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 6s 205us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 6s 202us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 6s 199us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 6s 202us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "10 10 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 16s 554us/step - loss: nan - acc: 0.2517 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 6s 199us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 6s 204us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 6s 198us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 6s 204us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "10 10 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 16s 551us/step - loss: 385.6992 - acc: 0.2026 - val_loss: 330.1896 - val_acc: 0.2342\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 6s 193us/step - loss: 348.2612 - acc: 0.1991 - val_loss: 347.7108 - val_acc: 0.1983\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 6s 188us/step - loss: 347.9779 - acc: 0.2037 - val_loss: 349.0051 - val_acc: 0.1983\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 6s 203us/step - loss: nan - acc: 0.2183 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 6s 204us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 6s 209us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "10 1.0 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 18s 610us/step - loss: nan - acc: 0.2517 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29639/29639 [==============================] - 6s 206us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 6s 204us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 6s 200us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 6s 206us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "10 1.0 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 17s 575us/step - loss: nan - acc: 0.2517 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 6s 203us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 6s 201us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 6s 204us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 6s 201us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "10 1.0 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 17s 572us/step - loss: 395.9872 - acc: 0.2020 - val_loss: 329.9381 - val_acc: 0.1567\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 6s 204us/step - loss: 356.8971 - acc: 0.2005 - val_loss: 369.7471 - val_acc: 0.1567\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 6s 202us/step - loss: nan - acc: 0.2352 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 6s 200us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 6s 201us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 6s 202us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "10 0.1 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 17s 577us/step - loss: nan - acc: 0.2517 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 6s 208us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 6s 206us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 6s 206us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 6s 203us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "10 0.1 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 17s 583us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 6s 204us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 6s 205us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 6s 205us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 6s 205us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "10 0.1 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 17s 589us/step - loss: nan - acc: 0.2407 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 6s 207us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 6s 206us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 6s 206us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 6s 207us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "10 0.01 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 17s 586us/step - loss: nan - acc: 0.2519 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 6s 211us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 6s 206us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 6s 211us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 6s 206us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "10 0.01 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 17s 590us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 6s 206us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 6s 205us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 6s 202us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 6s 204us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "10 0.01 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 18s 610us/step - loss: 382.4256 - acc: 0.2475 - val_loss: 322.6223 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 6s 206us/step - loss: nan - acc: 0.2515 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 6s 210us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 6s 205us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 6s 207us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 6s 204us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "10 0.001 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 17s 587us/step - loss: nan - acc: 0.2515 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 6s 206us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 6s 216us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 6s 202us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 6s 203us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "10 0.001 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 18s 593us/step - loss: nan - acc: 0.2517 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 6s 204us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 6s 193us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 6s 192us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 6s 191us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "10 0.001 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29639/29639 [==============================] - 18s 595us/step - loss: 377.1460 - acc: 0.2458 - val_loss: 328.1892 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 7s 233us/step - loss: 341.0811 - acc: 0.2490 - val_loss: 344.8335 - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 7s 233us/step - loss: 341.0098 - acc: 0.2509 - val_loss: 336.5324 - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 7s 234us/step - loss: 340.9588 - acc: 0.2440 - val_loss: 333.7834 - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 7s 233us/step - loss: 341.0492 - acc: 0.2481 - val_loss: 358.5252 - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 7s 233us/step - loss: 340.9478 - acc: 0.2497 - val_loss: 317.0765 - val_acc: 0.2478\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 7s 233us/step - loss: 340.9033 - acc: 0.2488 - val_loss: 345.7342 - val_acc: 0.2478\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 7s 232us/step - loss: 340.9456 - acc: 0.2476 - val_loss: 324.8913 - val_acc: 0.2478\n",
      "Epoch 9/80\n",
      "29639/29639 [==============================] - 7s 234us/step - loss: 340.8989 - acc: 0.2478 - val_loss: 350.9965 - val_acc: 0.2478\n",
      "Epoch 10/80\n",
      "29639/29639 [==============================] - 7s 235us/step - loss: 340.9634 - acc: 0.2461 - val_loss: 347.0184 - val_acc: 0.2342\n",
      "Epoch 11/80\n",
      "29639/29639 [==============================] - 7s 233us/step - loss: nan - acc: 0.2499 - val_loss: nan - val_acc: 0.2478\n",
      "1.0 10 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 18s 600us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 7s 235us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 7s 234us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 7s 237us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 7s 235us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "1.0 10 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 18s 599us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 7s 232us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 7s 233us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 7s 232us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 7s 232us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "1.0 10 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 18s 601us/step - loss: 42.6297 - acc: 0.2000 - val_loss: 35.0533 - val_acc: 0.1983\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 7s 235us/step - loss: nan - acc: 0.2498 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 7s 234us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 7s 235us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 7s 234us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 7s 235us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "1.0 1.0 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 18s 601us/step - loss: nan - acc: 0.2517 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 7s 233us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 7s 235us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 7s 234us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 7s 234us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "1.0 1.0 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 18s 604us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 7s 234us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 7s 236us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 7s 235us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 7s 236us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "1.0 1.0 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 18s 604us/step - loss: 40.6472 - acc: 0.2009 - val_loss: 33.6707 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 7s 234us/step - loss: 36.9751 - acc: 0.2000 - val_loss: 36.1319 - val_acc: 0.1983\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 7s 234us/step - loss: 36.9542 - acc: 0.1997 - val_loss: 35.4968 - val_acc: 0.1567\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 7s 233us/step - loss: 36.9602 - acc: 0.2000 - val_loss: 34.6041 - val_acc: 0.1630\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 7s 235us/step - loss: 36.9815 - acc: 0.1991 - val_loss: 37.7677 - val_acc: 0.1983\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 7s 234us/step - loss: 36.9921 - acc: 0.2014 - val_loss: 33.3707 - val_acc: 0.2478\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 7s 235us/step - loss: 36.9694 - acc: 0.2030 - val_loss: 36.6228 - val_acc: 0.2478\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 7s 234us/step - loss: 37.0285 - acc: 0.2089 - val_loss: 34.1793 - val_acc: 0.2342\n",
      "Epoch 9/80\n",
      "29639/29639 [==============================] - 7s 234us/step - loss: 37.0180 - acc: 0.1993 - val_loss: 36.0080 - val_acc: 0.1567\n",
      "Epoch 10/80\n",
      "29639/29639 [==============================] - 7s 235us/step - loss: 36.9684 - acc: 0.2042 - val_loss: 36.1084 - val_acc: 0.1567\n",
      "Epoch 11/80\n",
      "29639/29639 [==============================] - 7s 234us/step - loss: 36.9364 - acc: 0.2036 - val_loss: 33.5233 - val_acc: 0.2478\n",
      "1.0 0.1 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 18s 603us/step - loss: nan - acc: 0.2514 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 7s 237us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 7s 238us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 7s 238us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 7s 238us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "1.0 0.1 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 18s 614us/step - loss: nan - acc: 0.2515 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 7s 238us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 7s 239us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29639/29639 [==============================] - 7s 239us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 7s 239us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "1.0 0.1 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 18s 609us/step - loss: 40.6111 - acc: 0.2265 - val_loss: 32.8444 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 7s 238us/step - loss: 36.8335 - acc: 0.2272 - val_loss: 36.9787 - val_acc: 0.1983\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 7s 237us/step - loss: 36.8995 - acc: 0.2198 - val_loss: 34.4007 - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 7s 237us/step - loss: nan - acc: 0.2264 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 7s 238us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 7s 237us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "1.0 0.01 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 18s 605us/step - loss: nan - acc: 0.2514 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 7s 238us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 7s 235us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 7s 236us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 7s 235us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "1.0 0.01 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 18s 609us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 7s 238us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 7s 238us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 7s 239us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 7s 237us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "1.0 0.01 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 18s 612us/step - loss: 40.7345 - acc: 0.2465 - val_loss: 33.5245 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 7s 238us/step - loss: 37.0727 - acc: 0.2464 - val_loss: 36.2534 - val_acc: 0.2342\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 7s 239us/step - loss: 37.0415 - acc: 0.2510 - val_loss: 35.4194 - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 7s 239us/step - loss: 37.0235 - acc: 0.2471 - val_loss: 35.0457 - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 7s 240us/step - loss: nan - acc: 0.2503 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 7s 242us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "1.0 0.001 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 18s 618us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 7s 240us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 7s 228us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 6s 216us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 6s 209us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "1.0 0.001 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 18s 605us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 6s 211us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 6s 209us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 6s 211us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 6s 209us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "1.0 0.001 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 19s 626us/step - loss: nan - acc: 0.2475 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 6s 210us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 6s 209us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 6s 213us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 6s 207us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.1 10 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 19s 633us/step - loss: nan - acc: 0.2517 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 6s 214us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 6s 211us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 6s 214us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 6s 212us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.1 10 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 18s 608us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 6s 213us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 6s 211us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 6s 211us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 6s 210us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.1 10 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 19s 648us/step - loss: 7.6754 - acc: 0.2026 - val_loss: 5.5530 - val_acc: 0.1983\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 7s 227us/step - loss: 7.2368 - acc: 0.1996 - val_loss: 5.8846 - val_acc: 0.1630\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 7s 228us/step - loss: nan - acc: 0.2448 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 7s 220us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 7s 250us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 7s 222us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.1 1.0 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 19s 628us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 7s 227us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 7s 227us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 8s 259us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 8s 265us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.1 1.0 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 20s 664us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 7s 243us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 8s 278us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 8s 270us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 7s 241us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.1 1.0 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 20s 680us/step - loss: nan - acc: 0.2250 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 7s 230us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 6s 219us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 7s 220us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 6s 219us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.1 0.1 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 20s 665us/step - loss: nan - acc: 0.2514 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 8s 271us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 8s 275us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 9s 306us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 8s 257us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.1 0.1 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 21s 693us/step - loss: nan - acc: 0.2517 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 8s 276us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 7s 220us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 6s 217us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 6s 214us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.1 0.1 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 19s 641us/step - loss: nan - acc: 0.2247 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 6s 205us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 7s 238us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 7s 240us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 7s 239us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.1 0.01 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 20s 678us/step - loss: nan - acc: 0.2514 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 8s 263us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 7s 242us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 7s 238us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 7s 224us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.1 0.01 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 19s 631us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 7s 251us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 8s 253us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 8s 261us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 8s 262us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.1 0.01 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 20s 671us/step - loss: 6.7144 - acc: 0.2486 - val_loss: 4.8154 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 8s 271us/step - loss: nan - acc: 0.2512 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 7s 230us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 8s 258us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 8s 259us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 8s 285us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.1 0.001 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 19s 654us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 7s 245us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 7s 235us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 7s 232us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 7s 225us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.1 0.001 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 20s 682us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 8s 269us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 8s 270us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 8s 269us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 8s 270us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.1 0.001 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 20s 680us/step - loss: 6.7078 - acc: 0.2468 - val_loss: 4.8035 - val_acc: 0.2478\n",
      "Epoch 2/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29639/29639 [==============================] - 8s 278us/step - loss: 6.4271 - acc: 0.2464 - val_loss: 5.0346 - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 8s 280us/step - loss: 6.3712 - acc: 0.2456 - val_loss: 4.9971 - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 8s 280us/step - loss: 6.3655 - acc: 0.2465 - val_loss: 4.9462 - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 8s 278us/step - loss: 6.3372 - acc: 0.2476 - val_loss: 5.2382 - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 8s 279us/step - loss: nan - acc: 0.2502 - val_loss: nan - val_acc: 0.2478\n",
      "0.01 10 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 20s 681us/step - loss: nan - acc: 0.2514 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 7s 238us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 7s 244us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 7s 242us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 6s 208us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.01 10 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 21s 700us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 8s 268us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 8s 256us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 7s 246us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 8s 261us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.01 10 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 21s 707us/step - loss: 5.0144 - acc: 0.1985 - val_loss: 3.8549 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 7s 239us/step - loss: 4.9151 - acc: 0.2068 - val_loss: 2.2578 - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 6s 206us/step - loss: 4.9396 - acc: 0.2015 - val_loss: 3.3149 - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 6s 206us/step - loss: 4.9111 - acc: 0.1984 - val_loss: 3.5978 - val_acc: 0.2342\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 6s 206us/step - loss: nan - acc: 0.2300 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 6s 207us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 6s 206us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.01 1.0 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 20s 665us/step - loss: nan - acc: 0.2518 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 8s 255us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 7s 244us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 7s 244us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 8s 254us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.01 1.0 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 19s 652us/step - loss: nan - acc: 0.2515 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 8s 260us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 8s 260us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 8s 256us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 8s 261us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.01 1.0 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 19s 656us/step - loss: nan - acc: 0.2434 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 7s 243us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 8s 257us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 7s 244us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 7s 221us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.01 0.1 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 21s 692us/step - loss: nan - acc: 0.2513 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 7s 224us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 6s 212us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 7s 223us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 6s 210us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.01 0.1 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 20s 660us/step - loss: nan - acc: 0.2515 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 7s 221us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 6s 212us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 6s 210us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 7s 220us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.01 0.1 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 19s 631us/step - loss: nan - acc: 0.2330 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 7s 237us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 6s 212us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 6s 213us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 6s 217us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.01 0.01 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 20s 660us/step - loss: nan - acc: 0.2515 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 8s 255us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 8s 256us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 7s 227us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29639/29639 [==============================] - 7s 219us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.01 0.01 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 20s 666us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 7s 235us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 7s 249us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 7s 241us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 7s 249us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.01 0.01 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 20s 663us/step - loss: 3.3612 - acc: 0.2468 - val_loss: 1.9222 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 7s 240us/step - loss: 3.3122 - acc: 0.2472 - val_loss: 1.9496 - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 7s 230us/step - loss: nan - acc: 0.2458 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 7s 227us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 6s 214us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 7s 223us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.01 0.001 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 20s 682us/step - loss: nan - acc: 0.2517 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 8s 262us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 7s 239us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 6s 216us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 6s 215us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.01 0.001 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 20s 675us/step - loss: nan - acc: 0.2517 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 7s 236us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 6s 215us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 6s 215us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 6s 215us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.01 0.001 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 20s 685us/step - loss: 3.3675 - acc: 0.2469 - val_loss: 1.9115 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 8s 262us/step - loss: 3.2873 - acc: 0.2464 - val_loss: 1.9555 - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 8s 263us/step - loss: 3.2818 - acc: 0.2466 - val_loss: 1.9297 - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 7s 234us/step - loss: 3.3158 - acc: 0.2475 - val_loss: 1.9376 - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 6s 212us/step - loss: nan - acc: 0.2474 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 6s 211us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.001 10 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 20s 664us/step - loss: nan - acc: 0.2515 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 7s 232us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 6s 214us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 6s 213us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 6s 212us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.001 10 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 20s 688us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 8s 262us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 6s 216us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 7s 228us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 7s 220us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.001 10 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 21s 700us/step - loss: 3.9914 - acc: 0.2307 - val_loss: 2.4722 - val_acc: 0.1983\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 8s 271us/step - loss: 3.9920 - acc: 0.2252 - val_loss: 2.6127 - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 8s 262us/step - loss: 3.9341 - acc: 0.2295 - val_loss: 2.6821 - val_acc: 0.2342\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 8s 262us/step - loss: 3.9133 - acc: 0.2321 - val_loss: 2.7519 - val_acc: 0.2342\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 8s 261us/step - loss: 3.9361 - acc: 0.2296 - val_loss: 2.7289 - val_acc: 0.1983\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 8s 263us/step - loss: 3.9624 - acc: 0.2304 - val_loss: 2.3662 - val_acc: 0.1630\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 8s 263us/step - loss: 3.9342 - acc: 0.2252 - val_loss: 2.5908 - val_acc: 0.2478\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 8s 261us/step - loss: 3.9605 - acc: 0.2281 - val_loss: 2.6482 - val_acc: 0.1983\n",
      "Epoch 9/80\n",
      "29639/29639 [==============================] - 8s 257us/step - loss: 3.9764 - acc: 0.2262 - val_loss: 2.4404 - val_acc: 0.2478\n",
      "Epoch 10/80\n",
      "29639/29639 [==============================] - 6s 218us/step - loss: 3.9278 - acc: 0.2270 - val_loss: 2.5107 - val_acc: 0.1567\n",
      "Epoch 11/80\n",
      "29639/29639 [==============================] - 6s 216us/step - loss: nan - acc: 0.2261 - val_loss: nan - val_acc: 0.2478\n",
      "0.001 1.0 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 20s 688us/step - loss: nan - acc: 0.2514 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 8s 262us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 8s 261us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 8s 261us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 8s 262us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.001 1.0 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 21s 701us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 8s 273us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 8s 265us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 8s 264us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 8s 265us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.001 1.0 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 21s 696us/step - loss: nan - acc: 0.2472 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 7s 232us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 6s 218us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 6s 219us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 7s 227us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.001 0.1 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 20s 675us/step - loss: nan - acc: 0.2513 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 7s 247us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 6s 218us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 7s 235us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 8s 282us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.001 0.1 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 21s 720us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 8s 258us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 7s 220us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 7s 220us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 7s 220us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.001 0.1 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 21s 713us/step - loss: 3.0261 - acc: 0.2291 - val_loss: 1.6491 - val_acc: 0.1630\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 8s 269us/step - loss: 3.0219 - acc: 0.2246 - val_loss: 1.6486 - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 8s 269us/step - loss: 3.0339 - acc: 0.2316 - val_loss: 1.6495 - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 8s 269us/step - loss: nan - acc: 0.2379 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 8s 268us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 8s 269us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.001 0.01 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 21s 711us/step - loss: nan - acc: 0.2517 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 8s 268us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 8s 270us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 8s 268us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 8s 268us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.001 0.01 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 21s 707us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 8s 266us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 8s 262us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 8s 262us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 8s 272us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.001 0.01 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 21s 711us/step - loss: 3.0132 - acc: 0.2463 - val_loss: 1.6333 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 8s 262us/step - loss: 3.0276 - acc: 0.2473 - val_loss: 1.6324 - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 8s 272us/step - loss: nan - acc: 0.2496 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 8s 277us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 8s 267us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 8s 277us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.001 0.001 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 22s 735us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 7s 239us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 7s 238us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 7s 235us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 7s 244us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.001 0.001 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 21s 715us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 7s 240us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 7s 234us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 7s 236us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 7s 232us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.001 0.001 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 22s 750us/step - loss: 2.9768 - acc: 0.2463 - val_loss: 1.6255 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 7s 240us/step - loss: 2.9694 - acc: 0.2481 - val_loss: 1.6253 - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 7s 243us/step - loss: 2.9858 - acc: 0.2457 - val_loss: 1.6230 - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 7s 241us/step - loss: 2.9971 - acc: 0.2489 - val_loss: 1.6229 - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 7s 233us/step - loss: 2.9696 - acc: 0.2463 - val_loss: 1.6236 - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 7s 227us/step - loss: 2.9773 - acc: 0.2469 - val_loss: 1.6237 - val_acc: 0.2478\n",
      "Epoch 7/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29639/29639 [==============================] - 7s 227us/step - loss: nan - acc: 0.2500 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 7s 241us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "10 10 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 22s 754us/step - loss: nan - acc: 0.2515 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 9s 287us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 7s 234us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 8s 285us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 8s 274us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "10 10 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 21s 711us/step - loss: nan - acc: 0.2515 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 7s 251us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 7s 251us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 7s 239us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 7s 239us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "10 10 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 21s 708us/step - loss: nan - acc: 0.2498 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 7s 243us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 8s 253us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 8s 261us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 8s 267us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "10 1.0 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 23s 764us/step - loss: nan - acc: 0.2515 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 7s 244us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 8s 259us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 7s 242us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 8s 262us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "10 1.0 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 21s 712us/step - loss: nan - acc: 0.2517 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 7s 230us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 7s 250us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 7s 253us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 7s 241us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "10 1.0 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 21s 704us/step - loss: 381.8651 - acc: 0.2024 - val_loss: 321.7668 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 8s 258us/step - loss: nan - acc: 0.2481 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 7s 245us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 7s 250us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 7s 248us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 7s 235us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "10 0.1 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 21s 694us/step - loss: nan - acc: 0.2515 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 7s 240us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 7s 248us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 7s 252us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 7s 239us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "10 0.1 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 22s 757us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 7s 235us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 7s 232us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 7s 239us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 7s 252us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "10 0.1 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 21s 722us/step - loss: 381.2131 - acc: 0.2251 - val_loss: 323.6050 - val_acc: 0.1983\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 8s 270us/step - loss: nan - acc: 0.2227 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 8s 262us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 7s 242us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 7s 242us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 7s 240us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "10 0.01 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 22s 726us/step - loss: nan - acc: 0.2515 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 7s 244us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 7s 228us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 7s 222us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 7s 222us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "10 0.01 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 21s 704us/step - loss: nan - acc: 0.2517 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 8s 281us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29639/29639 [==============================] - 7s 246us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 7s 234us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 7s 232us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "10 0.01 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 23s 768us/step - loss: 380.4083 - acc: 0.2481 - val_loss: 326.7795 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 7s 251us/step - loss: nan - acc: 0.2502 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 7s 239us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 7s 239us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 7s 234us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 7s 240us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "10 0.001 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 22s 756us/step - loss: nan - acc: 0.2517 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 8s 259us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 7s 236us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 7s 223us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 7s 242us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "10 0.001 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 22s 743us/step - loss: nan - acc: 0.2517 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 9s 315us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 7s 248us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 7s 240us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 8s 254us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "10 0.001 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 22s 736us/step - loss: 378.3762 - acc: 0.2443 - val_loss: 323.1485 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 8s 281us/step - loss: 342.1102 - acc: 0.2480 - val_loss: 347.4699 - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 8s 259us/step - loss: 342.1320 - acc: 0.2480 - val_loss: 339.1854 - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 8s 260us/step - loss: 342.0595 - acc: 0.2497 - val_loss: 335.4046 - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 7s 240us/step - loss: 342.0391 - acc: 0.2479 - val_loss: 361.5632 - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 7s 249us/step - loss: 342.0222 - acc: 0.2484 - val_loss: 318.0813 - val_acc: 0.2478\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 7s 251us/step - loss: 341.9769 - acc: 0.2466 - val_loss: 343.4741 - val_acc: 0.2478\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 7s 248us/step - loss: 341.9823 - acc: 0.2500 - val_loss: 322.6616 - val_acc: 0.2478\n",
      "Epoch 9/80\n",
      "29639/29639 [==============================] - 7s 223us/step - loss: nan - acc: 0.2476 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 10/80\n",
      "29639/29639 [==============================] - 7s 224us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 11/80\n",
      "29639/29639 [==============================] - 7s 234us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "1.0 10 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 22s 736us/step - loss: nan - acc: 0.2517 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 8s 271us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 8s 257us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 8s 277us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 7s 250us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "1.0 10 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 22s 743us/step - loss: nan - acc: 0.2515 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 8s 278us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 8s 271us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 8s 269us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 7s 226us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "1.0 10 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 22s 750us/step - loss: nan - acc: 0.2268 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 7s 239us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 7s 232us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 7s 242us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 7s 240us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "1.0 1.0 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 24s 825us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 7s 251us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 7s 228us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 7s 225us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 7s 231us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "1.0 1.0 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 22s 731us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 9s 290us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 9s 290us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 9s 302us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 8s 262us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "1.0 1.0 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 22s 751us/step - loss: 42.5927 - acc: 0.1993 - val_loss: 35.5287 - val_acc: 0.2342\n",
      "Epoch 2/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29639/29639 [==============================] - 9s 294us/step - loss: 38.5230 - acc: 0.2026 - val_loss: 39.0359 - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 8s 264us/step - loss: 38.5011 - acc: 0.2024 - val_loss: 36.4916 - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 8s 272us/step - loss: 38.4562 - acc: 0.2047 - val_loss: 37.3531 - val_acc: 0.1567\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 7s 248us/step - loss: 38.4970 - acc: 0.1964 - val_loss: 39.4567 - val_acc: 0.1983\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 7s 251us/step - loss: 38.4887 - acc: 0.2010 - val_loss: 34.5088 - val_acc: 0.2478\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 8s 279us/step - loss: nan - acc: 0.2245 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 8s 273us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 9/80\n",
      "29639/29639 [==============================] - 8s 257us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 10/80\n",
      "29639/29639 [==============================] - 8s 281us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 11/80\n",
      "29639/29639 [==============================] - 9s 296us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "1.0 0.1 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 23s 769us/step - loss: nan - acc: 0.2518 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 8s 259us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 8s 267us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 9s 296us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 8s 275us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "1.0 0.1 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 23s 781us/step - loss: nan - acc: 0.2517 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 9s 292us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 8s 257us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 8s 259us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 8s 259us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "1.0 0.1 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 23s 789us/step - loss: 41.1874 - acc: 0.2213 - val_loss: 34.7638 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 8s 268us/step - loss: 37.3973 - acc: 0.2210 - val_loss: 37.3061 - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 8s 275us/step - loss: 37.4565 - acc: 0.2227 - val_loss: 35.2804 - val_acc: 0.1567\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 8s 263us/step - loss: nan - acc: 0.2349 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 8s 269us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 7s 244us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "1.0 0.01 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 22s 754us/step - loss: nan - acc: 0.2513 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 8s 274us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 8s 267us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 9s 297us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 8s 280us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "1.0 0.01 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 24s 802us/step - loss: nan - acc: 0.2517 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 7s 247us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 7s 232us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 7s 233us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 7s 241us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "1.0 0.01 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 22s 743us/step - loss: nan - acc: 0.2476 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 7s 247us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 7s 252us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 8s 262us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 7s 253us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "1.0 0.001 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 23s 783us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 9s 320us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 9s 311us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 8s 283us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 8s 276us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "1.0 0.001 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 22s 750us/step - loss: nan - acc: 0.2517 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 9s 299us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 9s 296us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 9s 298us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 9s 297us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "1.0 0.001 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 22s 752us/step - loss: 40.6576 - acc: 0.2493 - val_loss: 34.1180 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 9s 290us/step - loss: nan - acc: 0.2500 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 9s 299us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 8s 284us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 9s 297us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 9s 298us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1 10 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 22s 745us/step - loss: nan - acc: 0.2514 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 9s 297us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 8s 285us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 9s 290us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 9s 298us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.1 10 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 23s 771us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 9s 299us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 9s 290us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 8s 279us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 9s 298us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.1 10 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 23s 776us/step - loss: 7.8711 - acc: 0.2017 - val_loss: 5.9553 - val_acc: 0.1983\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 9s 299us/step - loss: nan - acc: 0.2326 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 9s 296us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 8s 280us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 9s 289us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 9s 299us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.1 1.0 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 23s 775us/step - loss: nan - acc: 0.2517 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 9s 301us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 8s 282us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 8s 281us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 9s 298us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.1 1.0 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 23s 774us/step - loss: nan - acc: 0.2515 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 9s 300us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 8s 286us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 8s 282us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 9s 288us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.1 1.0 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 22s 759us/step - loss: 6.8440 - acc: 0.1977 - val_loss: 4.9014 - val_acc: 0.2342\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 9s 304us/step - loss: nan - acc: 0.2471 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 9s 295us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 9s 300us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 9s 301us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 9s 293us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.1 0.1 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 23s 776us/step - loss: nan - acc: 0.2515 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 9s 293us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 9s 294us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 9s 300us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 9s 293us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.1 0.1 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 23s 759us/step - loss: nan - acc: 0.2515 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 8s 270us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 8s 282us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 9s 296us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 9s 302us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.1 0.1 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 23s 761us/step - loss: nan - acc: 0.2382 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 10s 328us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 10s 324us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 7s 253us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 8s 258us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.1 0.01 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 23s 777us/step - loss: nan - acc: 0.2515 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 8s 258us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 8s 267us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 8s 264us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 8s 263us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.1 0.01 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 24s 826us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 8s 254us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 8s 253us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 9s 312us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29639/29639 [==============================] - 8s 267us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.1 0.01 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 24s 796us/step - loss: 6.8782 - acc: 0.2462 - val_loss: 4.7757 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 8s 285us/step - loss: 6.3902 - acc: 0.2472 - val_loss: 5.2312 - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 8s 265us/step - loss: 6.4401 - acc: 0.2483 - val_loss: 5.0073 - val_acc: 0.2342\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 8s 259us/step - loss: 6.4424 - acc: 0.2472 - val_loss: 5.1097 - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 8s 254us/step - loss: 6.4277 - acc: 0.2461 - val_loss: 5.2396 - val_acc: 0.2342\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 8s 269us/step - loss: 6.3954 - acc: 0.2485 - val_loss: 4.8431 - val_acc: 0.2342\n",
      "0.1 0.001 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 25s 840us/step - loss: nan - acc: 0.2514 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 8s 276us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 8s 268us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 8s 270us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 8s 274us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.1 0.001 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 25s 832us/step - loss: nan - acc: 0.2515 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 9s 320us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 9s 300us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 10s 331us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 9s 296us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.1 0.001 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 24s 812us/step - loss: 6.7209 - acc: 0.2476 - val_loss: 4.8269 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 9s 307us/step - loss: 6.3351 - acc: 0.2472 - val_loss: 5.0323 - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 9s 302us/step - loss: 6.3178 - acc: 0.2485 - val_loss: 4.9387 - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 7s 249us/step - loss: 6.3114 - acc: 0.2474 - val_loss: 4.8726 - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 7s 242us/step - loss: 6.3434 - acc: 0.2492 - val_loss: 5.1909 - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 7s 242us/step - loss: nan - acc: 0.2500 - val_loss: nan - val_acc: 0.2478\n",
      "0.01 10 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 24s 819us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 9s 315us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 10s 339us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 8s 277us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 8s 275us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.01 10 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 23s 789us/step - loss: nan - acc: 0.2517 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 8s 264us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 8s 256us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 8s 270us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 8s 259us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.01 10 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 25s 854us/step - loss: 4.0774 - acc: 0.2020 - val_loss: 2.5251 - val_acc: 0.1567\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 8s 253us/step - loss: 3.9868 - acc: 0.2003 - val_loss: 2.5757 - val_acc: 0.2342\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 8s 254us/step - loss: 4.0847 - acc: 0.2054 - val_loss: 2.7901 - val_acc: 0.2342\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 7s 253us/step - loss: 4.0604 - acc: 0.2052 - val_loss: 2.8201 - val_acc: 0.1567\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 8s 254us/step - loss: nan - acc: 0.2376 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 7s 253us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.01 1.0 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 23s 791us/step - loss: nan - acc: 0.2515 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 8s 274us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 8s 282us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 7s 251us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 8s 273us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.01 1.0 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 24s 798us/step - loss: nan - acc: 0.2515 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 8s 254us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 7s 244us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 7s 244us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 8s 257us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.01 1.0 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 23s 781us/step - loss: nan - acc: 0.2163 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 9s 299us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 8s 259us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 8s 256us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 8s 255us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.01 0.1 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 23s 778us/step - loss: nan - acc: 0.2515 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29639/29639 [==============================] - 8s 275us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 8s 266us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 8s 258us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 8s 260us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.01 0.1 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 23s 769us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 8s 258us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 8s 258us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 8s 268us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 7s 245us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.01 0.1 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 24s 798us/step - loss: 3.3775 - acc: 0.2225 - val_loss: 1.9412 - val_acc: 0.2342\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 8s 262us/step - loss: 3.2967 - acc: 0.2259 - val_loss: 1.9578 - val_acc: 0.1983\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 8s 287us/step - loss: 3.3174 - acc: 0.2168 - val_loss: 1.9524 - val_acc: 0.1630\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 8s 262us/step - loss: 3.3951 - acc: 0.2211 - val_loss: 1.9448 - val_acc: 0.1630\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 8s 256us/step - loss: 3.3363 - acc: 0.2269 - val_loss: 1.9740 - val_acc: 0.1983\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 8s 259us/step - loss: 3.3085 - acc: 0.2214 - val_loss: 1.9324 - val_acc: 0.2478\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 8s 255us/step - loss: 3.3210 - acc: 0.2189 - val_loss: 1.9573 - val_acc: 0.2478\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 8s 270us/step - loss: 3.3388 - acc: 0.2273 - val_loss: 1.9368 - val_acc: 0.1567\n",
      "Epoch 9/80\n",
      "29639/29639 [==============================] - 9s 302us/step - loss: 3.2969 - acc: 0.2253 - val_loss: 1.9611 - val_acc: 0.2342\n",
      "Epoch 10/80\n",
      "29639/29639 [==============================] - 8s 276us/step - loss: nan - acc: 0.2377 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 11/80\n",
      "29639/29639 [==============================] - 8s 275us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.01 0.01 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 25s 834us/step - loss: nan - acc: 0.2515 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 9s 312us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 8s 269us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 9s 319us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 9s 319us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.01 0.01 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 25s 850us/step - loss: nan - acc: 0.2515 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 9s 293us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 8s 277us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 8s 268us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 9s 296us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.01 0.01 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 24s 794us/step - loss: 3.3330 - acc: 0.2469 - val_loss: 1.9219 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 8s 258us/step - loss: 3.2880 - acc: 0.2482 - val_loss: 1.9465 - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 8s 260us/step - loss: 3.2900 - acc: 0.2481 - val_loss: 1.9365 - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 8s 269us/step - loss: 3.2809 - acc: 0.2484 - val_loss: 1.9316 - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 9s 290us/step - loss: nan - acc: 0.2501 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 9s 288us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.01 0.001 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 25s 835us/step - loss: nan - acc: 0.2515 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 8s 265us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 8s 261us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 8s 262us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 8s 267us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.01 0.001 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 25s 854us/step - loss: nan - acc: 0.2515 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 10s 351us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 9s 302us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 8s 258us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 8s 263us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.01 0.001 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 24s 822us/step - loss: 3.3780 - acc: 0.2465 - val_loss: 1.9257 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 8s 263us/step - loss: 3.3004 - acc: 0.2466 - val_loss: 1.9585 - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 8s 269us/step - loss: 3.2885 - acc: 0.2497 - val_loss: 1.9330 - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 8s 273us/step - loss: 3.3049 - acc: 0.2478 - val_loss: 1.9365 - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 9s 311us/step - loss: 3.3208 - acc: 0.2495 - val_loss: 1.9568 - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 8s 278us/step - loss: nan - acc: 0.2504 - val_loss: nan - val_acc: 0.2478\n",
      "0.001 10 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 25s 828us/step - loss: nan - acc: 0.2518 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 9s 301us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 8s 271us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 8s 265us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29639/29639 [==============================] - 8s 264us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.001 10 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 25s 830us/step - loss: nan - acc: 0.2515 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 9s 303us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 8s 257us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 8s 266us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 8s 269us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.001 10 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 25s 848us/step - loss: 4.6747 - acc: 0.2309 - val_loss: 3.4686 - val_acc: 0.2342\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 8s 275us/step - loss: nan - acc: 0.2438 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 8s 264us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 8s 261us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 8s 269us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 8s 266us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.001 1.0 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 26s 880us/step - loss: nan - acc: 0.2517 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 10s 336us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 9s 291us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 9s 288us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 8s 278us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.001 1.0 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 25s 848us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 8s 281us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 8s 281us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 8s 276us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 8s 282us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.001 1.0 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 25s 859us/step - loss: 3.0937 - acc: 0.2281 - val_loss: 1.7269 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 9s 304us/step - loss: 3.1341 - acc: 0.2259 - val_loss: 1.7229 - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 9s 290us/step - loss: 3.1229 - acc: 0.2290 - val_loss: 1.7373 - val_acc: 0.2342\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 9s 290us/step - loss: 3.0993 - acc: 0.2289 - val_loss: 1.7462 - val_acc: 0.1630\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 9s 290us/step - loss: nan - acc: 0.2306 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 8s 278us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 8s 286us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.001 0.1 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 27s 912us/step - loss: nan - acc: 0.2517 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 8s 276us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 9s 299us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 8s 277us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 9s 295us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.001 0.1 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 26s 866us/step - loss: nan - acc: 0.2517 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 10s 330us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 10s 326us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 9s 312us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 9s 308us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.001 0.1 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 25s 832us/step - loss: nan - acc: 0.2341 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 10s 324us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 10s 322us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 10s 340us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 10s 322us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.001 0.01 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 25s 836us/step - loss: nan - acc: 0.2514 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 9s 318us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 9s 317us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 10s 321us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 9s 318us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.001 0.01 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 25s 842us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 8s 281us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 8s 263us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 8s 258us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 8s 262us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.001 0.01 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 25s 847us/step - loss: 3.0200 - acc: 0.2465 - val_loss: 1.6323 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 8s 283us/step - loss: 3.0024 - acc: 0.2486 - val_loss: 1.6314 - val_acc: 0.2478\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 9s 291us/step - loss: nan - acc: 0.2518 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 9s 300us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 9s 298us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 9s 295us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.001 0.001 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 24s 823us/step - loss: nan - acc: 0.2515 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 8s 279us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 8s 273us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 9s 295us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 9s 295us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.001 0.001 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 25s 847us/step - loss: nan - acc: 0.2517 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 10s 329us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 9s 305us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 8s 264us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 8s 280us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.001 0.001 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 25s 842us/step - loss: 2.9823 - acc: 0.2485 - val_loss: 1.6258 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 9s 319us/step - loss: 2.9830 - acc: 0.2490 - val_loss: 1.6267 - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 10s 321us/step - loss: nan - acc: 0.2506 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 8s 270us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 8s 276us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 8s 274us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "10 10 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 28s 939us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 10s 353us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 10s 341us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 10s 340us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 9s 305us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "10 10 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 26s 889us/step - loss: nan - acc: 0.2515 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 9s 315us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 9s 307us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 9s 306us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 8s 283us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "10 10 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 27s 927us/step - loss: nan - acc: 0.2254 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 10s 327us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 9s 307us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 9s 319us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 9s 314us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "10 1.0 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 29s 969us/step - loss: nan - acc: 0.2517 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 9s 314us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 9s 300us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 10s 328us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 9s 302us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "10 1.0 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 27s 912us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 9s 306us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 10s 329us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 9s 319us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 9s 300us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "10 1.0 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 27s 905us/step - loss: 376.7398 - acc: 0.1991 - val_loss: 321.5081 - val_acc: 0.1567\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 10s 333us/step - loss: 341.0971 - acc: 0.2004 - val_loss: 344.9660 - val_acc: 0.2342\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 9s 306us/step - loss: 341.0709 - acc: 0.2026 - val_loss: 336.3135 - val_acc: 0.2342\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 9s 287us/step - loss: 341.0018 - acc: 0.1997 - val_loss: 334.4601 - val_acc: 0.1630\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 9s 291us/step - loss: nan - acc: 0.2261 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 9s 288us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "10 0.1 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 26s 889us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 10s 331us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 10s 324us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 10s 339us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 9s 294us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "10 0.1 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29639/29639 [==============================] - 27s 910us/step - loss: nan - acc: 0.2517 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 9s 307us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 10s 328us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 9s 309us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 9s 307us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "10 0.1 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 30s 1ms/step - loss: nan - acc: 0.2481 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 10s 335us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 9s 309us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 9s 309us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 9s 296us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "10 0.01 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 27s 918us/step - loss: nan - acc: 0.2514 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 11s 356us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 9s 316us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 9s 312us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 9s 306us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "10 0.01 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 29s 983us/step - loss: nan - acc: 0.2515 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 9s 315us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 9s 297us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 9s 307us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 9s 314us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "10 0.01 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 27s 900us/step - loss: nan - acc: 0.2501 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 9s 310us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 9s 293us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 9s 290us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 9s 296us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "10 0.001 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 27s 897us/step - loss: nan - acc: 0.2514 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 11s 357us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 11s 357us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 10s 354us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 10s 346us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "10 0.001 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 27s 918us/step - loss: nan - acc: 0.2515 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 9s 290us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 9s 297us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 9s 293us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 9s 290us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "10 0.001 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 28s 959us/step - loss: 377.6809 - acc: 0.2471 - val_loss: 321.7864 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 10s 346us/step - loss: 342.8790 - acc: 0.2469 - val_loss: 347.1244 - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 9s 308us/step - loss: 342.8925 - acc: 0.2477 - val_loss: 343.2587 - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 9s 312us/step - loss: 342.8529 - acc: 0.2476 - val_loss: 335.2046 - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 9s 299us/step - loss: nan - acc: 0.2508 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 9s 301us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "1.0 10 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 27s 909us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 9s 317us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 9s 304us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 10s 324us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 10s 333us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "1.0 10 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 30s 1ms/step - loss: nan - acc: 0.2515 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 10s 347us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 9s 320us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 10s 321us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 9s 310us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "1.0 10 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 27s 927us/step - loss: 42.9110 - acc: 0.1993 - val_loss: 35.7649 - val_acc: 0.2342\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 11s 384us/step - loss: 39.3254 - acc: 0.1994 - val_loss: 38.2726 - val_acc: 0.1630\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 9s 318us/step - loss: 39.3370 - acc: 0.1974 - val_loss: 38.8846 - val_acc: 0.2342\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 9s 309us/step - loss: 39.3461 - acc: 0.1961 - val_loss: 37.1607 - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 9s 310us/step - loss: 39.3021 - acc: 0.2015 - val_loss: 40.0540 - val_acc: 0.1567\n",
      "Epoch 6/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29639/29639 [==============================] - 10s 322us/step - loss: 39.3394 - acc: 0.2042 - val_loss: 35.1543 - val_acc: 0.2478\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 9s 309us/step - loss: 39.3475 - acc: 0.2007 - val_loss: 39.4982 - val_acc: 0.2342\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 10s 351us/step - loss: 39.3291 - acc: 0.2014 - val_loss: 36.4191 - val_acc: 0.1567\n",
      "Epoch 9/80\n",
      "29639/29639 [==============================] - 10s 332us/step - loss: 39.3387 - acc: 0.2078 - val_loss: 38.9050 - val_acc: 0.2342\n",
      "Epoch 10/80\n",
      "29639/29639 [==============================] - 10s 337us/step - loss: 39.3228 - acc: 0.2005 - val_loss: 38.0135 - val_acc: 0.2478\n",
      "Epoch 11/80\n",
      "29639/29639 [==============================] - 9s 314us/step - loss: nan - acc: 0.2258 - val_loss: nan - val_acc: 0.2478\n",
      "1.0 1.0 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 29s 974us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 10s 335us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 10s 342us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 10s 335us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 9s 316us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "1.0 1.0 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 28s 946us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 11s 383us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 10s 339us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 10s 341us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 10s 328us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "1.0 1.0 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 28s 948us/step - loss: 40.3646 - acc: 0.1978 - val_loss: 33.9125 - val_acc: 0.1567\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 11s 359us/step - loss: nan - acc: 0.2314 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 9s 314us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 10s 326us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 10s 323us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 9s 306us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "1.0 0.1 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 28s 961us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 11s 372us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 9s 318us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 9s 317us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 9s 298us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "1.0 0.1 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 30s 1ms/step - loss: nan - acc: 0.2517 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 10s 326us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 10s 343us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 10s 326us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 10s 338us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "1.0 0.1 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 28s 954us/step - loss: nan - acc: 0.2448 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 10s 321us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 10s 325us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 11s 358us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 11s 367us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "1.0 0.01 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 29s 971us/step - loss: nan - acc: 0.2517 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 11s 376us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 11s 378us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 11s 356us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 11s 359us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "1.0 0.01 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 30s 1ms/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 10s 323us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 10s 328us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 9s 318us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 9s 317us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "1.0 0.01 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 29s 968us/step - loss: nan - acc: 0.2502 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 10s 329us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 10s 324us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 10s 337us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 9s 319us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "1.0 0.001 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 29s 982us/step - loss: nan - acc: 0.2517 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 12s 389us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 11s 385us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 11s 387us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29639/29639 [==============================] - 12s 388us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "1.0 0.001 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 29s 984us/step - loss: nan - acc: 0.2517 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 11s 387us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 12s 418us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 12s 393us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 10s 349us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "1.0 0.001 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 30s 997us/step - loss: 39.8979 - acc: 0.2469 - val_loss: 33.8302 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 12s 392us/step - loss: nan - acc: 0.2475 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 11s 381us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 11s 387us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 12s 389us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 12s 396us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.1 10 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 31s 1ms/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 12s 394us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 11s 382us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 12s 402us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 12s 404us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.1 10 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 30s 1ms/step - loss: nan - acc: 0.2517 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 11s 383us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 12s 401us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 12s 396us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 11s 382us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.1 10 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 31s 1ms/step - loss: 11.3801 - acc: 0.2015 - val_loss: 9.4306 - val_acc: 0.1983\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 12s 405us/step - loss: 10.9070 - acc: 0.2001 - val_loss: 9.7899 - val_acc: 0.2342\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 12s 403us/step - loss: 10.8397 - acc: 0.1987 - val_loss: 8.6447 - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 12s 401us/step - loss: 10.8450 - acc: 0.1994 - val_loss: 9.1578 - val_acc: 0.1983\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 12s 405us/step - loss: 10.8815 - acc: 0.2017 - val_loss: 11.2014 - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 12s 403us/step - loss: 10.8778 - acc: 0.1970 - val_loss: 10.7786 - val_acc: 0.1983\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 12s 406us/step - loss: 10.8604 - acc: 0.2003 - val_loss: 10.0718 - val_acc: 0.1983\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 12s 398us/step - loss: 10.8616 - acc: 0.2038 - val_loss: 7.5856 - val_acc: 0.1630\n",
      "Epoch 9/80\n",
      "29639/29639 [==============================] - 12s 403us/step - loss: 10.8305 - acc: 0.1949 - val_loss: 9.5918 - val_acc: 0.1630\n",
      "Epoch 10/80\n",
      "29639/29639 [==============================] - 12s 397us/step - loss: 10.8823 - acc: 0.1982 - val_loss: 9.7059 - val_acc: 0.1630\n",
      "Epoch 11/80\n",
      "29639/29639 [==============================] - 12s 405us/step - loss: 10.8499 - acc: 0.1980 - val_loss: 8.4881 - val_acc: 0.2478\n",
      "Epoch 12/80\n",
      "29639/29639 [==============================] - 12s 405us/step - loss: nan - acc: 0.2191 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 13/80\n",
      "29639/29639 [==============================] - 12s 392us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.1 1.0 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 31s 1ms/step - loss: nan - acc: 0.2515 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 12s 394us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 10s 324us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 9s 319us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 9s 317us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.1 1.0 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 30s 1ms/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 11s 384us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 11s 385us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 11s 386us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 11s 386us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.1 1.0 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 30s 1ms/step - loss: nan - acc: 0.2410 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 11s 376us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 11s 376us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 11s 358us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 10s 322us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.1 0.1 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 31s 1ms/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 11s 387us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 12s 412us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 10s 354us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 10s 336us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.1 0.1 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 30s 1ms/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 12s 401us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 12s 401us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 14s 458us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 12s 401us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.1 0.1 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 32s 1ms/step - loss: 6.7603 - acc: 0.2249 - val_loss: 4.8566 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 11s 356us/step - loss: 6.3861 - acc: 0.2194 - val_loss: 5.0639 - val_acc: 0.1567\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 10s 341us/step - loss: nan - acc: 0.2351 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 10s 352us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 11s 362us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 10s 354us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.1 0.01 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 31s 1ms/step - loss: nan - acc: 0.2517 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 11s 355us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 11s 360us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 10s 347us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 11s 358us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.1 0.01 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 32s 1ms/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 11s 363us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 11s 368us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 11s 355us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 10s 344us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.1 0.01 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 31s 1ms/step - loss: nan - acc: 0.2498 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 10s 340us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 10s 348us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 10s 351us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 10s 354us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.1 0.001 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 32s 1ms/step - loss: nan - acc: 0.2513 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 11s 364us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 11s 358us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 11s 355us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 10s 343us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.1 0.001 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 32s 1ms/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 11s 372us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 10s 337us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 10s 351us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 11s 355us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.1 0.001 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 30s 1ms/step - loss: 6.7087 - acc: 0.2489 - val_loss: 4.7954 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 12s 396us/step - loss: 6.2974 - acc: 0.2488 - val_loss: 5.0304 - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 12s 399us/step - loss: 6.3526 - acc: 0.2482 - val_loss: 4.9889 - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 10s 347us/step - loss: 6.2978 - acc: 0.2512 - val_loss: 4.9054 - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 10s 342us/step - loss: 6.3346 - acc: 0.2482 - val_loss: 5.2145 - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 10s 337us/step - loss: 6.2869 - acc: 0.2477 - val_loss: 4.6944 - val_acc: 0.2342\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 10s 326us/step - loss: nan - acc: 0.2446 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 10s 338us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 9/80\n",
      "29639/29639 [==============================] - 10s 339us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 10/80\n",
      "29639/29639 [==============================] - 10s 329us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 11/80\n",
      "29639/29639 [==============================] - 10s 342us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.01 10 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 30s 1ms/step - loss: nan - acc: 0.2515 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 8s 282us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 8s 284us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 8s 284us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 9s 291us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.01 10 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 29s 988us/step - loss: nan - acc: 0.2517 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 10s 328us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 10s 333us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 10s 345us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 10s 335us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.01 10 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29639/29639 [==============================] - 29s 990us/step - loss: nan - acc: 0.2327 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 9s 294us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 9s 290us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 9s 292us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 9s 292us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.01 1.0 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 29s 994us/step - loss: nan - acc: 0.2513 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 9s 302us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 8s 283us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 8s 284us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 9s 298us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.01 1.0 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 28s 958us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 10s 351us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 12s 416us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 11s 388us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 11s 387us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.01 1.0 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 30s 997us/step - loss: 3.6596 - acc: 0.2006 - val_loss: 2.1614 - val_acc: 0.2342\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 8s 282us/step - loss: 3.5626 - acc: 0.2018 - val_loss: 2.1822 - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 8s 282us/step - loss: 3.6031 - acc: 0.2081 - val_loss: 2.3053 - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 9s 292us/step - loss: 3.5502 - acc: 0.2057 - val_loss: 2.2073 - val_acc: 0.1567\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 9s 288us/step - loss: 3.5840 - acc: 0.2012 - val_loss: 2.1994 - val_acc: 0.1567\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 8s 287us/step - loss: nan - acc: 0.2457 - val_loss: nan - val_acc: 0.2478\n",
      "0.01 0.1 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 29s 989us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 9s 308us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 9s 300us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 9s 309us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 9s 293us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.01 0.1 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 28s 949us/step - loss: nan - acc: 0.2517 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 9s 311us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 10s 334us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 10s 334us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 9s 297us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.01 0.1 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 30s 996us/step - loss: 3.3642 - acc: 0.2279 - val_loss: 1.9600 - val_acc: 0.1983\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 9s 303us/step - loss: 3.3615 - acc: 0.2294 - val_loss: 1.9851 - val_acc: 0.1567\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 9s 311us/step - loss: 3.3630 - acc: 0.2164 - val_loss: 1.9949 - val_acc: 0.2342\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 9s 288us/step - loss: 3.3993 - acc: 0.2228 - val_loss: 1.9788 - val_acc: 0.1983\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 9s 288us/step - loss: 3.3722 - acc: 0.2204 - val_loss: 1.9979 - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 9s 310us/step - loss: 3.3055 - acc: 0.2246 - val_loss: 1.9550 - val_acc: 0.1983\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 10s 325us/step - loss: nan - acc: 0.2438 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 8/80\n",
      "29639/29639 [==============================] - 11s 363us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 9/80\n",
      "29639/29639 [==============================] - 9s 303us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 10/80\n",
      "29639/29639 [==============================] - 9s 301us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 11/80\n",
      "29639/29639 [==============================] - 9s 302us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.01 0.01 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 30s 1ms/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 9s 293us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 9s 293us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 9s 319us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 11s 374us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.01 0.01 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 31s 1ms/step - loss: nan - acc: 0.2515 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 10s 337us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 10s 341us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 11s 364us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 10s 351us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.01 0.01 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 31s 1ms/step - loss: 3.3419 - acc: 0.2479 - val_loss: 1.9225 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 11s 358us/step - loss: 3.3269 - acc: 0.2478 - val_loss: 1.9516 - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 11s 373us/step - loss: 3.3086 - acc: 0.2473 - val_loss: 1.9421 - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 11s 361us/step - loss: nan - acc: 0.2483 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29639/29639 [==============================] - 11s 361us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 11s 360us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.01 0.001 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 34s 1ms/step - loss: nan - acc: 0.2517 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 11s 368us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 11s 379us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 14s 458us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 13s 447us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.01 0.001 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 33s 1ms/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 11s 373us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 11s 373us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 11s 374us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 11s 379us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.01 0.001 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 33s 1ms/step - loss: nan - acc: 0.2476 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 11s 372us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 11s 362us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 11s 363us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 11s 368us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.001 10 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 36s 1ms/step - loss: nan - acc: 0.2515 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 11s 379us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 11s 369us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 10s 347us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 11s 366us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.001 10 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 33s 1ms/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 12s 407us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 11s 356us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 11s 356us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 11s 375us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.001 10 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 33s 1ms/step - loss: 5.5870 - acc: 0.2371 - val_loss: 4.0332 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 12s 415us/step - loss: 5.5223 - acc: 0.2382 - val_loss: 3.6245 - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 11s 383us/step - loss: 5.5074 - acc: 0.2385 - val_loss: 4.1434 - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 11s 373us/step - loss: 5.5259 - acc: 0.2427 - val_loss: 4.8320 - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 10s 344us/step - loss: 5.5196 - acc: 0.2375 - val_loss: 4.7317 - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 11s 367us/step - loss: 5.5274 - acc: 0.2387 - val_loss: 4.2820 - val_acc: 0.2478\n",
      "Epoch 7/80\n",
      "29639/29639 [==============================] - 12s 401us/step - loss: 5.5181 - acc: 0.2387 - val_loss: 4.7269 - val_acc: 0.2478\n",
      "0.001 1.0 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 34s 1ms/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 12s 400us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 14s 473us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 11s 379us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 11s 377us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.001 1.0 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 35s 1ms/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 13s 429us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 13s 444us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 11s 376us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 11s 359us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.001 1.0 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 34s 1ms/step - loss: nan - acc: 0.2404 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 12s 392us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 10s 332us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 9s 315us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 10s 334us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.001 0.1 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 31s 1ms/step - loss: nan - acc: 0.2515 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 9s 317us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 9s 307us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 9s 296us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 9s 303us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.001 0.1 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 31s 1ms/step - loss: nan - acc: 0.2517 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 11s 359us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 11s 354us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 10s 352us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 10s 342us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.001 0.1 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 31s 1ms/step - loss: 3.0948 - acc: 0.2327 - val_loss: 1.6669 - val_acc: 0.2342\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 9s 306us/step - loss: 3.0154 - acc: 0.2441 - val_loss: 1.6691 - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 9s 319us/step - loss: 3.0738 - acc: 0.2396 - val_loss: 1.6788 - val_acc: 0.1983\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 9s 300us/step - loss: 3.0808 - acc: 0.2374 - val_loss: 1.6744 - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 9s 299us/step - loss: 3.0688 - acc: 0.2344 - val_loss: 1.6766 - val_acc: 0.2342\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 9s 301us/step - loss: nan - acc: 0.2494 - val_loss: nan - val_acc: 0.2478\n",
      "0.001 0.01 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 32s 1ms/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 10s 323us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 12s 401us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 9s 308us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 9s 301us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.001 0.01 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 32s 1ms/step - loss: nan - acc: 0.2518 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 10s 332us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 9s 305us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 9s 306us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 10s 327us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.001 0.01 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 35s 1ms/step - loss: 3.0490 - acc: 0.2464 - val_loss: 1.6366 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 11s 372us/step - loss: 3.0382 - acc: 0.2454 - val_loss: 1.6398 - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 12s 413us/step - loss: nan - acc: 0.2515 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 10s 333us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 10s 329us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 10s 326us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.001 0.001 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 32s 1ms/step - loss: nan - acc: 0.2514 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 10s 323us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 10s 342us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 9s 302us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 10s 348us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.001 0.001 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 33s 1ms/step - loss: nan - acc: 0.2517 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 14s 457us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 13s 428us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 14s 473us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 12s 389us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "0.001 0.001 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 36s 1ms/step - loss: 2.9816 - acc: 0.2458 - val_loss: 1.6279 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 11s 375us/step - loss: nan - acc: 0.2448 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 11s 381us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 11s 379us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 11s 384us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 12s 414us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "10 10 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 39s 1ms/step - loss: nan - acc: 0.2515 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 10s 350us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 10s 350us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 10s 346us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 10s 348us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "10 10 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 33s 1ms/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 11s 356us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 11s 355us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 11s 356us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 10s 353us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "10 10 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 34s 1ms/step - loss: 381.9382 - acc: 0.2016 - val_loss: 323.7697 - val_acc: 0.1567\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 10s 343us/step - loss: 345.7525 - acc: 0.2011 - val_loss: 354.8026 - val_acc: 0.1567\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 10s 339us/step - loss: 345.5895 - acc: 0.2011 - val_loss: 347.5316 - val_acc: 0.2342\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 10s 345us/step - loss: nan - acc: 0.2456 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 10s 340us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 10s 341us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "10 1.0 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 33s 1ms/step - loss: nan - acc: 0.2514 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 10s 340us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 10s 336us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 10s 343us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 10s 332us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "10 1.0 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 33s 1ms/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 10s 336us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 10s 341us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 10s 340us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 10s 349us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "10 1.0 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 33s 1ms/step - loss: 378.7834 - acc: 0.1995 - val_loss: 323.8827 - val_acc: 0.1567\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 10s 345us/step - loss: 343.0841 - acc: 0.2083 - val_loss: 344.5249 - val_acc: 0.2342\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 10s 341us/step - loss: 343.0412 - acc: 0.2042 - val_loss: 337.4958 - val_acc: 0.1983\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 10s 349us/step - loss: nan - acc: 0.2472 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 10s 345us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 10s 338us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "10 0.1 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 33s 1ms/step - loss: nan - acc: 0.2515 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 10s 339us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 10s 341us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 10s 334us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 10s 352us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "10 0.1 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 33s 1ms/step - loss: nan - acc: 0.2517 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 10s 347us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 10s 344us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 10s 335us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 10s 339us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "10 0.1 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 33s 1ms/step - loss: nan - acc: 0.2271 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 10s 348us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 10s 350us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 10s 342us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 10s 342us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "10 0.01 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 33s 1ms/step - loss: nan - acc: 0.2514 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 10s 343us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 10s 348us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 10s 351us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 10s 349us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "10 0.01 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 33s 1ms/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 10s 352us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 10s 348us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 10s 344us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 11s 355us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "10 0.01 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 33s 1ms/step - loss: nan - acc: 0.2511 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 10s 351us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 10s 345us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 10s 344us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 10s 348us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "10 0.001 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 34s 1ms/step - loss: nan - acc: 0.2514 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 10s 349us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 10s 344us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 10s 350us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 10s 347us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "10 0.001 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 34s 1ms/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 11s 355us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 10s 345us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 10s 346us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 10s 347us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "10 0.001 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 34s 1ms/step - loss: 391.1533 - acc: 0.2438 - val_loss: 325.3380 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 10s 350us/step - loss: 352.7792 - acc: 0.2461 - val_loss: 364.4195 - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 10s 345us/step - loss: nan - acc: 0.2488 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 10s 350us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 10s 354us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 10s 348us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "1.0 10 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 34s 1ms/step - loss: nan - acc: 0.2514 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 10s 352us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 10s 344us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 11s 357us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 10s 351us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "1.0 10 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 34s 1ms/step - loss: nan - acc: 0.2515 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 11s 359us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 11s 356us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 10s 354us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 11s 358us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "1.0 10 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 34s 1ms/step - loss: 42.9339 - acc: 0.1994 - val_loss: 36.0467 - val_acc: 0.1567\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 10s 353us/step - loss: nan - acc: 0.2409 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 11s 355us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 10s 344us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 10s 342us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 10s 344us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "1.0 1.0 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 34s 1ms/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 10s 340us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 10s 342us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 10s 345us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 10s 344us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "1.0 1.0 [0.2, 0.9]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 33s 1ms/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 10s 343us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 10s 345us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 10s 344us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 10s 344us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "1.0 1.0 [0.1, 0.1]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 34s 1ms/step - loss: 40.7744 - acc: 0.2029 - val_loss: 33.7196 - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 10s 343us/step - loss: 37.2094 - acc: 0.1955 - val_loss: 36.4510 - val_acc: 0.1630\n",
      "Epoch 3/80\n",
      "29639/29639 [==============================] - 11s 372us/step - loss: nan - acc: 0.2440 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 4/80\n",
      "29639/29639 [==============================] - 11s 372us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 5/80\n",
      "29639/29639 [==============================] - 14s 462us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 6/80\n",
      "29639/29639 [==============================] - 13s 449us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "1.0 0.1 [0.2, 0.5]\n",
      "Train on 29639 samples, validate on 10181 samples\n",
      "Epoch 1/80\n",
      "29639/29639 [==============================] - 37s 1ms/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 2/80\n",
      "29639/29639 [==============================] - 16s 523us/step - loss: nan - acc: 0.2516 - val_loss: nan - val_acc: 0.2478\n",
      "Epoch 3/80\n",
      "21376/29639 [====================>.........] - ETA: 2s - loss: nan - acc: 0.2525"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-6b6f490109cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel_family\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-a81ddf060531>\u001b[0m in \u001b[0;36mgrid_search\u001b[0;34m(model_factory, compiler, model_fiter)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0minner_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_l1_regularizations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias_l1_regularizations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0minner_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_l2_regularizations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias_l1_regularizations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0minner_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_l1_regularizations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias_l2_regularizations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0minner_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_l2_regularizations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias_l2_regularizations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-a81ddf060531>\u001b[0m in \u001b[0;36minner_loop\u001b[0;34m(*param_lists)\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregularizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_reg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregularizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_reg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mcompiler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_fiter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0mhistories\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-5639492b0fe5>\u001b[0m in \u001b[0;36mfit_model\u001b[0;34m(model, x_train, y_train, x_test, y_test)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m80\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_delta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m     )   \n\u001b[1;32m     65\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    958\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 960\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1655\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1656\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1657\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1659\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2353\u001b[0m             \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2354\u001b[0m         \u001b[0mfetches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdates_op\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2355\u001b[0;31m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2356\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m   2357\u001b[0m                               **self.session_kwargs)\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    174\u001b[0m             \u001b[0mcandidate_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_keras_initialized'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m                     \u001b[0mcandidate_vars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcandidate_vars\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_family = [model_1_hidden_factory, model_4_hidden_factory]\n",
    "\n",
    "results = []\n",
    "for model in model_family:\n",
    "    res = grid_search(model, compile_model, fit_model)\n",
    "    results.append(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7: Convolutional networks\n",
    "\n",
    "### Description\n",
    "\n",
    "Convolutional neural networks have an inductive bias that is well adapted to image classification.\n",
    "\n",
    "1. Design a convolutional neural network, play with the parameters and fit it. Hint: You may get valuable inspiration from the keras [examples](https://github.com/keras-team/keras/tree/master/examples), e.g. [mnist_cnn](https://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py).\n",
    "2. Plot the learning curves of the convolutional neural network together with the so far best performing model.\n",
    "\n",
    "## Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
